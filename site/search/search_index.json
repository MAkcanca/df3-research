{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"DF3 \u2014 Forensic Image Analysis","text":"<p>Agentic forensic image analysis for detecting AI-generated and manipulated images</p> <p>DF3 combines vision-capable large language models with specialized forensic tools to classify images as <code>real</code>, <code>fake</code>, or <code>uncertain</code>. The system provides structured verdicts with transparent reasoning.</p> <p> Quick Start  User Guide  Evaluation Results</p>"},{"location":"#overview","title":"Overview","text":"<p>DF3 targets three categories:</p> <ul> <li>AI-generated images \u2014 Fully synthetic (DALL-E, Midjourney, Stable Diffusion)</li> <li>Manipulated images \u2014 Edited photographs (splicing, inpainting)</li> <li>Deepfakes \u2014 Face swaps and identity manipulations</li> </ul> <p>The agent-based approach:</p> <ol> <li>Analyzes the image visually for synthesis/manipulation artifacts</li> <li>Invokes forensic tools to gather technical evidence</li> <li>Synthesizes findings into a structured verdict with reasoning</li> </ol>"},{"location":"#two-operating-modes","title":"Two Operating Modes","text":"Mode Description Latency Vision-Only Pure LLM visual analysis 2-10s Tool-Augmented Agent invokes forensic tools 30-90s"},{"location":"#forensic-tools","title":"Forensic Tools","text":"Tool Purpose TruFor Neural forgery detection ELA Error Level Analysis (JPEG) JPEG Analysis Quantization table analysis Frequency Analysis DCT/FFT patterns Residual Extraction DRUNet noise analysis Metadata EXIF/XMP/C2PA extraction Code Execution Custom Python analysis"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code># Single image\npython scripts/analyze_image.py --image photo.jpg\n\n# Vision-only (faster)\npython scripts/analyze_image.py --image photo.jpg --no-tools\n\n# Batch evaluation\npython scripts/evaluate_llms.py --dataset data/eval.jsonl --models gpt-5.1\n</code></pre> <p>Output:</p> <pre><code>Verdict: FAKE\nConfidence: 0.87\n\nRationale: The image shows clear signs of AI generation including \nmalformed fingers (6 digits), unnaturally smooth skin texture, \nand inconsistent lighting/shadow direction.\n\nTools used: perform_trufor, perform_ela\n</code></pre>"},{"location":"#architecture","title":"Architecture","text":"<pre><code>flowchart LR\n    IMG[Image] --&gt; V[Vision LLM]\n    V --&gt;|description| A[Agent]\n    V --&gt;|no-tools mode| S[Structuring]\n    A &lt;--&gt; T[Forensic Tools]\n    A --&gt; S[BAML Structuring]\n    S --&gt; R[ForensicAnalysisResult]</code></pre>"},{"location":"#three-way-classification","title":"Three-Way Classification","text":"<p>Unlike binary classifiers, DF3 can output <code>uncertain</code> for ambiguous cases:</p> <ul> <li>High confidence \u2192 Automated processing</li> <li>Low confidence \u2192 Human review</li> <li>Conflicting evidence \u2192 Expert analysis</li> </ul>"},{"location":"#model-support","title":"Model Support","text":"<p>DF3 uses LangChain and BAML libraries for model API integration, standardized on either OpenRouter-style or OpenAI-style API calls. Any vision-capable model that supports tool calling (at least in prompt format) is supported, including:</p> <ul> <li>Models accessible via OpenAI-compatible endpoints (cloud or local)</li> <li>Models accessible via OpenRouter</li> <li>Locally run LLMs (via OpenAI-compatible local servers or LangChain's local integrations)</li> <li>Any provider supported by LangChain's model integrations</li> </ul> <p>The system does not require specific model implementations\u2014it works with any model that can process images and handle tool/function calling semantics, whether hosted remotely or running locally.</p>"},{"location":"#getting-started","title":"Getting Started","text":"Step Link Install Installation Guide First analysis Quick Start Configure Configuration Evaluate Batch Evaluation"},{"location":"#documentation-structure","title":"Documentation Structure","text":"Section Contents Getting Started Installation, quickstart, first analysis User Guide Workflows, interpreting results Architecture System design, BAML integration Forensic Tools Tool reference Evaluation Methodology, metrics, results Reference Configuration, troubleshooting Research Limitations, reproducibility"},{"location":"references/","title":"References","text":"<p>This page contains academic papers, research publications, and other references relevant to the DF3 forensic image analysis system.</p>"},{"location":"references/#core-algorithms-methods","title":"Core Algorithms &amp; Methods","text":""},{"location":"references/#trufor-neural-forgery-detection","title":"TruFor: Neural Forgery Detection","text":"<p>Citation: <pre><code>@InProceedings{Guillaro_2023_CVPR,\n    author    = {Guillaro, Fabrizio and Cozzolino, Davide and Sud, Avneesh \n                 and Dufour, Nicholas and Verdoliva, Luisa},\n    title     = {TruFor: Leveraging All-Round Clues for Trustworthy Image \n                 Forgery Detection and Localization},\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision \n                 and Pattern Recognition (CVPR)},\n    month     = {June},\n    year      = {2023},\n    pages     = {20606-20615}\n}\n</code></pre></p>"},{"location":"references/#error-level-analysis-ela","title":"Error Level Analysis (ELA)","text":"<p>Reference: - Krawetz, Neal. \"A Picture's Worth: Digital Image Analysis and Forensics.\" Black Hat DC 2008. Paper</p>"},{"location":"references/#jpeg-ghosts-detection","title":"JPEG Ghosts Detection","text":"<p>Citation: <pre><code>@article{farid2009jpeg,\n    author = {Farid, Hany},\n    title = {Exposing Digital Forgeries from JPEG Ghosts},\n    journal = {IEEE Transactions on Information Forensics and Security},\n    year = {2009},\n    volume = {4},\n    number = {1},\n    pages = {154-160}\n}\n</code></pre></p> <p>Paper: Exposing Digital Forgeries from JPEG Ghosts</p>"},{"location":"references/#image-resampling-detection","title":"Image Resampling Detection","text":"<p>Citation: <pre><code>@inproceedings{popescu2005resampling,\n    author = {Popescu, Alin C. and Farid, Hany},\n    title = {Exposing Digital Forgeries by Detecting Traces of Re-sampling},\n    booktitle = {IEEE Transactions on Signal Processing},\n    year = {2005},\n    volume = {53},\n    number = {2},\n    pages = {758-767}\n}\n</code></pre></p> <p>Paper: Exposing Digital Forgeries by Detecting Traces of Re-sampling</p>"},{"location":"references/#noise-based-forensics","title":"Noise-Based Forensics","text":"<p>Citation: <pre><code>@article{mahdian2009noise,\n    author = {Mahdian, Babak and Saic, Stanislav},\n    title = {Using noise inconsistencies for blind image forensics},\n    journal = {Image and Vision Computing},\n    year = {2009},\n    volume = {27},\n    number = {10},\n    pages = {1497-1503}\n}\n</code></pre></p> <p>Paper: Using noise inconsistencies for blind image forensics</p>"},{"location":"references/#noiseprint-camera-model-fingerprint","title":"Noiseprint: Camera Model Fingerprint","text":"<p>Reference: - Cozzolino, Davide and Verdoliva, Luisa. \"Noiseprint: a CNN-based camera model fingerprint.\" Website</p>"},{"location":"references/#contrast-enhancement-detection","title":"Contrast Enhancement Detection","text":"<p>Citation: <pre><code>@article{lin2016contrast,\n    author = {Lin, Xufeng and Wei, Xingjie and Li, Chang-Tsun},\n    title = {Two Improved Forensic Methods of Detecting Contrast Enhancement in Digital Images},\n    journal = {IEEE Transactions on Information Forensics and Security},\n    year = {2016},\n    volume = {11},\n    number = {8},\n    pages = {1725-1735}\n}\n</code></pre></p>"},{"location":"references/#survey-papers-overviews","title":"Survey Papers &amp; Overviews","text":""},{"location":"references/#digital-image-forensics-ai-vs-traditional-approaches","title":"Digital Image Forensics: AI vs Traditional Approaches","text":"<p>Reference: - UHstudent. \"Digital Image Forensics: A Comparative Study between AI and traditional approaches.\" Thesis</p>"},{"location":"references/#tools-libraries","title":"Tools &amp; Libraries","text":""},{"location":"references/#sherloq","title":"Sherloq","text":"<p>The DF3 project incorporates algorithms and techniques inspired by the Sherloq forensic analysis tool. For more information, see the Sherloq repository.</p>"},{"location":"references/#datasets","title":"Datasets","text":""},{"location":"references/#genimage-dataset","title":"GenImage Dataset","text":"<p>Reference: \"GenImage: A Million-Scale Benchmark for Detecting AI-Generated Image\"</p> <p>Used in evaluation and benchmarking of AI-generated image detection capabilities.</p>"},{"location":"references/#see-also","title":"See Also","text":"<ul> <li>Tools Overview \u2014 Detailed documentation of forensic tools</li> <li>Research \u2014 Reproducibility and research methodology</li> <li>Team \u2014 Project contributors</li> </ul>"},{"location":"team/","title":"Team","text":""},{"location":"team/#contributors","title":"Contributors","text":""},{"location":"team/#mustafa-akcanca","title":"Mustafa Akcanca","text":"<p>Contribution: Worked on researching and developing the initial version of DF3.</p> <p>GitHub: @makcanca</p>"},{"location":"api/baml-functions/","title":"BAML Functions API Reference","text":"<p>Reference documentation for BAML-based structured output functions.</p>"},{"location":"api/baml-functions/#overview","title":"Overview","text":"<p>DF3 uses BAML for reliable structured outputs. The key functions are:</p> Function Purpose Output <code>AnalyzeImageVisionOnly</code> Free-form vision reasoning string <code>AnalyzeImageVisionOnlyStructured</code> Vision with structured output ForensicAnalysisResult <code>StructureForensicAnalysis</code> Extract structure from text ForensicAnalysisResult"},{"location":"api/baml-functions/#types","title":"Types","text":""},{"location":"api/baml-functions/#verdict","title":"Verdict","text":"<pre><code>class Verdict(Enum):\n    REAL = \"real\"        # Image appears authentic\n    FAKE = \"fake\"        # Image appears AI-generated or manipulated\n    UNCERTAIN = \"uncertain\"  # Insufficient or conflicting evidence\n</code></pre>"},{"location":"api/baml-functions/#forensicanalysisresult","title":"ForensicAnalysisResult","text":"<pre><code>@dataclass\nclass ForensicAnalysisResult:\n    verdict: Verdict          # Classification result\n    confidence: float         # 0.0 - 1.0\n    rationale: str            # Brief justification (max 80 words)\n    visual_description: str   # Description of image contents\n    forensic_summary: str     # Summary of forensic analysis\n    full_text: str            # Complete narrative\n</code></pre>"},{"location":"api/baml-functions/#python-functions","title":"Python Functions","text":""},{"location":"api/baml-functions/#analyze_vision_only_structured_baml","title":"analyze_vision_only_structured_baml()","text":"<p>Combined vision analysis with structured output.</p> <pre><code>async def analyze_vision_only_structured_baml(\n    image_path: str,\n    model: Optional[str] = None,\n    api_key: Optional[str] = None,\n    base_url: Optional[str] = None,\n    default_headers: Optional[Dict[str, str]] = None,\n) -&gt; Dict[str, Any]\n</code></pre>"},{"location":"api/baml-functions/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>image_path</code> str required Path to image file <code>model</code> str | None <code>\"gpt-5-mini\"</code> LLM model to use <code>api_key</code> str | None env var API key <code>base_url</code> str | None None Custom API endpoint <code>default_headers</code> Dict | None None Custom HTTP headers"},{"location":"api/baml-functions/#returns","title":"Returns","text":"<pre><code>{\n    \"verdict\": \"fake\",\n    \"confidence\": 0.85,\n    \"rationale\": \"Multiple anatomical anomalies...\",\n    \"visual_description\": \"Portrait of a woman...\",\n    \"forensic_summary\": \"No tools used\",\n    \"full_text\": \"### Visual Description\\n...\"\n}\n</code></pre>"},{"location":"api/baml-functions/#example","title":"Example","text":"<pre><code>import asyncio\nfrom src.agents.baml_forensic import analyze_vision_only_structured_baml\n\nresult = asyncio.run(\n    analyze_vision_only_structured_baml(\n        \"photo.jpg\",\n        model=\"gpt-5.1\",\n    )\n)\nprint(f\"Verdict: {result['verdict']}\")\n</code></pre>"},{"location":"api/baml-functions/#structure_analysis_baml","title":"structure_analysis_baml()","text":"<p>Extract structured data from free-form reasoning text.</p> <pre><code>async def structure_analysis_baml(\n    reasoning_output: str,\n    model: Optional[str] = None,\n    api_key: Optional[str] = None,\n    base_url: Optional[str] = None,\n    default_headers: Optional[Dict[str, str]] = None,\n) -&gt; Dict[str, Any]\n</code></pre>"},{"location":"api/baml-functions/#parameters_1","title":"Parameters","text":"Parameter Type Default Description <code>reasoning_output</code> str required Free-form analysis text <code>model</code> str | None <code>\"gpt-5-mini\"</code> LLM model to use <code>api_key</code> str | None env var API key <code>base_url</code> str | None None Custom API endpoint <code>default_headers</code> Dict | None None Custom HTTP headers"},{"location":"api/baml-functions/#returns_1","title":"Returns","text":"<p>Same structure as <code>analyze_vision_only_structured_baml()</code>.</p>"},{"location":"api/baml-functions/#example_1","title":"Example","text":"<pre><code>import asyncio\nfrom src.agents.baml_forensic import structure_analysis_baml\n\nagent_output = \"\"\"\n### Visual Analysis\nThe image shows a person with anatomical anomalies...\n\n### Conclusion\n**Verdict: fake**\n**Confidence (0-1): 0.85**\n\"\"\"\n\nresult = asyncio.run(\n    structure_analysis_baml(agent_output, model=\"gpt-5-mini\")\n)\n</code></pre>"},{"location":"api/baml-functions/#analyze_vision_only_baml","title":"analyze_vision_only_baml()","text":"<p>Free-form vision reasoning without structured output.</p> <pre><code>async def analyze_vision_only_baml(\n    image_path: str,\n    model: Optional[str] = None,\n    api_key: Optional[str] = None,\n    base_url: Optional[str] = None,\n    default_headers: Optional[Dict[str, str]] = None,\n) -&gt; Dict[str, Any]\n</code></pre>"},{"location":"api/baml-functions/#returns_2","title":"Returns","text":"<pre><code>{\n    \"reasoning_output\": \"### Visual Description\\n...\",\n    \"image_path\": \"/path/to/image.jpg\"\n}\n</code></pre>"},{"location":"api/baml-functions/#baml-function-definitions","title":"BAML Function Definitions","text":""},{"location":"api/baml-functions/#analyzeimagevisiononlystructured","title":"AnalyzeImageVisionOnlyStructured","text":"<pre><code>function AnalyzeImageVisionOnlyStructured(image: image) -&gt; ForensicAnalysisResult {\n  client DynamicForensicClient\n  prompt #\"\n    You are a forensic image analyst. Analyze this image and assess \n    whether it appears AI-generated, synthetic, or a deepfake.\n\n    CRITICAL: You MUST always start your analysis by describing what \n    is actually in the image...\n\n    {{ _.role(\"user\") }} {{ image }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n</code></pre>"},{"location":"api/baml-functions/#structureforensicanalysis","title":"StructureForensicAnalysis","text":"<pre><code>function StructureForensicAnalysis(reasoning_output: string) -&gt; ForensicAnalysisResult {\n  client DynamicForensicClient\n  prompt #\"\n    Extract structured information from this forensic analysis \n    reasoning output...\n\n    {{ reasoning_output }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n</code></pre>"},{"location":"api/baml-functions/#client-registry","title":"Client Registry","text":"<p>The <code>DynamicForensicClient</code> is overridden at runtime:</p> <pre><code>def _create_client_registry(\n    model: str,\n    api_key: Optional[str] = None,\n    base_url: Optional[str] = None,\n    default_headers: Optional[Dict[str, str]] = None,\n) -&gt; ClientRegistry:\n    \"\"\"Create BAML ClientRegistry with specified model.\"\"\"\n    cr = ClientRegistry()\n\n    options = {\n        \"model\": model,\n        \"api_key\": api_key or os.environ.get(\"OPENAI_API_KEY\"),\n    }\n\n    if base_url:\n        options[\"base_url\"] = base_url\n\n    provider = \"openai-responses\"\n    if base_url and \"openrouter.ai\" in base_url.lower():\n        provider = \"openai\"\n\n    cr.add_llm_client(\n        name=\"DynamicForensicClient\",\n        provider=provider,\n        options=options,\n    )\n\n    return cr\n</code></pre>"},{"location":"api/baml-functions/#vision-caching","title":"Vision Caching","text":"<p>Vision outputs are cached to avoid redundant API calls:</p> <pre><code># Check cache\ncached = cache.get_vision_output(\n    vision_model=model,\n    image_path=image_path,\n    cache_tag=_vision_cache_tag(),\n)\n\n# Store in cache\ncache.set_vision_output(\n    vision_model=model,\n    image_path=image_path,\n    output=result,\n    cache_tag=_vision_cache_tag(),\n)\n</code></pre>"},{"location":"api/baml-functions/#cache-tag","title":"Cache Tag","text":"<p>The cache tag ensures invalidation when prompts change:</p> <pre><code>def _vision_cache_tag() -&gt; str:\n    # Check environment override\n    env = os.getenv(\"DF3_VISION_CACHE_TAG\")\n    if env:\n        return env\n\n    # Hash BAML source file\n    baml_path = \"baml_src/forensic_analysis.baml\"\n    if baml_path.exists():\n        return f\"baml:{hashlib.sha256(baml_path.read_bytes()).hexdigest()[:16]}\"\n\n    return \"unknown\"\n</code></pre>"},{"location":"api/baml-functions/#error-handling","title":"Error Handling","text":"<pre><code>from baml_py import BamlValidationError\n\ntry:\n    result = await analyze_vision_only_structured_baml(\"image.jpg\")\nexcept BamlValidationError as e:\n    # Output didn't match schema\n    print(f\"Validation error: {e}\")\nexcept FileNotFoundError:\n    # Image not found\n    print(\"Image file not found\")\nexcept Exception as e:\n    # Other errors\n    print(f\"Analysis failed: {e}\")\n</code></pre>"},{"location":"api/baml-functions/#see-also","title":"See Also","text":"<ul> <li>ForensicAgent \u2014 Main API reference</li> <li>BAML Integration \u2014 Architecture details</li> <li>Configuration \u2014 Model configuration</li> </ul>"},{"location":"api/forensic-agent/","title":"ForensicAgent API Reference","text":"<p>Complete API reference for the <code>ForensicAgent</code> class.</p>"},{"location":"api/forensic-agent/#class-definition","title":"Class Definition","text":"<pre><code>from src.agents import ForensicAgent\n\nclass ForensicAgent:\n    \"\"\"\n    Forensic image analysis agent that combines vision-capable LLMs\n    with forensic tools to detect AI-generated and manipulated images.\n    \"\"\"\n</code></pre>"},{"location":"api/forensic-agent/#constructor","title":"Constructor","text":"<pre><code>def __init__(\n    self,\n    llm_model: str = \"gpt-5.1\",\n    vision_model: Optional[str] = None,\n    structuring_model: Optional[str] = None,\n    temperature: float = 0.0,\n    reasoning_effort: Optional[str] = None,\n    api_key: Optional[str] = None,\n    base_url: Optional[str] = None,\n    default_headers: Optional[Dict[str, str]] = None,\n    decision_policy: Optional[str] = None,\n    max_iterations: Optional[int] = 15,\n    enable_checkpointer: bool = True,\n)\n</code></pre>"},{"location":"api/forensic-agent/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>llm_model</code> str <code>\"gpt-5.1\"</code> Model for agent reasoning and tool calling <code>vision_model</code> str | None None Model for vision analysis (defaults to <code>llm_model</code>) <code>structuring_model</code> str | None None Model for BAML structuring (defaults to <code>llm_model</code>) <code>temperature</code> float <code>0.0</code> LLM temperature (0.0 for deterministic) <code>reasoning_effort</code> str | None None Reasoning effort level (provider-specific) <code>api_key</code> str | None None API key (defaults to <code>OPENAI_API_KEY</code> env var) <code>base_url</code> str | None None Custom API base URL (e.g., OpenRouter) <code>default_headers</code> Dict | None None Custom HTTP headers (e.g., OpenRouter headers) <code>decision_policy</code> str | None None Reserved for future use <code>max_iterations</code> int | None <code>15</code> Maximum tool calls per analysis <code>enable_checkpointer</code> bool <code>True</code> Enable LangGraph state checkpointing"},{"location":"api/forensic-agent/#example","title":"Example","text":"<pre><code># Basic initialization\nagent = ForensicAgent()\n\n# Custom configuration\nagent = ForensicAgent(\n    llm_model=\"gpt-5.2\",\n    vision_model=\"gpt-5.2\",\n    structuring_model=\"gpt-5-mini\",\n    temperature=0.0,\n    max_iterations=20,\n)\n\n# OpenRouter configuration\nagent = ForensicAgent(\n    llm_model=\"anthropic/claude-sonnet-4\",\n    api_key=os.environ[\"OPENROUTER_API_KEY\"],\n    base_url=\"https://openrouter.ai/api/v1\",\n)\n</code></pre>"},{"location":"api/forensic-agent/#methods","title":"Methods","text":""},{"location":"api/forensic-agent/#analyze","title":"analyze()","text":"<p>Main analysis method for processing images.</p> <pre><code>def analyze(\n    self,\n    image_path: str,\n    user_query: Optional[str] = None,\n    use_tools: bool = True,\n    pass_image_to_agent: bool = False,\n) -&gt; Dict[str, Any]\n</code></pre>"},{"location":"api/forensic-agent/#parameters_1","title":"Parameters","text":"Parameter Type Default Description <code>image_path</code> str required Path to the image file <code>user_query</code> str | None None Optional specific question about the image <code>use_tools</code> bool <code>True</code> Whether to use forensic tools <code>pass_image_to_agent</code> bool <code>False</code> Include image in agent message (vs text only)"},{"location":"api/forensic-agent/#returns","title":"Returns","text":"<p>Dictionary containing analysis results:</p> <pre><code>{\n    # Core verdict\n    \"verdict\": str,           # \"real\" | \"fake\" | \"uncertain\"\n    \"confidence\": float,      # 0.0 - 1.0\n    \"rationale\": str,         # Explanation for verdict\n\n    # Descriptions\n    \"visual_description\": str,\n    \"forensic_summary\": str,\n\n    # Tool information\n    \"tool_usage\": List[str],  # Tool names used\n    \"tool_details\": List[Dict],\n    \"tool_results\": List[Dict],\n\n    # Timing\n    \"timings\": {\n        \"vision_llm_seconds\": float,\n        \"agent_graph_seconds\": float,\n        \"total_seconds\": float,\n    },\n\n    # Model provenance\n    \"models\": {\n        \"agent\": str,\n        \"vision\": str,\n        \"structuring\": str,\n    },\n\n    # Raw outputs\n    \"raw_text\": str,\n    \"raw_parsed\": Dict,\n    \"prompts\": Dict[str, str],\n\n    # Report\n    \"report_markdown\": str,   # SWGDE-style report\n\n    # Metadata\n    \"image_path\": str,\n}\n</code></pre>"},{"location":"api/forensic-agent/#example_1","title":"Example","text":"<pre><code>agent = ForensicAgent()\n\n# Basic analysis with tools\nresult = agent.analyze(\"photo.jpg\")\nprint(f\"Verdict: {result['verdict']}\")\nprint(f\"Confidence: {result['confidence']:.2f}\")\n\n# Vision-only analysis\nresult = agent.analyze(\"photo.jpg\", use_tools=False)\n\n# With specific question\nresult = agent.analyze(\n    \"portrait.jpg\",\n    user_query=\"Is this a deepfake?\",\n    use_tools=True,\n)\n</code></pre>"},{"location":"api/forensic-agent/#instance-attributes","title":"Instance Attributes","text":"Attribute Type Description <code>llm</code> ChatOpenAI LangChain LLM instance for agent <code>llm_model</code> str Model identifier <code>vision_model</code> str Vision step model identifier <code>structuring_model</code> str Structuring step model identifier <code>tools</code> List[Tool] Registered forensic tools <code>agent_executor</code> CompiledGraph LangGraph agent graph <code>system_prompt</code> str Agent system prompt <code>max_iterations</code> int | None Maximum iterations"},{"location":"api/forensic-agent/#class-methods","title":"Class Methods","text":""},{"location":"api/forensic-agent/#_normalize_verdict","title":"_normalize_verdict()","text":"<p>Normalize verdict string to canonical form.</p> <pre><code>@staticmethod\ndef _normalize_verdict(verdict: Optional[str]) -&gt; str:\n    \"\"\"\n    Normalize verdict to 'real', 'fake', or 'uncertain'.\n\n    Handles variations like 'AI-generated' -&gt; 'fake',\n    'authentic' -&gt; 'real', 'inconclusive' -&gt; 'uncertain'.\n    \"\"\"\n</code></pre>"},{"location":"api/forensic-agent/#usage-patterns","title":"Usage Patterns","text":""},{"location":"api/forensic-agent/#single-image-analysis","title":"Single Image Analysis","text":"<pre><code>from src.agents import ForensicAgent\n\nagent = ForensicAgent()\nresult = agent.analyze(\"suspicious_image.jpg\")\n\nif result[\"verdict\"] == \"fake\":\n    print(f\"Image appears fake: {result['rationale']}\")\nelif result[\"verdict\"] == \"uncertain\":\n    print(\"Image needs human review\")\nelse:\n    print(\"Image appears authentic\")\n</code></pre>"},{"location":"api/forensic-agent/#batch-processing","title":"Batch Processing","text":"<pre><code>from pathlib import Path\nfrom src.agents import ForensicAgent\n\nagent = ForensicAgent()\nresults = []\n\nfor image_path in Path(\"images/\").glob(\"*.jpg\"):\n    result = agent.analyze(str(image_path))\n    results.append({\n        \"image\": str(image_path),\n        \"verdict\": result[\"verdict\"],\n        \"confidence\": result[\"confidence\"],\n    })\n\n# Analyze results\nfake_count = sum(1 for r in results if r[\"verdict\"] == \"fake\")\nprint(f\"Found {fake_count} potentially fake images\")\n</code></pre>"},{"location":"api/forensic-agent/#comparing-modes","title":"Comparing Modes","text":"<pre><code>agent = ForensicAgent()\n\n# Vision-only (faster)\nvision_result = agent.analyze(\"image.jpg\", use_tools=False)\n\n# Tool-augmented (more thorough)\ntools_result = agent.analyze(\"image.jpg\", use_tools=True)\n\n# Compare\nprint(f\"Vision-only: {vision_result['verdict']} ({vision_result['confidence']:.2f})\")\nprint(f\"With tools: {tools_result['verdict']} ({tools_result['confidence']:.2f})\")\nprint(f\"Tools used: {tools_result['tool_usage']}\")\n</code></pre>"},{"location":"api/forensic-agent/#error-handling","title":"Error Handling","text":"<pre><code>from src.agents import ForensicAgent\n\nagent = ForensicAgent()\n\ntry:\n    result = agent.analyze(\"nonexistent.jpg\")\nexcept FileNotFoundError as e:\n    print(f\"Image not found: {e}\")\nexcept Exception as e:\n    print(f\"Analysis failed: {e}\")\n</code></pre>"},{"location":"api/forensic-agent/#memory-management","title":"Memory Management","text":""},{"location":"api/forensic-agent/#image-cache","title":"Image Cache","text":"<p>Encoded images are cached to avoid redundant base64 encoding:</p> <pre><code>from src.agents.forensic_agent import clear_image_cache\n\n# Clear the image encoding cache\nclear_image_cache()\n</code></pre>"},{"location":"api/forensic-agent/#tool-cache","title":"Tool Cache","text":"<p>Tool outputs are cached separately. See Configuration for cache control.</p>"},{"location":"api/forensic-agent/#thread-safety","title":"Thread Safety","text":"<p><code>ForensicAgent</code> instances are not thread-safe. For parallel processing:</p> <ol> <li>Create separate agent instances per thread</li> <li>Use thread-local storage</li> <li>Or use the batch evaluator which handles this</li> </ol> <pre><code>import threading\n\nthread_local = threading.local()\n\ndef get_agent():\n    if not hasattr(thread_local, \"agent\"):\n        thread_local.agent = ForensicAgent()\n    return thread_local.agent\n</code></pre>"},{"location":"api/forensic-agent/#see-also","title":"See Also","text":"<ul> <li>BAML Functions \u2014 Structured output functions</li> <li>Tool Functions \u2014 Forensic tool reference</li> <li>Configuration \u2014 Configuration options</li> </ul>"},{"location":"api/tool-functions/","title":"Tool Functions API Reference","text":"<p>Reference documentation for forensic tool functions.</p>"},{"location":"api/tool-functions/#tool-registration","title":"Tool Registration","text":"<p>Tools are registered in <code>src/tools/forensic_tools.py</code>:</p> <pre><code>from src.tools.forensic_tools import create_forensic_tools\n\ntools = create_forensic_tools(timing_hook=None)\n</code></pre>"},{"location":"api/tool-functions/#trufor","title":"TruFor","text":""},{"location":"api/tool-functions/#perform_trufor","title":"perform_trufor()","text":"<pre><code>def perform_trufor(input_str: str) -&gt; str:\n    \"\"\"\n    Run TruFor forgery detection and localization.\n\n    Args:\n        input_str: JSON string or plain path\n            - Plain: \"/path/to/image.jpg\"\n            - JSON: {\"path\": \"...\", \"return_map\": false}\n\n    Returns:\n        JSON string with results\n    \"\"\"\n</code></pre>"},{"location":"api/tool-functions/#input-schema","title":"Input Schema","text":"<pre><code>class TruForInput(BaseModel):\n    path: str  # Path to image file\n</code></pre>"},{"location":"api/tool-functions/#output","title":"Output","text":"<pre><code>{\n    \"tool\": \"perform_trufor\",\n    \"status\": \"completed\",\n    \"image_path\": \"/path/to/image.jpg\",\n    \"manipulation_probability\": 0.15,\n    \"detection_score\": 0.15,\n    \"localization_map\": null,\n    \"localization_map_size\": null,\n    \"note\": \"TruFor combines RGB features with Noiseprint++...\"\n}\n</code></pre>"},{"location":"api/tool-functions/#prewarm_trufor_model","title":"prewarm_trufor_model()","text":"<pre><code>def prewarm_trufor_model(device: str = None) -&gt; bool:\n    \"\"\"\n    Pre-warm TruFor model cache before workers start.\n\n    Args:\n        device: Optional device override (default: auto-detect)\n\n    Returns:\n        True if successful, False otherwise\n    \"\"\"\n</code></pre>"},{"location":"api/tool-functions/#ela","title":"ELA","text":""},{"location":"api/tool-functions/#perform_ela","title":"perform_ela()","text":"<pre><code>def perform_ela(input_str: str) -&gt; str:\n    \"\"\"\n    Run Error Level Analysis.\n\n    Args:\n        input_str: JSON string or plain path\n            - JSON: {\"path\": \"...\", \"quality\": 75, \"return_map\": false}\n\n    Returns:\n        JSON string with results (or skipped for non-JPEG)\n    \"\"\"\n</code></pre>"},{"location":"api/tool-functions/#input-schema_1","title":"Input Schema","text":"<pre><code>class ELAInput(BaseModel):\n    path: str                    # Path to image file\n    quality: int = 75            # JPEG recompression quality (1-100)\n</code></pre>"},{"location":"api/tool-functions/#output_1","title":"Output","text":"<pre><code>{\n    \"tool\": \"perform_ela\",\n    \"status\": \"completed\",\n    \"image_path\": \"/path/to/image.jpg\",\n    \"quality\": 75,\n    \"ela_mean\": 12.5,\n    \"ela_std\": 8.3,\n    \"ela_anomaly_score\": 2.1,\n    \"ela_map\": null,\n    \"ela_map_size\": null,\n    \"note\": \"ELA recompresses at fixed JPEG quality...\"\n}\n</code></pre>"},{"location":"api/tool-functions/#metadata","title":"Metadata","text":""},{"location":"api/tool-functions/#metadata_1","title":"metadata()","text":"<pre><code>def metadata(input_str: str) -&gt; str:\n    \"\"\"\n    Extract image metadata (EXIF/XMP/ICC) and C2PA credentials.\n\n    Args:\n        input_str: JSON string or plain path\n\n    Returns:\n        JSON string with metadata fields\n    \"\"\"\n</code></pre>"},{"location":"api/tool-functions/#output_2","title":"Output","text":"<pre><code>{\n    \"tool\": \"metadata\",\n    \"status\": \"completed\",\n    \"image_path\": \"/path/to/image.jpg\",\n    \"exif\": {...},\n    \"xmp\": {...},\n    \"icc\": {...},\n    \"c2pa\": {...}\n}\n</code></pre>"},{"location":"api/tool-functions/#jpeg-analysis","title":"JPEG Analysis","text":""},{"location":"api/tool-functions/#analyze_jpeg_compression","title":"analyze_jpeg_compression()","text":"<pre><code>def analyze_jpeg_compression(image_path: str) -&gt; str:\n    \"\"\"\n    Analyze JPEG compression artifacts.\n\n    Args:\n        image_path: Plain string path\n\n    Returns:\n        JSON string (or skipped for non-JPEG)\n    \"\"\"\n</code></pre>"},{"location":"api/tool-functions/#detect_jpeg_quantization","title":"detect_jpeg_quantization()","text":"<pre><code>def detect_jpeg_quantization(image_path: str) -&gt; str:\n    \"\"\"\n    Extract quantization tables and estimate quality.\n\n    Args:\n        image_path: Plain string path\n\n    Returns:\n        JSON string (or skipped for non-JPEG)\n    \"\"\"\n</code></pre>"},{"location":"api/tool-functions/#frequency-analysis","title":"Frequency Analysis","text":""},{"location":"api/tool-functions/#analyze_frequency_domain","title":"analyze_frequency_domain()","text":"<pre><code>def analyze_frequency_domain(image_path: str) -&gt; str:\n    \"\"\"\n    Analyze DCT/FFT frequency domain features.\n\n    Args:\n        image_path: Plain string path\n\n    Returns:\n        JSON string with frequency features\n    \"\"\"\n</code></pre>"},{"location":"api/tool-functions/#residual-analysis","title":"Residual Analysis","text":""},{"location":"api/tool-functions/#extract_residuals","title":"extract_residuals()","text":"<pre><code>def extract_residuals(image_path: str) -&gt; str:\n    \"\"\"\n    Extract DRUNet residual statistics.\n\n    Args:\n        image_path: Plain string path\n\n    Returns:\n        JSON string with residual statistics\n    \"\"\"\n</code></pre>"},{"location":"api/tool-functions/#output_3","title":"Output","text":"<pre><code>{\n    \"tool\": \"extract_residuals\",\n    \"status\": \"completed\",\n    \"image_path\": \"/path/to/image.jpg\",\n    \"residual_mean\": 0.0012,\n    \"residual_std\": 8.45,\n    \"residual_skew\": 0.23,\n    \"residual_kurtosis\": 3.12,\n    \"residual_energy\": 71.4,\n    \"residual_energy_mean\": 0.0089,\n    \"residual_energy_std\": 0.0034,\n    \"residual_energy_p95\": 0.0156\n}\n</code></pre>"},{"location":"api/tool-functions/#prewarm_residual_extractor","title":"prewarm_residual_extractor()","text":"<pre><code>def prewarm_residual_extractor() -&gt; bool:\n    \"\"\"\n    Pre-warm DRUNet model before workers start.\n\n    Returns:\n        True if successful, False otherwise\n    \"\"\"\n</code></pre>"},{"location":"api/tool-functions/#code-execution","title":"Code Execution","text":""},{"location":"api/tool-functions/#run_code_interpreter","title":"run_code_interpreter()","text":"<pre><code>def run_code_interpreter(input_str: str) -&gt; str:\n    \"\"\"\n    Execute Python code for custom analysis.\n\n    Args:\n        input_str: JSON string\n            {\"code\": \"...\", \"image_path\": \"...\"}\n\n    Returns:\n        JSON string with execution output\n    \"\"\"\n</code></pre>"},{"location":"api/tool-functions/#input-schema_2","title":"Input Schema","text":"<pre><code>class PythonCodeInput(BaseModel):\n    code: str                     # Python code to execute\n    image_path: Optional[str]     # Optional image path\n</code></pre>"},{"location":"api/tool-functions/#output_4","title":"Output","text":"<pre><code>{\n    \"tool\": \"execute_python_code\",\n    \"status\": \"completed\",\n    \"output\": \"stdout from code execution\",\n    \"artifacts\": [\"path/to/saved/file.png\"],\n    \"error\": null\n}\n</code></pre>"},{"location":"api/tool-functions/#tool-caching","title":"Tool Caching","text":""},{"location":"api/tool-functions/#toolcache","title":"ToolCache","text":"<pre><code>from src.tools.forensic.cache import ToolCache, get_cache, set_cache\n\n# Get global cache instance\ncache = get_cache()\n\n# Set custom cache\ncustom_cache = ToolCache(\n    cache_dir=\"/path/to/cache\",\n    enabled=True,\n)\nset_cache(custom_cache)\n\n# Get cached output\noutput = cache.get(tool_name, image_path, params)\n\n# Set cached output\ncache.set(tool_name, image_path, output, params)\n\n# Get cache statistics\nstats = cache.get_stats()\n</code></pre>"},{"location":"api/tool-functions/#cache-statistics","title":"Cache Statistics","text":"<pre><code>{\n    \"cache_dir\": \"/path/to/cache\",\n    \"entry_count\": 150,\n    \"total_size_mb\": 45.2,\n    \"enabled\": True\n}\n</code></pre>"},{"location":"api/tool-functions/#timing-hook","title":"Timing Hook","text":"<p>Tools support timing instrumentation:</p> <pre><code>def timing_hook(tool_name: str, seconds: float, error: Optional[str]) -&gt; None:\n    \"\"\"Called after each tool execution.\"\"\"\n    print(f\"{tool_name} took {seconds:.2f}s\")\n\ntools = create_forensic_tools(timing_hook=timing_hook)\n</code></pre>"},{"location":"api/tool-functions/#error-handling","title":"Error Handling","text":"<p>All tools return JSON with error information on failure:</p> <pre><code>{\n    \"tool\": \"perform_trufor\",\n    \"status\": \"error\",\n    \"error\": \"Description of what went wrong\"\n}\n</code></pre>"},{"location":"api/tool-functions/#status-values","title":"Status Values","text":"Status Meaning <code>completed</code> Tool executed successfully <code>error</code> Tool encountered an error <code>skipped</code> Tool not applicable (e.g., JPEG tool on PNG)"},{"location":"api/tool-functions/#direct-tool-usage","title":"Direct Tool Usage","text":"<p>Tools can be called directly without the agent:</p> <pre><code>from src.tools.forensic import perform_trufor, perform_ela\nimport json\n\n# TruFor\nresult = perform_trufor('{\"path\": \"image.jpg\"}')\ndata = json.loads(result)\nprint(f\"Manipulation probability: {data['manipulation_probability']}\")\n\n# ELA\nresult = perform_ela('{\"path\": \"image.jpg\", \"quality\": 75}')\ndata = json.loads(result)\nprint(f\"Anomaly score: {data['ela_anomaly_score']}\")\n</code></pre>"},{"location":"api/tool-functions/#see-also","title":"See Also","text":"<ul> <li>Tools Overview \u2014 High-level tool documentation</li> <li>ForensicAgent \u2014 Agent API reference</li> <li>Configuration \u2014 Cache and device configuration</li> </ul>"},{"location":"architecture/agent-pipeline/","title":"Agent Pipeline","text":"<p>Deep dive into the ForensicAgent's analysis pipeline and LangGraph integration.</p>"},{"location":"architecture/agent-pipeline/#forensicagent-class","title":"ForensicAgent Class","text":"<p>The <code>ForensicAgent</code> class in <code>src/agents/forensic_agent.py</code> is the main entry point for analysis.</p>"},{"location":"architecture/agent-pipeline/#initialization","title":"Initialization","text":"<pre><code>class ForensicAgent:\n    def __init__(\n        self,\n        llm_model: str = \"gpt-5.1\",\n        vision_model: Optional[str] = None,\n        structuring_model: Optional[str] = None,\n        temperature: float = 0.0,\n        reasoning_effort: Optional[str] = None,\n        api_key: Optional[str] = None,\n        base_url: Optional[str] = None,\n        default_headers: Optional[Dict[str, str]] = None,\n        decision_policy: Optional[str] = None,\n        max_iterations: Optional[int] = 15,\n        enable_checkpointer: bool = True,\n    ):\n</code></pre>"},{"location":"architecture/agent-pipeline/#key-components","title":"Key Components","text":"Component Purpose <code>self.llm</code> ChatOpenAI instance for agent reasoning <code>self.tools</code> List of LangChain Tool objects <code>self.agent_executor</code> LangGraph ReAct agent graph <code>self.llm_model</code> Model identifier for agent <code>self.vision_model</code> Model identifier for vision step <code>self.structuring_model</code> Model identifier for BAML structuring"},{"location":"architecture/agent-pipeline/#analysis-pipeline","title":"Analysis Pipeline","text":""},{"location":"architecture/agent-pipeline/#main-analysis-method","title":"Main Analysis Method","text":"<pre><code>def analyze(\n    self,\n    image_path: str,\n    user_query: Optional[str] = None,\n    use_tools: bool = True,\n    pass_image_to_agent: bool = False,\n) -&gt; Dict:\n</code></pre>"},{"location":"architecture/agent-pipeline/#pipeline-stages","title":"Pipeline Stages","text":"<pre><code>stateDiagram-v2\n    [*] --&gt; ValidateImage\n    ValidateImage --&gt; VisionAnalysis: Image exists\n    ValidateImage --&gt; Error: Image not found\n\n    VisionAnalysis --&gt; CheckToolsMode\n\n    CheckToolsMode --&gt; DirectStructuring: use_tools=False\n    CheckToolsMode --&gt; AgentReasoning: use_tools=True\n\n    AgentReasoning --&gt; ToolExecution: Tool call\n    ToolExecution --&gt; AgentReasoning: Tool result\n    AgentReasoning --&gt; BAMLStructuring: Reasoning complete\n\n    DirectStructuring --&gt; BuildResult\n    BAMLStructuring --&gt; BuildResult\n\n    BuildResult --&gt; GenerateReport\n    GenerateReport --&gt; [*]\n\n    Error --&gt; [*]</code></pre>"},{"location":"architecture/agent-pipeline/#stage-1-vision-analysis","title":"Stage 1: Vision Analysis","text":"<p>Every analysis starts with the BAML vision step, regardless of tool mode.</p>"},{"location":"architecture/agent-pipeline/#implementation","title":"Implementation","text":"<pre><code># Phase 0: always get a vision-only description first\nvision_result_dict = asyncio.run(\n    analyze_vision_only_structured_baml(\n        image_path,\n        model=self.vision_model,\n        api_key=self.api_key,\n        base_url=self.base_url,\n        default_headers=self.default_headers,\n    )\n)\n</code></pre>"},{"location":"architecture/agent-pipeline/#output","title":"Output","text":"<pre><code>{\n    \"verdict\": \"fake\",\n    \"confidence\": 0.75,\n    \"rationale\": \"Multiple anatomical anomalies...\",\n    \"visual_description\": \"Portrait of a woman...\",\n    \"forensic_summary\": \"No tools used\",\n    \"full_text\": \"### Visual Description\\n...\"\n}\n</code></pre>"},{"location":"architecture/agent-pipeline/#purpose","title":"Purpose","text":"<ol> <li>Initial assessment \u2014 Get visual verdict without tools</li> <li>Context for agent \u2014 Visual description guides tool selection</li> <li>Standalone mode \u2014 Complete analysis for <code>use_tools=False</code></li> </ol>"},{"location":"architecture/agent-pipeline/#stage-2-agent-reasoning-tools-mode","title":"Stage 2: Agent Reasoning (Tools Mode)","text":"<p>When <code>use_tools=True</code>, the LangGraph ReAct agent takes over.</p>"},{"location":"architecture/agent-pipeline/#agent-creation","title":"Agent Creation","text":"<pre><code>def _create_agent(self):\n    \"\"\"Create LangGraph agent with forensic tools.\"\"\"\n    system_prompt = get_system_prompt()\n\n    kwargs = {\n        \"model\": self.llm,\n        \"tools\": self.tools,\n        \"prompt\": system_prompt,\n    }\n    if self.enable_checkpointer:\n        kwargs[\"checkpointer\"] = MemorySaver()\n\n    graph = create_react_agent(**kwargs)\n    return graph\n</code></pre>"},{"location":"architecture/agent-pipeline/#react-loop","title":"ReAct Loop","text":"<p>The agent follows the ReAct (Reasoning + Acting) pattern:</p> <pre><code>flowchart TD\n    START[Start] --&gt; THINK[Think: Analyze situation]\n    THINK --&gt; DECIDE{Decide action}\n\n    DECIDE --&gt;|Need more info| ACT[Act: Call tool]\n    DECIDE --&gt;|Ready to conclude| FINISH[Finish: Output verdict]\n\n    ACT --&gt; OBSERVE[Observe: Get tool result]\n    OBSERVE --&gt; THINK\n\n    FINISH --&gt; END[End]</code></pre>"},{"location":"architecture/agent-pipeline/#agent-invocation","title":"Agent Invocation","text":"<pre><code># Build message with visual context\nagent_prompt = build_agent_prompt(\n    visual_summary=visual_summary,\n    image_path=image_path\n)\n\nmessages = [HumanMessage(content=agent_prompt)]\n\n# Invoke agent\nconfig = {\n    \"configurable\": {\"thread_id\": str(uuid.uuid4())},\n    \"recursion_limit\": 2 * self.max_iterations + 1,\n}\n\nresult = self.agent_executor.invoke(\n    {\"messages\": messages},\n    config=config\n)\n</code></pre>"},{"location":"architecture/agent-pipeline/#tool-execution-tracking","title":"Tool Execution Tracking","text":"<pre><code># Track tool usage from agent messages\ntool_usage = []\ntool_details = []\ntool_results = []\n\nfor msg in result['messages']:\n    if hasattr(msg, 'tool_calls') and msg.tool_calls:\n        for tool_call in msg.tool_calls:\n            tool_name = tool_call.get('name')\n            if tool_name:\n                tool_usage.append(tool_name)\n\n    if isinstance(msg, ToolMessage):\n        tool_name = getattr(msg, \"name\", \"unknown\")\n        tool_result = msg.content\n        # ... record details\n</code></pre>"},{"location":"architecture/agent-pipeline/#stage-3-output-structuring","title":"Stage 3: Output Structuring","text":"<p>The agent's free-form output is structured via BAML.</p>"},{"location":"architecture/agent-pipeline/#implementation_1","title":"Implementation","text":"<pre><code>structured = asyncio.run(structure_analysis_baml(\n    output,\n    model=self.structuring_model,\n    api_key=self.api_key,\n    base_url=self.base_url,\n    default_headers=self.default_headers,\n))\n</code></pre>"},{"location":"architecture/agent-pipeline/#why-separate-structuring","title":"Why Separate Structuring?","text":"<p>Research shows that requiring structured output during reasoning can degrade LLM performance (\"reasoning degradation\"). By separating:</p> <ol> <li>Reasoning phase \u2014 LLM outputs natural language freely</li> <li>Structuring phase \u2014 Dedicated call extracts structured data</li> </ol> <p>This approach maintains reasoning quality while ensuring reliable structured output.</p>"},{"location":"architecture/agent-pipeline/#result-building","title":"Result Building","text":""},{"location":"architecture/agent-pipeline/#final-result-dictionary","title":"Final Result Dictionary","text":"<pre><code>def _build_result(\n    self,\n    raw_text: str,\n    parsed: Optional[Dict[str, Any]],\n    tool_usage: List[str],\n    image_path: str,\n    prompts: Dict[str, str],\n) -&gt; Dict[str, Any]:\n    return {\n        \"verdict\": self._normalize_verdict(parsed.get(\"verdict\")),\n        \"confidence\": float(parsed.get(\"confidence\", 0.0)),\n        \"rationale\": parsed.get(\"rationale\", \"\"),\n        \"visual_description\": parsed.get(\"visual_description\", \"\"),\n        \"forensic_summary\": parsed.get(\"forensic_summary\", \"\"),\n        \"raw_text\": raw_text,\n        \"tool_usage\": tool_usage,\n        \"image_path\": image_path,\n        \"prompts\": prompts,\n        # ... additional fields\n    }\n</code></pre>"},{"location":"architecture/agent-pipeline/#verdict-normalization","title":"Verdict Normalization","text":"<pre><code>@staticmethod\ndef _normalize_verdict(verdict: Optional[str]) -&gt; str:\n    \"\"\"Normalize verdict to real/fake/uncertain.\"\"\"\n    if not verdict:\n        return \"uncertain\"\n\n    v = str(verdict).strip().lower()\n\n    fake_tokens = {\"fake\", \"ai-generated\", \"synthetic\", \"manipulated\", ...}\n    real_tokens = {\"real\", \"authentic\", \"natural\", \"genuine\"}\n    uncertain_tokens = {\"uncertain\", \"inconclusive\", \"unknown\", ...}\n\n    if v in fake_tokens:\n        return \"fake\"\n    if v in real_tokens:\n        return \"real\"\n    return \"uncertain\"\n</code></pre>"},{"location":"architecture/agent-pipeline/#iteration-control","title":"Iteration Control","text":""},{"location":"architecture/agent-pipeline/#max-iterations","title":"Max Iterations","text":"<p>The <code>max_iterations</code> parameter limits tool calls:</p> <pre><code># Calculate recursion_limit from max_iterations\n# Each iteration uses 2 steps (action + observation)\nconfig[\"recursion_limit\"] = 2 * self.max_iterations + 1\n</code></pre>"},{"location":"architecture/agent-pipeline/#default-15-iterations","title":"Default: 15 Iterations","text":"<ul> <li>Allows complex multi-tool investigations</li> <li>Prevents infinite loops</li> <li>Balances thoroughness with latency</li> </ul>"},{"location":"architecture/agent-pipeline/#adjusting-iterations","title":"Adjusting Iterations","text":"<pre><code># More thorough analysis\nagent = ForensicAgent(max_iterations=25)\n\n# Quick analysis\nagent = ForensicAgent(max_iterations=5)\n\n# No limit (not recommended)\nagent = ForensicAgent(max_iterations=None)\n</code></pre>"},{"location":"architecture/agent-pipeline/#checkpointing","title":"Checkpointing","text":"<p>LangGraph's MemorySaver enables:</p> <ul> <li>Conversation persistence</li> <li>State recovery on errors</li> <li>Multi-turn interactions</li> </ul>"},{"location":"architecture/agent-pipeline/#enabledisable","title":"Enable/Disable","text":"<pre><code># With checkpointing (default)\nagent = ForensicAgent(enable_checkpointer=True)\n\n# Without checkpointing (batch evaluation)\nagent = ForensicAgent(enable_checkpointer=False)\n</code></pre>"},{"location":"architecture/agent-pipeline/#per-invocation-isolation","title":"Per-Invocation Isolation","text":"<p>Each <code>analyze()</code> call uses a unique thread ID:</p> <pre><code>config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n</code></pre> <p>This ensures independent analysis sessions.</p>"},{"location":"architecture/agent-pipeline/#system-prompt","title":"System Prompt","text":"<p>The agent system prompt defines behavior and tool usage guidance.</p>"},{"location":"architecture/agent-pipeline/#key-sections","title":"Key Sections","text":"<p>From <code>src/agents/prompts.py</code>:</p> <ol> <li>Primary task definition</li> <li>Distinction between manipulation and synthesis</li> <li>Visual inspection guidelines</li> <li>Tool interpretation guidance</li> <li>Verdict decision logic</li> <li>Output format specification</li> <li>SWGDE best practices (appended)</li> </ol>"},{"location":"architecture/agent-pipeline/#prompt-loading","title":"Prompt Loading","text":"<pre><code>def get_system_prompt() -&gt; str:\n    base_prompt = \"\"\"You are a forensic image analysis agent...\"\"\"\n\n    swgde_best_practices = _load_swgde_best_practices_text()\n\n    if swgde_best_practices:\n        return base_prompt + \"\\n\\n---\\n\\n\" + swgde_best_practices\n    return base_prompt\n</code></pre> <p>See Prompts Reference for complete prompt documentation.</p>"},{"location":"architecture/agent-pipeline/#error-handling","title":"Error Handling","text":""},{"location":"architecture/agent-pipeline/#image-validation","title":"Image Validation","text":"<pre><code>if not os.path.exists(image_path):\n    raise FileNotFoundError(f\"Image not found: {image_path}\")\n</code></pre>"},{"location":"architecture/agent-pipeline/#tool-errors","title":"Tool Errors","text":"<p>Tool errors are captured in results:</p> <pre><code>tool_details.append({\n    \"tool\": tool_name,\n    \"seconds\": seconds,\n    \"status\": parsed_tool.get(\"status\", \"unknown\"),\n    \"error\": parsed_tool.get(\"error\"),\n})\n</code></pre>"},{"location":"architecture/agent-pipeline/#baml-structuring-failures","title":"BAML Structuring Failures","text":"<pre><code>try:\n    structured = asyncio.run(structure_analysis_baml(output, ...))\nexcept Exception as e:\n    logger.error(f\"BAML structuring failed: {e}\")\n    raise  # BAML is required\n</code></pre>"},{"location":"architecture/agent-pipeline/#timing-instrumentation","title":"Timing Instrumentation","text":""},{"location":"architecture/agent-pipeline/#timing-breakdown","title":"Timing Breakdown","text":"<pre><code>timings: Dict[str, Any] = {}\n\n# Vision step timing\nt0 = time.perf_counter()\nvision_result = await analyze_vision_only_structured_baml(...)\ntimings[\"vision_llm_seconds\"] = time.perf_counter() - t0\n\n# Agent step timing\nstart_time = time.perf_counter()\nresult = self.agent_executor.invoke(...)\ntimings[\"agent_graph_seconds\"] = time.perf_counter() - start_time\n\n# Total timing\ntimings[\"total_seconds\"] = time.perf_counter() - t_total0\n</code></pre>"},{"location":"architecture/agent-pipeline/#tool-level-timing","title":"Tool-Level Timing","text":"<p>Via timing hook:</p> <pre><code>def _tool_timing_hook(self, tool_name: str, seconds: float, error: Optional[str]):\n    if self._current_tool_timings is not None:\n        self._current_tool_timings.append({\n            \"tool\": tool_name,\n            \"seconds\": seconds,\n            \"error\": error,\n        })\n</code></pre>"},{"location":"architecture/agent-pipeline/#next-steps","title":"Next Steps","text":"<ul> <li>BAML Integration \u2014 Structured output system</li> <li>Tools Overview \u2014 Forensic tool details</li> <li>API Reference \u2014 Complete API documentation</li> </ul>"},{"location":"architecture/baml-integration/","title":"BAML Integration","text":"<p>How DF3 uses BAML for reliable structured outputs without reasoning degradation.</p>"},{"location":"architecture/baml-integration/#what-is-baml","title":"What is BAML?","text":"<p>BAML (Basically, A Made-Up Language) is a domain-specific language for building LLM functions with structured outputs. It provides:</p> <ul> <li>Type-safe outputs \u2014 Guaranteed to match your schema</li> <li>Multi-provider support \u2014 OpenAI, Anthropic, Google, etc.</li> <li>Retry and fallback \u2014 Built-in resilience</li> <li>Prompt templates \u2014 Jinja-like templating</li> </ul>"},{"location":"architecture/baml-integration/#the-reasoning-degradation-problem","title":"The Reasoning Degradation Problem","text":"<p>Research shows that requiring structured output (JSON) during reasoning can degrade LLM performance:</p> <pre><code>flowchart LR\n    subgraph Traditional[\"Traditional Approach\"]\n        direction TB\n        T1[Input] --&gt; T2[Reason + Format]\n        T2 --&gt; T3[JSON Output]\n    end\n\n    subgraph TwoStep[\"DF3 Approach\"]\n        direction TB\n        D1[Input] --&gt; D2[Reason Freely]\n        D2 --&gt; D3[Markdown Output]\n        D3 --&gt; D4[Structure Separately]\n        D4 --&gt; D5[JSON Output]\n    end\n\n    style T2 fill:#ef4444,color:#fff\n    style D2 fill:#22c55e,color:#fff\n    style D4 fill:#22c55e,color:#fff</code></pre> <p>Why this happens:</p> <ul> <li>JSON formatting constraints reduce available \"thinking\" capacity</li> <li>LLMs may truncate reasoning to fit format</li> <li>Schema adherence competes with analytical depth</li> </ul> <p>DF3's solution:</p> <ol> <li>Let the LLM reason freely in natural language</li> <li>Use a separate call to extract structured data</li> </ol>"},{"location":"architecture/baml-integration/#baml-files","title":"BAML Files","text":"<p>DF3's BAML definitions are in <code>baml_src/</code>:</p> <pre><code>baml_src/\n\u251c\u2500\u2500 clients.baml           # LLM client configurations\n\u251c\u2500\u2500 forensic_analysis.baml # Analysis functions and types\n\u2514\u2500\u2500 generators.baml        # Code generation config\n</code></pre>"},{"location":"architecture/baml-integration/#type-definitions","title":"Type Definitions","text":"<pre><code>// Verdict enum\nenum Verdict {\n  REAL @description(\"Image appears authentic and natural\")\n  FAKE @description(\"Image appears AI-generated, synthetic, or manipulated\")\n  UNCERTAIN @description(\"Insufficient evidence or conflicting indicators\")\n}\n\n// Structured output schema\nclass ForensicAnalysisResult {\n  verdict Verdict\n  confidence float @assert(between_0_and_1, {{ this &gt;= 0.0 and this &lt;= 1.0 }})\n  rationale string @description(\"Brief justification for the verdict (max 80 words)\")\n  visual_description string @description(\"Description of what is in the image\")\n  forensic_summary string @description(\"Summary of forensic tools used or 'No tools used'\")\n  full_text string @description(\"Complete formatted narrative combining all sections\")\n}\n</code></pre>"},{"location":"architecture/baml-integration/#function-definitions","title":"Function Definitions","text":"<p>Vision-only (unstructured reasoning):</p> <pre><code>function AnalyzeImageVisionOnly(image: image) -&gt; string {\n  client DynamicForensicClient\n  prompt #\"\n    You are a forensic image analyst...\n\n    Provide your analysis in MARKDOWN format...\n\n    {{ _.role(\"user\") }} {{ image }}\n  \"#\n}\n</code></pre> <p>Structuring (extract structured data):</p> <pre><code>function StructureForensicAnalysis(reasoning_output: string) -&gt; ForensicAnalysisResult {\n  client DynamicForensicClient\n  prompt #\"\n    Extract structured information from this forensic analysis reasoning output...\n\n    {{ reasoning_output }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n</code></pre>"},{"location":"architecture/baml-integration/#client-configuration","title":"Client Configuration","text":""},{"location":"architecture/baml-integration/#dynamic-client-override","title":"Dynamic Client Override","text":"<p>DF3 uses a dynamic client that can be overridden at runtime:</p> <pre><code>// Default client (overridden at runtime)\nclient&lt;llm&gt; DynamicForensicClient {\n  provider openai-responses\n  retry_policy Exponential\n  options {\n    model \"gpt-5-mini\"  // Default, overridden by Python\n    api_key env.OPENAI_API_KEY\n  }\n}\n</code></pre>"},{"location":"architecture/baml-integration/#python-client-registry","title":"Python Client Registry","text":"<pre><code>def _create_client_registry(\n    model: str,\n    api_key: Optional[str] = None,\n    base_url: Optional[str] = None,\n    default_headers: Optional[Dict[str, str]] = None,\n) -&gt; baml_py_core.ClientRegistry:\n    \"\"\"Create a BAML ClientRegistry that overrides DynamicForensicClient.\"\"\"\n\n    cr = baml_py_core.ClientRegistry()\n\n    options = {\n        \"model\": model,\n        \"api_key\": api_key or os.environ.get(\"OPENAI_API_KEY\"),\n    }\n\n    if base_url:\n        options[\"base_url\"] = base_url\n\n    # Choose provider based on endpoint\n    provider = \"openai-responses\"\n    if base_url and \"openrouter.ai\" in str(base_url).lower():\n        provider = \"openai\"  # OpenRouter uses Chat Completions API\n\n    cr.add_llm_client(\n        name=\"DynamicForensicClient\",\n        provider=provider,\n        options=options,\n    )\n\n    return cr\n</code></pre>"},{"location":"architecture/baml-integration/#python-integration","title":"Python Integration","text":""},{"location":"architecture/baml-integration/#baml-function-calls","title":"BAML Function Calls","text":"<p>From <code>src/agents/baml_forensic.py</code>:</p> <p>Vision-only structured:</p> <pre><code>async def analyze_vision_only_structured_baml(\n    image_path: str,\n    model: Optional[str] = None,\n    api_key: Optional[str] = None,\n    base_url: Optional[str] = None,\n    default_headers: Optional[Dict[str, str]] = None,\n) -&gt; Dict[str, Any]:\n    \"\"\"Combined vision-only analysis with structured output.\"\"\"\n\n    # Load image as BAML Image type\n    with open(image_path, \"rb\") as f:\n        encoded = base64.b64encode(f.read()).decode('utf-8')\n    image = BamlImage.from_base64(mime_type, encoded)\n\n    # Create client registry with specified model\n    client_registry = _create_client_registry(model, api_key, base_url, default_headers)\n\n    # Call BAML function\n    result = await b.AnalyzeImageVisionOnlyStructured(\n        image,\n        baml_options={\"client_registry\": client_registry}\n    )\n\n    # Convert to dictionary\n    return {\n        \"verdict\": result.verdict.value.lower(),\n        \"confidence\": result.confidence,\n        \"rationale\": result.rationale,\n        \"visual_description\": result.visual_description,\n        \"forensic_summary\": result.forensic_summary,\n        \"full_text\": result.full_text,\n    }\n</code></pre> <p>Structuring:</p> <pre><code>async def structure_analysis_baml(\n    reasoning_output: str,\n    model: Optional[str] = None,\n    ...\n) -&gt; Dict[str, Any]:\n    \"\"\"Structure unstructured reasoning output.\"\"\"\n\n    client_registry = _create_client_registry(model, ...)\n\n    result = await b.StructureForensicAnalysis(\n        reasoning_output,\n        baml_options={\"client_registry\": client_registry}\n    )\n\n    return {\n        \"verdict\": result.verdict.value.lower(),\n        \"confidence\": result.confidence,\n        # ...\n    }\n</code></pre>"},{"location":"architecture/baml-integration/#vision-cache-integration","title":"Vision Cache Integration","text":"<p>BAML vision outputs are cached to avoid redundant API calls:</p> <pre><code>async def analyze_vision_only_structured_baml(...):\n    # Check cache first\n    if get_cache is not None:\n        cache = get_cache()\n        cached_result = cache.get_vision_output(\n            vision_model=model,\n            image_path=image_path,\n            cache_tag=_vision_cache_tag(),\n        )\n        if cached_result is not None:\n            return cached_result\n\n    # ... perform analysis ...\n\n    # Store in cache\n    if get_cache is not None:\n        cache.set_vision_output(\n            vision_model=model,\n            image_path=image_path,\n            output=result,\n            cache_tag=_vision_cache_tag(),\n        )\n\n    return result\n</code></pre>"},{"location":"architecture/baml-integration/#cache-tag","title":"Cache Tag","text":"<p>The cache tag ensures cache invalidation when prompts change:</p> <pre><code>def _vision_cache_tag() -&gt; str:\n    \"\"\"Return cache discriminator based on BAML file hash.\"\"\"\n    # Check environment override\n    env = os.getenv(\"DF3_VISION_CACHE_TAG\")\n    if env:\n        return str(env)\n\n    # Hash the BAML source file\n    baml_path = repo_root / \"baml_src\" / \"forensic_analysis.baml\"\n    if baml_path.exists():\n        h = hashlib.sha256(baml_path.read_bytes()).hexdigest()[:16]\n        return f\"baml:{h}\"\n\n    return \"unknown\"\n</code></pre>"},{"location":"architecture/baml-integration/#regenerating-the-client","title":"Regenerating the Client","text":"<p>After modifying <code>baml_src/*.baml</code> files:</p> <pre><code>pip install baml-py  # If not installed\nbaml-cli generate\n</code></pre> <p>This regenerates <code>baml_client/</code>:</p> <pre><code>baml_client/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 async_client.py      # Async function calls\n\u251c\u2500\u2500 sync_client.py       # Sync function calls\n\u251c\u2500\u2500 types.py             # Type definitions\n\u251c\u2500\u2500 config.py\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"architecture/baml-integration/#best-practices","title":"Best Practices","text":""},{"location":"architecture/baml-integration/#prompt-design","title":"Prompt Design","text":"<ol> <li>Include <code>{{ ctx.output_format }}</code> \u2014 Tells BAML to insert schema instructions</li> <li>Use <code>{{ _.role(\"user\") }}</code> \u2014 Marks where user content starts</li> <li>Don't repeat schema fields \u2014 BAML handles this</li> <li>Keep prompts focused \u2014 Let reasoning happen, don't over-constrain</li> </ol> <p>See Prompts Reference for complete BAML prompt documentation.</p>"},{"location":"architecture/baml-integration/#type-design","title":"Type Design","text":"<ol> <li>Use enums for categories \u2014 <code>Verdict</code> instead of free-form string</li> <li>Add descriptions \u2014 Help the LLM understand field purposes</li> <li>Use assertions \u2014 <code>@assert</code> for constraints like confidence bounds</li> <li>Keep structures flat \u2014 Avoid deep nesting when possible</li> </ol>"},{"location":"architecture/baml-integration/#error-handling","title":"Error Handling","text":"<pre><code>try:\n    result = await b.StructureForensicAnalysis(text, ...)\nexcept BamlValidationError as e:\n    # Output didn't match schema\n    logger.error(f\"Validation failed: {e}\")\nexcept BamlClientFinishReasonError as e:\n    # LLM stopped unexpectedly\n    logger.error(f\"Finish reason error: {e}\")\n</code></pre>"},{"location":"architecture/baml-integration/#supported-providers","title":"Supported Providers","text":"<p>BAML supports multiple LLM providers:</p> Provider BAML Provider Name Notes OpenAI <code>openai-responses</code> Responses API (preferred) OpenAI <code>openai</code> Chat Completions API Anthropic <code>anthropic</code> Claude models Google <code>google-ai</code> Gemini models Azure <code>azure-openai</code> Azure-hosted OpenAI Vertex AI <code>vertex-ai</code> Google Cloud AWS Bedrock <code>aws-bedrock</code> AWS-hosted models"},{"location":"architecture/baml-integration/#openrouter-configuration","title":"OpenRouter Configuration","text":"<p>For OpenRouter, use the <code>openai</code> provider (Chat Completions):</p> <pre><code>if \"openrouter.ai\" in base_url:\n    provider = \"openai\"  # Not openai-responses\n</code></pre>"},{"location":"architecture/baml-integration/#next-steps","title":"Next Steps","text":"<ul> <li>System Overview \u2014 Complete architecture</li> <li>API Reference \u2014 BAML function reference</li> <li>Configuration \u2014 Model configuration</li> </ul>"},{"location":"architecture/overview/","title":"System Overview","text":"<p>Technical architecture of DF3's forensic image analysis system.</p>"},{"location":"architecture/overview/#high-level-architecture","title":"High-Level Architecture","text":"<pre><code>flowchart TB\n    subgraph Input[\"Input Layer\"]\n        IMG[Image File]\n        CFG[Configuration]\n    end\n\n    subgraph Core[\"Core Analysis Engine\"]\n        direction TB\n\n        subgraph Agent[\"ForensicAgent\"]\n            INIT[Initialize Models]\n            VISION[Vision Analysis]\n            TOOLS[Tool Orchestration]\n            STRUCT[Output Structuring]\n        end\n\n        subgraph BAML[\"BAML Layer\"]\n            BV[AnalyzeImageVisionOnlyStructured]\n            BS[StructureForensicAnalysis]\n        end\n\n        subgraph LangGraph[\"LangGraph Layer\"]\n            REACT[ReAct Agent]\n            GRAPH[State Graph]\n        end\n    end\n\n    subgraph Tools[\"Forensic Tools\"]\n        TF[TruFor]\n        ELA[ELA]\n        JPEG[JPEG Analysis]\n        FREQ[Frequency Analysis]\n        RES[Residuals]\n        META[Metadata]\n        CODE[Code Execution]\n    end\n\n    subgraph Cache[\"Caching Layer\"]\n        TC[Tool Cache]\n        VC[Vision Cache]\n    end\n\n    subgraph Output[\"Output Layer\"]\n        RESULT[ForensicAnalysisResult]\n        REPORT[SWGDE Report]\n    end\n\n    IMG --&gt; Agent\n    CFG --&gt; Agent\n    Agent --&gt; BAML\n    Agent --&gt; LangGraph\n    LangGraph --&gt; Tools\n    Tools --&gt; Cache\n    BAML --&gt; Output\n\n    style Agent fill:#4a1d96,color:#fff\n    style BAML fill:#065f46,color:#fff\n    style LangGraph fill:#b45309,color:#fff\n    style Tools fill:#1e40af,color:#fff</code></pre>"},{"location":"architecture/overview/#component-overview","title":"Component Overview","text":""},{"location":"architecture/overview/#entry-points","title":"Entry Points","text":"Component Path Purpose <code>analyze_image.py</code> <code>scripts/</code> Single image analysis CLI <code>evaluate_llms.py</code> <code>scripts/</code> Batch evaluation CLI <code>ForensicAgent</code> <code>src/agents/</code> Programmatic API"},{"location":"architecture/overview/#core-engine","title":"Core Engine","text":"Component Path Purpose <code>ForensicAgent</code> <code>src/agents/forensic_agent.py</code> Main orchestrator class <code>baml_forensic.py</code> <code>src/agents/</code> BAML function wrappers <code>prompts.py</code> <code>src/agents/</code> System and user prompts"},{"location":"architecture/overview/#forensic-tools","title":"Forensic Tools","text":"Component Path Purpose <code>forensic_tools.py</code> <code>src/tools/</code> Tool registration for LangGraph <code>trufor_tools.py</code> <code>src/tools/forensic/</code> TruFor neural forgery detection <code>ela_tools.py</code> <code>src/tools/forensic/</code> Error Level Analysis <code>jpeg_tools.py</code> <code>src/tools/forensic/</code> JPEG compression analysis <code>frequency_tools.py</code> <code>src/tools/forensic/</code> DCT/FFT frequency analysis <code>noise_tools.py</code> <code>src/tools/forensic/</code> DRUNet residual extraction <code>metadata_tools.py</code> <code>src/tools/forensic/</code> EXIF/XMP/C2PA extraction <code>code_execution_tool.py</code> <code>src/tools/forensic/</code> Dynamic Python execution"},{"location":"architecture/overview/#support-systems","title":"Support Systems","text":"Component Path Purpose <code>cache.py</code> <code>src/tools/forensic/</code> Tool and vision output caching <code>weight_downloader.py</code> <code>src/utils/</code> Auto-download model weights <code>image_auth_report.py</code> <code>src/reporting/</code> SWGDE-style report generation"},{"location":"architecture/overview/#directory-structure","title":"Directory Structure","text":"<pre><code>df3/\n\u251c\u2500\u2500 baml_src/                   # BAML function definitions\n\u2502   \u251c\u2500\u2500 clients.baml            # LLM client configurations\n\u2502   \u251c\u2500\u2500 forensic_analysis.baml  # Analysis functions\n\u2502   \u2514\u2500\u2500 generators.baml         # Code generation config\n\u2502\n\u251c\u2500\u2500 baml_client/                # Generated BAML Python client\n\u2502   \u251c\u2500\u2500 async_client.py\n\u2502   \u251c\u2500\u2500 sync_client.py\n\u2502   \u2514\u2500\u2500 types.py\n\u2502\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 agents/\n\u2502   \u2502   \u251c\u2500\u2500 forensic_agent.py   # Main agent class\n\u2502   \u2502   \u251c\u2500\u2500 baml_forensic.py    # BAML integration\n\u2502   \u2502   \u251c\u2500\u2500 prompts.py          # Prompt definitions\n\u2502   \u2502   \u2514\u2500\u2500 evidence_normalizer.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 tools/\n\u2502   \u2502   \u251c\u2500\u2500 forensic/           # Tool implementations\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 trufor_tools.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 ela_tools.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 jpeg_tools.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 frequency_tools.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 noise_tools.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 metadata_tools.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 code_execution_tool.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 cache.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 drunet/         # DRUNet model\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 trufor_support/ # TruFor model\n\u2502   \u2502   \u2514\u2500\u2500 forensic_tools.py   # Tool registration\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 reporting/\n\u2502   \u2502   \u2514\u2500\u2500 image_auth_report.py\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 utils/\n\u2502       \u2514\u2500\u2500 weight_downloader.py\n\u2502\n\u251c\u2500\u2500 scripts/\n\u2502   \u251c\u2500\u2500 analyze_image.py        # Single image CLI\n\u2502   \u251c\u2500\u2500 evaluate_llms.py        # Batch evaluation\n\u2502   \u2514\u2500\u2500 summarize_results.py    # Results aggregation\n\u2502\n\u251c\u2500\u2500 weights/\n\u2502   \u2514\u2500\u2500 trufor/\n\u2502       \u2514\u2500\u2500 trufor.pth.tar      # TruFor weights (~180MB)\n\u2502\n\u251c\u2500\u2500 tests/                      # Test suite\n\u251c\u2500\u2500 docs/                       # Legacy docs\n\u251c\u2500\u2500 markdown_docs/              # MkDocs documentation\n\u2514\u2500\u2500 results/                    # Evaluation results\n</code></pre>"},{"location":"architecture/overview/#data-flow","title":"Data Flow","text":""},{"location":"architecture/overview/#single-image-analysis","title":"Single Image Analysis","text":"<pre><code>sequenceDiagram\n    participant User\n    participant FA as ForensicAgent\n    participant BAML as BAML Functions\n    participant LG as LangGraph\n    participant Tools as Forensic Tools\n    participant Cache\n\n    User-&gt;&gt;FA: analyze(image_path)\n\n    Note over FA: Step 1: Vision Analysis\n    FA-&gt;&gt;Cache: Check vision cache\n    alt Cache Hit\n        Cache--&gt;&gt;FA: Cached result\n    else Cache Miss\n        FA-&gt;&gt;BAML: AnalyzeImageVisionOnlyStructured(image)\n        BAML--&gt;&gt;FA: VisionResult\n        FA-&gt;&gt;Cache: Store result\n    end\n\n    alt use_tools = True\n        Note over FA: Step 2: Tool-Augmented Analysis\n        FA-&gt;&gt;LG: Create ReAct agent with tools\n        loop Agent Reasoning\n            LG-&gt;&gt;LG: Decide next action\n            alt Tool Call\n                LG-&gt;&gt;Cache: Check tool cache\n                alt Cache Hit\n                    Cache--&gt;&gt;LG: Cached output\n                else Cache Miss\n                    LG-&gt;&gt;Tools: Execute tool\n                    Tools--&gt;&gt;LG: Tool output\n                    LG-&gt;&gt;Cache: Store output\n                end\n            else Finish\n                LG--&gt;&gt;FA: Final reasoning\n            end\n        end\n\n        Note over FA: Step 3: Structuring\n        FA-&gt;&gt;BAML: StructureForensicAnalysis(reasoning)\n        BAML--&gt;&gt;FA: Structured result\n    end\n\n    FA--&gt;&gt;User: ForensicAnalysisResult</code></pre>"},{"location":"architecture/overview/#key-design-decisions","title":"Key Design Decisions","text":""},{"location":"architecture/overview/#two-step-structured-output","title":"Two-Step Structured Output","text":"<p>Problem: Requiring JSON output during reasoning degrades LLM performance.</p> <p>Solution: Separate reasoning from structuring:</p> <ol> <li>Vision/Agent step: Free-form markdown output</li> <li>Structuring step: Extract structured data via BAML</li> </ol> <p>Implementation: <code>StructureForensicAnalysis</code> in <code>forensic_analysis.baml</code></p>"},{"location":"architecture/overview/#agentic-tool-selection","title":"Agentic Tool Selection","text":"<p>Problem: Fixed tool pipelines can't adapt to image content.</p> <p>Solution: LangGraph ReAct agent decides which tools to use based on:</p> <ul> <li>Initial visual analysis results</li> <li>Previous tool outputs</li> <li>Remaining uncertainty</li> </ul> <p>Implementation: <code>create_react_agent</code> with <code>create_forensic_tools()</code> in <code>forensic_agent.py</code></p>"},{"location":"architecture/overview/#three-way-classification","title":"Three-Way Classification","text":"<p>Problem: Binary classifiers force incorrect answers on ambiguous cases.</p> <p>Solution: Include <code>uncertain</code> as a valid verdict for:</p> <ul> <li>Conflicting evidence</li> <li>Low confidence</li> <li>Image quality issues</li> </ul> <p>Implementation: <code>Verdict</code> enum in <code>forensic_analysis.baml</code></p>"},{"location":"architecture/overview/#transparent-caching","title":"Transparent Caching","text":"<p>Problem: Repeated analyses waste compute; stale caches invalidate experiments.</p> <p>Solution: Cache with versioning:</p> <ul> <li>Tool outputs cached by (tool_name, image_hash, params)</li> <li>Vision outputs cached with prompt hash</li> <li>Cache tag prevents stale reads after prompt changes</li> </ul> <p>Implementation: <code>ToolCache</code> in <code>cache.py</code></p>"},{"location":"architecture/overview/#technology-stack","title":"Technology Stack","text":""},{"location":"architecture/overview/#core-dependencies","title":"Core Dependencies","text":"Package Version Purpose LangChain 0.3+ LLM abstraction layer LangGraph 0.2+ Agent orchestration BAML 0.70+ Structured LLM outputs PyTorch 2.0+ Neural network backend OpenAI SDK 1.0+ API client"},{"location":"architecture/overview/#image-processing","title":"Image Processing","text":"Package Purpose Pillow Image loading and manipulation OpenCV Advanced image processing NumPy Numerical operations SciPy Scientific computing"},{"location":"architecture/overview/#development","title":"Development","text":"Package Purpose pytest Testing ruff Linting and formatting mypy Type checking MkDocs Documentation"},{"location":"architecture/overview/#configuration-layers","title":"Configuration Layers","text":"<pre><code>flowchart LR\n    subgraph Environment\n        ENV[Environment Variables]\n    end\n\n    subgraph Files\n        DOTENV[.env file]\n        BAML[baml_src/*.baml]\n    end\n\n    subgraph Runtime\n        CLI[CLI Arguments]\n        API[API Parameters]\n    end\n\n    ENV --&gt; Config\n    DOTENV --&gt; ENV\n    BAML --&gt; Config\n    CLI --&gt; Config\n    API --&gt; Config\n\n    Config --&gt; ForensicAgent</code></pre> <p>Precedence (highest to lowest):</p> <ol> <li>API/CLI parameters</li> <li>Environment variables</li> <li><code>.env</code> file</li> <li>BAML defaults</li> <li>Code defaults</li> </ol>"},{"location":"architecture/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Agent Pipeline \u2014 Detailed agent architecture</li> <li>BAML Integration \u2014 Structured output system</li> <li>Tools Overview \u2014 Forensic tool details</li> </ul>"},{"location":"evaluation/dataset-provenance/","title":"Dataset Provenance","text":"<p>Documentation of the evaluation dataset, including sources and sampling methodology.</p>"},{"location":"evaluation/dataset-provenance/#evaluation-dataset","title":"Evaluation Dataset","text":"<p>The evaluation uses a single dataset sampled from multiple sources. Different runs use different sample limits (n=200 or n=500) but draw from the same underlying sample pool.</p> Property Value Total Samples 500 (247 fake, 253 real) ID Digest Computed from sorted sample IDs Location <code>data2/samples.jsonl</code> Format Mix ~80% JPEG, ~20% PNG"},{"location":"evaluation/dataset-provenance/#sample-limit-configurations","title":"Sample Limit Configurations","text":"Limit Digest Usage n=500 <code>f987165daff0de70</code> Full dataset runs n=200 <code>1f78e35118013ed4</code> Subset (first 200 by stable order) <p>The n=200 runs use the first 200 samples from the same shuffled dataset. Results from different limits should be compared with caution due to sample composition differences.</p>"},{"location":"evaluation/dataset-provenance/#source-datasets","title":"Source Datasets","text":""},{"location":"evaluation/dataset-provenance/#synthetic-images-fake-class","title":"Synthetic Images (Fake Class)","text":"<p>GenImage</p> <ul> <li>Source: GenImage GitHub</li> <li>Reference: \"GenImage: A Million-Scale Benchmark for Detecting AI-Generated Image\"</li> <li>Structure: Organized by generator (Midjourney, Stable Diffusion, etc.)</li> <li>Note: The \"nature\" subset contains real images from ImageNet</li> </ul> <p>DRAGON</p> <ul> <li>Source: Hugging Face</li> </ul> <p>Nano-banana 150k</p> <ul> <li>Sources:</li> <li>Nano-consistent-150k</li> <li>Nano-banana-150k</li> </ul>"},{"location":"evaluation/dataset-provenance/#real-images-real-class","title":"Real Images (Real Class)","text":"<p>GenImage Nature Subset</p> <ul> <li>Real images collected from ImageNet</li> <li>Paired with synthetic images in the GenImage dataset</li> </ul> <p>Nano-banana Real Subset</p> <ul> <li>Real images as provided by dataset hosting</li> </ul>"},{"location":"evaluation/dataset-provenance/#sampling-methodology","title":"Sampling Methodology","text":""},{"location":"evaluation/dataset-provenance/#sampling-command","title":"Sampling Command","text":"<pre><code>python scripts/sample_dataset.py \\\n    --num_samples 500 \\\n    --output_dir samples \\\n    --dataset_dir .\\dataset\\ \\\n    --seed 42\n</code></pre>"},{"location":"evaluation/dataset-provenance/#folder-discovery","title":"Folder Discovery","text":"Class Folder Names Real <code>real</code>, <code>nature</code>, <code>authentic</code>, <code>genuine</code>, <code>real_images</code> Fake <code>fake</code>, <code>ai</code>, <code>synthetic</code>, <code>generated</code>, <code>deepfake</code>, <code>fake_images</code>"},{"location":"evaluation/dataset-provenance/#process","title":"Process","text":"<ol> <li>Recursive enumeration of images (jpg, jpeg, png, bmp, tiff)</li> <li>Random sampling with fixed seed for reproducibility</li> <li>Copy to output directory with canonical names</li> <li>Generate JSONL manifest</li> <li>Shuffle entries</li> </ol>"},{"location":"evaluation/dataset-provenance/#output-format","title":"Output Format","text":"<pre><code>{\"id\": \"sample-001\", \"image\": \"sample-001.jpg\", \"label\": \"real\"}\n</code></pre> <p>Both <code>image</code>/<code>label</code> and <code>image_path</code>/<code>ground_truth</code> field variants are supported.</p>"},{"location":"evaluation/dataset-provenance/#dataset-digest-computation","title":"Dataset Digest Computation","text":"<pre><code>import hashlib\nsample_ids = sorted([r[\"id\"] for r in dataset])\ndigest = hashlib.sha256(str(sample_ids).encode()).hexdigest()[:16]\n</code></pre> <p>Compare results only when digests match.</p>"},{"location":"evaluation/dataset-provenance/#licensing","title":"Licensing","text":"Dataset License GenImage See repository DRAGON See Hugging Face card Nano-banana See Hugging Face card ImageNet Restricted research use <p>For external delivery, consider manifest-with-hashes approach for restricted sources.</p>"},{"location":"evaluation/dataset-provenance/#see-also","title":"See Also","text":"<ul> <li>Methodology \u2014 Evaluation framework</li> <li>Benchmark Results \u2014 Performance data</li> </ul>"},{"location":"evaluation/methodology/","title":"Evaluation Methodology","text":"<p>How DF3 evaluates performance and what the metrics mean.</p>"},{"location":"evaluation/methodology/#evaluation-framework","title":"Evaluation Framework","text":"<p>DF3's evaluation is designed for selective classification (triage), not forced binary classification. This means:</p> <ul> <li>The system can output <code>uncertain</code> instead of guessing</li> <li>Metrics measure both accuracy and coverage</li> <li>Human review is part of the intended workflow</li> </ul>"},{"location":"evaluation/methodology/#the-three-way-triage-model","title":"The Three-Way Triage Model","text":"<pre><code>flowchart LR\n    subgraph Input\n        IMG[Image Pool]\n    end\n\n    subgraph DF3[\"DF3 Triage\"]\n        REAL[Predict: REAL]\n        FAKE[Predict: FAKE]\n        UNC[Predict: UNCERTAIN]\n    end\n\n    subgraph Output\n        PASS[Auto-pass]\n        FLAG[Auto-flag]\n        REVIEW[Human Review]\n    end\n\n    IMG --&gt; DF3\n    REAL --&gt; PASS\n    FAKE --&gt; FLAG\n    UNC --&gt; REVIEW\n\n    style PASS fill:#22c55e,color:#fff\n    style FLAG fill:#ef4444,color:#fff\n    style REVIEW fill:#f59e0b,color:#fff</code></pre>"},{"location":"evaluation/methodology/#why-three-way","title":"Why Three-Way?","text":"<p>Traditional binary classifiers force a decision:</p> <ul> <li>Problem: Ambiguous cases get wrong answers</li> <li>Cost: False positives and false negatives have real impact</li> </ul> <p>Three-way triage enables:</p> <ul> <li>Appropriate uncertainty: \"I don't know\" is valid</li> <li>Intelligent routing: Hard cases go to humans</li> <li>Calibrated confidence: Trust high-confidence decisions</li> </ul>"},{"location":"evaluation/methodology/#metric-categories","title":"Metric Categories","text":""},{"location":"evaluation/methodology/#primary-metrics","title":"Primary Metrics","text":"Metric Formula Meaning accuracy (TP + TN) / N Overall correct rate (abstentions = wrong) accuracy_answered (TP + TN) / answered Correct rate among answered samples coverage answered / N Fraction of samples answered"},{"location":"evaluation/methodology/#class-specific-metrics","title":"Class-Specific Metrics","text":"Metric Description precision_fake TP / (TP + FP) \u2014 When we say \"fake\", how often correct? recall_fake TP / (TP + FN) \u2014 What fraction of fakes do we catch? f1_fake Harmonic mean of precision and recall for fake class precision_real TN / (TN + FN) \u2014 When we say \"real\", how often correct? recall_real TN / (TN + FP) \u2014 What fraction of reals do we pass? f1_real Harmonic mean for real class"},{"location":"evaluation/methodology/#triage-specific-metrics","title":"Triage-Specific Metrics","text":"Metric Formula Meaning fake_slip_rate FN / N_fake Fakes incorrectly passed as real real_false_flag_rate FP / N_real Reals incorrectly flagged as fake fake_catch_rate TP / N_fake Fakes correctly caught real_pass_rate TN / N_real Reals correctly passed"},{"location":"evaluation/methodology/#balanced-metrics","title":"Balanced Metrics","text":"Metric Formula Meaning balanced_accuracy (TPR_fake + TPR_real) / 2 Balanced across classes MCC Matthews Correlation Coefficient Handles class imbalance"},{"location":"evaluation/methodology/#confusion-matrix","title":"Confusion Matrix","text":"<p>For three-way classification:</p> <pre><code>                    Predicted\n                    REAL    FAKE    UNCERTAIN\n            REAL    TN      FP      Abstain_real\nGround      FAKE    FN      TP      Abstain_fake\nTruth\n</code></pre>"},{"location":"evaluation/methodology/#treatment-of-abstentions","title":"Treatment of Abstentions","text":"Metric Type Abstention Treatment Overall accuracy Count as wrong Answered-only metrics Exclude from calculation Coverage Measures how often we answer Class-conditional Track separately by ground truth"},{"location":"evaluation/methodology/#evaluation-protocol","title":"Evaluation Protocol","text":""},{"location":"evaluation/methodology/#dataset-requirements","title":"Dataset Requirements","text":"<pre><code>{\"id\": \"sample-001\", \"image\": \"path/to/image.jpg\", \"label\": \"real\"}\n{\"id\": \"sample-002\", \"image\": \"path/to/image.jpg\", \"label\": \"fake\"}\n</code></pre> <ul> <li>Balanced classes \u2014 Equal or known ratio of real/fake</li> <li>Known ground truth \u2014 Labels are accurate</li> <li>Representative samples \u2014 Reflect intended use case</li> </ul>"},{"location":"evaluation/methodology/#standard-evaluation","title":"Standard Evaluation","text":"<pre><code>python scripts/evaluate_llms.py \\\n    --dataset data/benchmark.jsonl \\\n    --models gpt-5.1 \\\n    --tools both \\\n    --temperature 0.0 \\\n    --output results.jsonl \\\n    --metrics-output metrics.json\n</code></pre>"},{"location":"evaluation/methodology/#multi-trial-evaluation","title":"Multi-Trial Evaluation","text":"<p>For variance estimation:</p> <pre><code>python scripts/evaluate_llms.py \\\n    --dataset data/benchmark.jsonl \\\n    --models gpt-5.1 \\\n    --trials 5 \\\n    --temperature 0.0\n</code></pre>"},{"location":"evaluation/methodology/#statistical-comparisons","title":"Statistical Comparisons","text":""},{"location":"evaluation/methodology/#mcnemar-test","title":"McNemar Test","text":"<p>For comparing tools vs no-tools on the same samples:</p> <pre><code># In summarize_results.py\n# Tests whether discordant pairs are equally distributed\n# H0: No difference between configurations\n# p &lt; 0.05 suggests significant difference\n</code></pre>"},{"location":"evaluation/methodology/#wilson-confidence-intervals","title":"Wilson Confidence Intervals","text":"<p>95% confidence intervals for accuracy and coverage:</p> <pre><code># Wilson score interval for binomial proportions\n# More accurate than normal approximation for small samples\n</code></pre>"},{"location":"evaluation/methodology/#calibration-diagnostics","title":"Calibration Diagnostics","text":""},{"location":"evaluation/methodology/#expected-calibration-error-ece","title":"Expected Calibration Error (ECE)","text":"<p>Measures how well confidence predicts accuracy:</p> <pre><code>ECE = \u03a3 (|bin_count| / n) \u00d7 |accuracy(bin) - confidence(bin)|\n</code></pre> <ul> <li>Low ECE \u2014 Confidence predicts accuracy well</li> <li>High ECE \u2014 Confidence is miscalibrated</li> </ul>"},{"location":"evaluation/methodology/#brier-score","title":"Brier Score","text":"<p>Mean squared error of probabilistic predictions:</p> <pre><code>Brier = (1/n) \u00d7 \u03a3 (confidence - correct)\u00b2\n</code></pre> <ul> <li>Lower is better</li> <li>Range: 0 (perfect) to 1 (worst)</li> </ul>"},{"location":"evaluation/methodology/#latency-considerations","title":"Latency Considerations","text":""},{"location":"evaluation/methodology/#cache-effects","title":"Cache Effects","text":"<p>Caching Confounds Latency</p> <p>When tool cache is enabled, latency may reflect cache hits (&lt;250ms) rather than actual computation. For valid latency comparisons:</p> <ul> <li>Disable caching: <code>--disable-tool-cache</code></li> <li>Or report cache-hit vs cache-miss separately</li> </ul>"},{"location":"evaluation/methodology/#timing-breakdown","title":"Timing Breakdown","text":"<p>Results include timing components:</p> <pre><code>{\n    \"timings\": {\n        \"vision_llm_seconds\": 3.2,\n        \"agent_graph_seconds\": 8.5,\n        \"total_seconds\": 12.1\n    }\n}\n</code></pre>"},{"location":"evaluation/methodology/#reproducibility-requirements","title":"Reproducibility Requirements","text":"<p>For scientifically valid evaluation:</p>"},{"location":"evaluation/methodology/#control-variables","title":"Control Variables","text":"Variable Recommendation Temperature <code>0.0</code> for determinism Model version Document exact model ID Prompt version Hash prompts, track changes Cache state Disable or document"},{"location":"evaluation/methodology/#record-provenance","title":"Record Provenance","text":"<p>Each result should include:</p> <ul> <li>Model identifiers (agent, vision, structuring)</li> <li>Prompt hashes</li> <li>Cache state</li> <li>Timestamp</li> </ul>"},{"location":"evaluation/methodology/#comparing-configurations","title":"Comparing Configurations","text":""},{"location":"evaluation/methodology/#valid-comparisons","title":"Valid Comparisons","text":"Comparison Requirements Model A vs Model B Same dataset, same mode Tools vs No-Tools Same model, same dataset Vision model variants Same agent model, same dataset"},{"location":"evaluation/methodology/#invalid-comparisons","title":"Invalid Comparisons","text":"Comparison Problem Different datasets Different difficulty Different sample sizes Statistical power differs Mixed cache states Latency not comparable Different prompts Different behavior"},{"location":"evaluation/methodology/#interpretation-guidelines","title":"Interpretation Guidelines","text":""},{"location":"evaluation/methodology/#what-high-accuracy-means","title":"What High Accuracy Means","text":"<ul> <li>System correctly classifies most images</li> <li>But: may be conservative (high abstention)</li> </ul>"},{"location":"evaluation/methodology/#what-high-coverage-means","title":"What High Coverage Means","text":"<ul> <li>System answers most questions</li> <li>But: may be less accurate on hard cases</li> </ul>"},{"location":"evaluation/methodology/#ideal-characteristics","title":"Ideal Characteristics","text":"<ul> <li>High accuracy_answered \u2014 When we answer, we're right</li> <li>Reasonable coverage \u2014 We answer most cases</li> <li>Low slip rates \u2014 Few fakes pass through</li> <li>Low false-flag rates \u2014 Few reals incorrectly flagged</li> </ul>"},{"location":"evaluation/methodology/#see-also","title":"See Also","text":"<ul> <li>Metrics Reference \u2014 Detailed metric definitions</li> <li>Benchmark Results \u2014 Current evaluation results</li> <li>Reproducibility \u2014 Reproduction guidelines</li> </ul>"},{"location":"evaluation/metrics/","title":"Metrics Reference","text":"<p>Complete definitions of all evaluation metrics computed by DF3.</p>"},{"location":"evaluation/metrics/#notation","title":"Notation","text":"Symbol Meaning N Total samples TP True Positives (fake correctly identified as fake) TN True Negatives (real correctly identified as real) FP False Positives (real incorrectly flagged as fake) FN False Negatives (fake incorrectly passed as real) N_fake Total fake samples in dataset N_real Total real samples in dataset answered Samples with prediction in abstain Samples with prediction = uncertain"},{"location":"evaluation/metrics/#core-metrics","title":"Core Metrics","text":""},{"location":"evaluation/metrics/#accuracy","title":"accuracy","text":"<p>Overall accuracy including abstentions as wrong.</p> \\[\\text{accuracy} = \\frac{TP + TN}{N}\\] <ul> <li>Abstentions and errors count as incorrect</li> <li>Use for overall triage performance</li> <li>Lower bound on system capability</li> </ul>"},{"location":"evaluation/metrics/#accuracy_answered","title":"accuracy_answered","text":"<p>Accuracy on samples where the system provided an answer.</p> \\[\\text{accuracy_answered} = \\frac{TP + TN}{\\text{answered}}\\] <ul> <li>Only considers real/fake predictions</li> <li>Measures decision quality when confident</li> <li>Higher than overall accuracy</li> </ul>"},{"location":"evaluation/metrics/#coverage","title":"coverage","text":"<p>Fraction of samples where system provided an answer.</p> \\[\\text{coverage} = \\frac{\\text{answered}}{N}\\] <ul> <li>Range: 0 to 1</li> <li>Higher = more answers</li> <li>Trade-off with accuracy_answered</li> </ul>"},{"location":"evaluation/metrics/#class-specific-metrics","title":"Class-Specific Metrics","text":""},{"location":"evaluation/metrics/#fake-class-positive","title":"Fake Class (Positive)","text":""},{"location":"evaluation/metrics/#precision_fake","title":"precision_fake","text":"<p>When predicting \"fake\", how often correct?</p> \\[\\text{precision_fake} = \\frac{TP}{TP + FP}\\]"},{"location":"evaluation/metrics/#recall_fake","title":"recall_fake","text":"<p>What fraction of fakes are caught?</p> \\[\\text{recall_fake} = \\frac{TP}{TP + FN}\\]"},{"location":"evaluation/metrics/#f1_fake","title":"f1_fake","text":"<p>Harmonic mean of precision and recall.</p> \\[\\text{f1_fake} = \\frac{2 \\times \\text{precision_fake} \\times \\text{recall_fake}}{\\text{precision_fake} + \\text{recall_fake}}\\]"},{"location":"evaluation/metrics/#real-class-negative","title":"Real Class (Negative)","text":""},{"location":"evaluation/metrics/#precision_real","title":"precision_real","text":"<p>When predicting \"real\", how often correct?</p> \\[\\text{precision_real} = \\frac{TN}{TN + FN}\\]"},{"location":"evaluation/metrics/#recall_real","title":"recall_real","text":"<p>What fraction of reals are correctly passed?</p> \\[\\text{recall_real} = \\frac{TN}{TN + FP}\\]"},{"location":"evaluation/metrics/#f1_real","title":"f1_real","text":"<p>Harmonic mean for real class.</p> \\[\\text{f1_real} = \\frac{2 \\times \\text{precision_real} \\times \\text{recall_real}}{\\text{precision_real} + \\text{recall_real}}\\]"},{"location":"evaluation/metrics/#balanced-metrics","title":"Balanced Metrics","text":""},{"location":"evaluation/metrics/#balanced_accuracy","title":"balanced_accuracy","text":"<p>Average of class-specific true positive rates.</p> \\[\\text{balanced_accuracy} = \\frac{TPR_{fake} + TPR_{real}}{2}\\] <p>Where: - \\(TPR_{fake} = \\text{recall_fake}\\) - \\(TPR_{real} = \\text{recall_real}\\)</p> <p>Useful when classes are imbalanced.</p>"},{"location":"evaluation/metrics/#mcc-matthews-correlation-coefficient","title":"MCC (Matthews Correlation Coefficient)","text":"<p>Balanced measure that accounts for all confusion matrix elements.</p> \\[MCC = \\frac{TP \\times TN - FP \\times FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}\\] <ul> <li>Range: -1 to +1</li> <li>0 = random guessing</li> <li>+1 = perfect prediction</li> <li>-1 = perfect inverse prediction</li> </ul>"},{"location":"evaluation/metrics/#triage-metrics","title":"Triage Metrics","text":""},{"location":"evaluation/metrics/#fake_slip_rate","title":"fake_slip_rate","text":"<p>Fraction of fakes that slip through as \"real\".</p> \\[\\text{fake_slip_rate} = \\frac{FN}{N_{fake}}\\] <ul> <li>Critical metric for fraud detection</li> <li>Lower is better</li> <li>Does NOT include abstentions (they go to review)</li> </ul>"},{"location":"evaluation/metrics/#real_false_flag_rate","title":"real_false_flag_rate","text":"<p>Fraction of reals incorrectly flagged as \"fake\".</p> \\[\\text{real_false_flag_rate} = \\frac{FP}{N_{real}}\\] <ul> <li>Measures false alarm rate</li> <li>Lower is better</li> <li>Impacts user trust</li> </ul>"},{"location":"evaluation/metrics/#fake_catch_rate","title":"fake_catch_rate","text":"<p>Fraction of fakes correctly identified.</p> \\[\\text{fake_catch_rate} = \\frac{TP}{N_{fake}}\\] <ul> <li>Same as recall_fake</li> <li>Higher is better</li> </ul>"},{"location":"evaluation/metrics/#real_pass_rate","title":"real_pass_rate","text":"<p>Fraction of reals correctly passed.</p> \\[\\text{real_pass_rate} = \\frac{TN}{N_{real}}\\] <ul> <li>Same as recall_real</li> <li>Higher is better</li> </ul>"},{"location":"evaluation/metrics/#abstention-metrics","title":"Abstention Metrics","text":""},{"location":"evaluation/metrics/#abstain_rate","title":"abstain_rate","text":"<p>Overall rate of uncertain verdicts.</p> \\[\\text{abstain_rate} = \\frac{\\text{abstain}}{N}\\]"},{"location":"evaluation/metrics/#abstain_rate_fake","title":"abstain_rate_fake","text":"<p>Abstention rate on fake samples.</p> \\[\\text{abstain_rate_fake} = \\frac{\\text{abstain_fake}}{N_{fake}}\\]"},{"location":"evaluation/metrics/#abstain_rate_real","title":"abstain_rate_real","text":"<p>Abstention rate on real samples.</p> \\[\\text{abstain_rate_real} = \\frac{\\text{abstain_real}}{N_{real}}\\]"},{"location":"evaluation/metrics/#coverage_fake-coverage_real","title":"coverage_fake / coverage_real","text":"<p>Class-conditional coverage.</p> \\[\\text{coverage_fake} = 1 - \\text{abstain_rate_fake}\\]"},{"location":"evaluation/metrics/#calibration-metrics","title":"Calibration Metrics","text":""},{"location":"evaluation/metrics/#ece-expected-calibration-error","title":"ECE (Expected Calibration Error)","text":"<p>How well does confidence predict accuracy?</p> \\[ECE = \\sum_{b=1}^{B} \\frac{|B_b|}{N} \\times |acc(B_b) - conf(B_b)|\\] <p>Where: - B = number of bins (default: 10) - \\(B_b\\) = samples in bin b - \\(acc(B_b)\\) = accuracy of samples in bin - \\(conf(B_b)\\) = average confidence in bin</p> <p>Interpretation:</p> ECE Interpretation &lt; 0.05 Well calibrated 0.05 - 0.15 Moderately calibrated &gt; 0.15 Poorly calibrated"},{"location":"evaluation/metrics/#brier-score","title":"Brier Score","text":"<p>Mean squared error of probabilistic predictions.</p> \\[\\text{Brier} = \\frac{1}{N} \\sum_{i=1}^{N} (p_i - o_i)^2\\] <p>Where: - \\(p_i\\) = predicted probability of correct class - \\(o_i\\) = 1 if correct, 0 if incorrect</p>"},{"location":"evaluation/metrics/#latency-metrics","title":"Latency Metrics","text":""},{"location":"evaluation/metrics/#avg_latency_seconds","title":"avg_latency_seconds","text":"<p>Average end-to-end latency per sample.</p>"},{"location":"evaluation/metrics/#timing-components","title":"Timing Components","text":"Component Description vision_llm_seconds Time for vision analysis agent_graph_seconds Time for agent reasoning + tools total_seconds End-to-end time"},{"location":"evaluation/metrics/#percentiles","title":"Percentiles","text":"<ul> <li>p50 \u2014 Median latency</li> <li>p95 \u2014 95<sup>th</sup> percentile latency</li> </ul>"},{"location":"evaluation/metrics/#tool-usage-metrics","title":"Tool Usage Metrics","text":""},{"location":"evaluation/metrics/#avg_tool_count","title":"avg_tool_count","text":"<p>Average number of tools invoked per sample.</p>"},{"location":"evaluation/metrics/#avg_tool_seconds_total","title":"avg_tool_seconds_total","text":"<p>Average total time spent in tools per sample.</p>"},{"location":"evaluation/metrics/#statistical-measures","title":"Statistical Measures","text":""},{"location":"evaluation/metrics/#wilson-95-ci","title":"Wilson 95% CI","text":"<p>Confidence interval for proportions:</p> \\[p \\pm z \\sqrt{\\frac{p(1-p)}{n}}\\] <p>With Wilson score correction for small samples.</p>"},{"location":"evaluation/metrics/#standard-deviation-multi-trial","title":"Standard Deviation (Multi-Trial)","text":"<p>For multi-trial evaluations:</p> \\[\\sigma = \\sqrt{\\frac{\\sum (x_i - \\bar{x})^2}{n-1}}\\]"},{"location":"evaluation/metrics/#metric-selection-guide","title":"Metric Selection Guide","text":""},{"location":"evaluation/metrics/#for-overall-performance","title":"For Overall Performance","text":"<ul> <li>accuracy_answered \u2014 Quality of confident predictions</li> <li>coverage \u2014 How often system answers</li> <li>balanced_accuracy \u2014 Account for class imbalance</li> </ul>"},{"location":"evaluation/metrics/#for-safetyrisk","title":"For Safety/Risk","text":"<ul> <li>fake_slip_rate \u2014 Fakes that escape detection</li> <li>real_false_flag_rate \u2014 Reals incorrectly flagged</li> <li>abstain_rate \u2014 How much goes to human review</li> </ul>"},{"location":"evaluation/metrics/#for-model-comparison","title":"For Model Comparison","text":"<ul> <li>accuracy \u2014 Overall ranking</li> <li>f1_fake \u2014 Fake detection capability</li> <li>MCC \u2014 Balanced comparison</li> </ul>"},{"location":"evaluation/metrics/#for-deployment-decisions","title":"For Deployment Decisions","text":"<ul> <li>avg_latency_seconds \u2014 System responsiveness</li> <li>coverage \u2014 Automation potential</li> <li>accuracy_answered \u2014 Trust in answers</li> </ul>"},{"location":"evaluation/metrics/#see-also","title":"See Also","text":"<ul> <li>Methodology \u2014 Evaluation framework</li> <li>Benchmark Results \u2014 Current results</li> <li>Interpreting Results \u2014 Understanding outputs</li> </ul>"},{"location":"evaluation/results/","title":"Benchmark Results (Complete)","text":"<p>This page summarizes all evaluated configurations present in <code>results/</code>, derived from per-sample outputs (<code>results/*.jsonl</code>) and computed summaries (<code>artifacts/eval_summary.json</code>).</p> <p>Do not cherry-pick</p> <p>The inventory below is constructed programmatically from the <code>results/</code> folder, so it includes every run in this repository at the time of generation.</p> <p>Vision model vs agent model</p> <p>DF3 uses a vision model (for the initial vision step) and an agent model (for tool orchestration and synthesis). Some runs use a different vision model than the agent model.</p> <p>Where the per-sample <code>models.vision</code> field is missing, vision provenance is inferred from the filename conventions and run policy (documented in <code>artifacts/eval_summary.json</code>).</p> <p>Latency can be confounded by cache</p> <p>Some runs have near-zero vision latency due to cache hits. Treat latency comparisons as valid only when cache state is controlled.</p>"},{"location":"evaluation/results/#example-dataset-sample-image-model-output","title":"Example dataset sample (image + model output)","text":"<p>Below is a single real dataset example from a tools-enabled run, showing the input image, ground-truth label, and the model's full narrative output.</p> <p></p> <p>Sample metadata</p> <ul> <li><code>id</code>: <code>sample-281</code></li> <li><code>image</code>: <code>data2/sample-281.jpeg</code></li> <li><code>label</code> (ground truth): <code>real</code></li> <li><code>model</code>: <code>google/gemini-3-flash-preview</code> (tools mode)</li> <li><code>prediction</code>: <code>real</code></li> <li><code>confidence</code>: <code>0.95</code></li> <li><code>tools used</code>: <code>metadata</code>, <code>perform_trufor</code>, <code>perform_ela</code></li> </ul> <p>Structured output (parsed)</p> <pre><code>{\n  \"verdict\": \"real\",\n  \"confidence\": 0.95,\n  \"rationale\": \"Low manipulation probabilities (TruFor 0.143, ELA below threshold), consistent lighting, textures, and physical details (wood grain, rivets, stain) support authenticity. The visible iStockphoto watermark explains missing original metadata; low resolution is a noted limitation but does not outweigh the forensic indicators.\",\n  \"visual_description\": \"A wine cellar/storage area with several oak barrels; two barrels rest on white metal racks in the foreground with a reddish wine stain on one. Additional barrels are stacked in the background. Soft directional indoor lighting; an iStockphoto watermark is overlaid across the center.\",\n  \"forensic_summary\": \"Tools used: Metadata analysis (EXIF/C2PA), TruFor manipulation score, Error Level Analysis (ELA), and image specification inspection.\"\n}\n</code></pre> Full model output (raw_analysis)"},{"location":"evaluation/results/#observations","title":"Observations","text":"<p>The image depicts a wine cellar or storage area with several oak barrels. Two barrels are in the foreground, resting on white metal racks. The wood grain, metal hoops, and rivets appear physically consistent. A reddish wine stain is visible on the top of one barrel, which is a common real-world occurrence. In the background, other barrels are stacked vertically and horizontally. An \"iStockphoto\" watermark is prominently overlaid across the center. The lighting is soft and directional, consistent with indoor cellar lighting, and shadows align with the objects.</p>"},{"location":"evaluation/results/#tool-outputs","title":"Tool Outputs","text":"<ul> <li>Metadata: No EXIF data or C2PA manifests found. Contains an sRGB ICC profile.</li> <li>TruFor: Manipulation probability of 0.143, which is low, suggesting no significant splicing or localized editing.</li> <li>ELA: Anomaly score of 1.867. This is below the typical threshold of 2.0 for localized anomalies, indicating uniform compression levels across the image.</li> <li>Image Specs: Small resolution (380x252), which is typical for thumbnails or web previews.</li> </ul>"},{"location":"evaluation/results/#interpretation","title":"Interpretation","text":"<p>The visual evidence is highly consistent with an authentic photograph. The textures of the wood, the specific wear on the metal hoops, and the background depth of field appear natural. While the \"iStockphoto\" watermark is technically an \"addition\" to the original capture, it is a standard commercial overlay and not a \"forgery\" in the forensic sense. The low TruFor and ELA scores suggest the image structure is consistent and hasn't undergone malicious manipulation. There are no anatomical or physical \"AI artifacts\" (like garbled wood grain or impossible geometry) visible even upon close inspection.</p>"},{"location":"evaluation/results/#limitations","title":"Limitations","text":"<ul> <li>Low Resolution: The small image size (380x252) can mask fine-grained AI artifacts or subtle editing traces.</li> <li>Watermark: The presence of a watermark confirms the image has been processed/saved by a third party (iStock), which explains the lack of original camera metadata.</li> <li>Synthetic Quality: High-quality AI can occasionally produce simple scenes like this without obvious errors, though the specific \"stock photo\" look and consistent physics point toward a real source.</li> </ul>"},{"location":"evaluation/results/#conclusion","title":"Conclusion","text":"<p>The image shows no signs of AI generation or forensic manipulation. The presence of a known commercial watermark and the consistent physical details (stains, rivets, wood grain) strongly support its authenticity as a standard stock photograph.</p> <p>Verdict: real</p> <p>Confidence (0-1): 0.95</p>"},{"location":"evaluation/results/#key-findings-high-level","title":"Key findings (high level)","text":"<ul> <li>n=500 runs (digest <code>f987165daff0de70</code>): highest overall triage performance from Gemini 3 Flash Preview in vision-only mode (high accuracy with high coverage).</li> <li>n=200 runs (digest <code>1f78e35118013ed4</code>): same dataset with limit=200; evaluated only in tools mode for available runs.</li> <li>Abstention dominates some configurations: for some models, \"accuracy when answered\" can be high even though overall accuracy is low due to abstaining most of the time.</li> <li>Tool usage statistics are descriptive: differences between \"tool used\" vs \"not used\" are confounded by difficulty and are not causal.</li> </ul>"},{"location":"evaluation/results/#complete-model-inventory-all-configs-found","title":"Complete model inventory (all configs found)","text":"<p>The tables below include every model/configuration found in <code>results/*.jsonl</code>, including agent model, vision model, dataset, and mode.</p>"},{"location":"evaluation/results/#complete-model-inventory-derived-from-results","title":"Complete Model Inventory (derived from results/*)","text":"<p>This table is generated from <code>artifacts/eval_summary.json</code> (itself computed from <code>results/*.jsonl</code>).</p>"},{"location":"evaluation/results/#sample-limit-n200-digest-1f78e35118013ed4","title":"Sample limit: n=200 (digest <code>1f78e35118013ed4</code>)","text":"Agent Model Vision Model Mode JSONL n Acc Cov Acc(ans) MCC(ans) Abstain FakeSlip RealFalseFlag Lat_p50(s) Lat_p95(s) xiaomi/mimo-v2-flash:free google/gemini-3-flash-preview tools G3vision_agent_mimov2flash.jsonl 200 0.650 0.860 0.756 0.558 0.140 0.408 0.020 44.373 72.020 x-ai/grok-4.1-fast google/gemini-3-flash-preview tools G3vision_agent_grok41fast.jsonl 200 0.505 0.605 0.835 0.611 0.390 0.112 0.088 40.101 55.719 anthropic/claude-sonnet-4.5 anthropic/claude-sonnet-4.5 tools B_or_sonnet45.jsonl 200 0.475 0.600 0.792 0.578 0.400 0.112 0.137 54.979 70.626 deepseek/deepseek-v3.2 google/gemini-3-flash-preview tools G3vision_agent_deepseekv32.jsonl 200 0.470 0.605 0.777 0.555 0.390 0.184 0.088 59.138 289.982 moonshotai/kimi-k2-thinking:nitro google/gemini-3-flash-preview tools G3vision_agent_kimi_k2.jsonl 200 0.295 0.355 0.831 0.652 0.630 0.051 0.069 32.391 52.559"},{"location":"evaluation/results/#sample-limit-n500-digest-f987165daff0de70","title":"Sample limit: n=500 (digest <code>f987165daff0de70</code>)","text":"Agent Model Vision Model Mode JSONL n Acc Cov Acc(ans) MCC(ans) Abstain FakeSlip RealFalseFlag Lat_p50(s) Lat_p95(s) google/gemini-3-flash-preview google/gemini-3-flash-preview no-tools G3visiononly_notools.jsonl 500 0.928 0.984 0.943 0.890 0.016 0.101 0.012 6.049 8.349 z-ai/glm-4.7:nitro google/gemini-3-flash-preview no-tools G3vision_agent_glm47.jsonl 500 0.914 0.980 0.933 0.869 0.020 0.117 0.016 0.029 0.039 z-ai/glm-4.6:nitro z-ai/glm-4.6:nitro no-tools Glm46v-vision_agent_glm46.jsonl 500 0.418 0.636 0.657 0.222 0.364 0.441 0.000 0.012 0.026 gpt-5-mini gpt-5.2 no-tools A_openai_gpt5mini_v52.jsonl 500 0.074 0.078 0.949 0.000 0.922 0.000 0.008 0.027 0.038 gpt-5.2 gpt-5.2 no-tools A_openai_gpt52_visiononly.jsonl 500 0.074 0.078 0.949 0.000 0.922 0.000 0.008 0.037 0.048 google/gemini-3-flash-preview google/gemini-3-flash-preview tools B_or_g3flashprev.jsonl 500 0.782 0.934 0.837 0.674 0.062 0.166 0.138 33.320 47.370 z-ai/glm-4.7:nitro google/gemini-3-flash-preview tools G3vision_agent_glm47.jsonl 500 0.456 0.508 0.898 0.793 0.490 0.040 0.063 34.514 55.146 z-ai/glm-4.6:nitro z-ai/glm-4.6:nitro tools Glm46v-vision_agent_glm46.jsonl 500 0.448 0.750 0.597 0.142 0.232 0.587 0.024 32.253 49.041 gpt-5-mini gpt-5.2 tools A_openai_gpt5mini_v52.jsonl 500 0.240 0.294 0.816 0.659 0.700 0.008 0.099 88.935 128.741 gpt-5.2 gpt-5.2 tools A_openai_gpt52_e2e_tools.jsonl 500 0.080 0.112 0.714 0.332 0.888 0.000 0.063 35.935 57.543"},{"location":"evaluation/results/#tool-usage-all-tools-runs-descriptive-non-causal","title":"Tool usage (all tools-runs; descriptive, non-causal)","text":"<p>Tool selection is confounded by image difficulty; do not interpret deltas as causal.</p> Tool Calls UsedRate Acc(ans)Used Acc(ans)NotUsed DeltaAcc(ans) perform_trufor 2760 0.789 0.827 0.664 0.164 metadata 2229 0.637 0.842 0.694 0.148 perform_ela 2066 0.590 0.823 0.746 0.077 extract_residuals 928 0.265 0.830 0.760 0.069 execute_python_code 298 0.085 0.808 0.774 0.034 detect_jpeg_quantization 45 0.013 0.800 0.778 0.022 analyze_frequency_domain 29 0.008 0.500 0.779 -0.279 analyze_jpeg_compression 14 0.004 1.000 0.778 0.222"},{"location":"evaluation/results/#reproducibility","title":"Reproducibility","text":"<p>To recompute the derived JSON summary from <code>results/</code>:</p> <pre><code>python scripts/summarize_results.py --results-dir results --out artifacts/eval_summary.json --out-md markdown_docs/evaluation_report.generated.md\n</code></pre>"},{"location":"evaluation/results/#see-also","title":"See also","text":"<ul> <li>Methodology</li> <li>Metrics reference</li> <li>Dataset provenance</li> </ul>"},{"location":"getting-started/first-analysis/","title":"Your First Analysis","text":"<p>A detailed walkthrough of analyzing an image with DF3, explaining each step and output.</p>"},{"location":"getting-started/first-analysis/#sample-analysis","title":"Sample Analysis","text":"<p>Let's analyze an example image step by step. We'll use an image from the <code>example_images/</code> folder.</p>"},{"location":"getting-started/first-analysis/#running-the-analysis","title":"Running the Analysis","text":"<pre><code>python scripts/analyze_image.py --image example_images/example1.jpg\n</code></pre>"},{"location":"getting-started/first-analysis/#what-happens-behind-the-scenes","title":"What Happens Behind the Scenes","text":"<pre><code>sequenceDiagram\n    participant User\n    participant Script as analyze_image.py\n    participant Agent as ForensicAgent\n    participant Vision as Vision LLM\n    participant Tools as Forensic Tools\n    participant BAML as BAML Structuring\n\n    User-&gt;&gt;Script: --image example1.jpg\n    Script-&gt;&gt;Agent: analyze(image_path)\n    Agent-&gt;&gt;Vision: Analyze image visually\n    Vision--&gt;&gt;Agent: Visual description + initial verdict\n    Agent-&gt;&gt;Tools: TruFor analysis\n    Tools--&gt;&gt;Agent: manipulation_probability: 0.12\n    Agent-&gt;&gt;Tools: ELA analysis\n    Tools--&gt;&gt;Agent: ela_anomaly_score: 1.4\n    Agent-&gt;&gt;BAML: Structure the analysis\n    BAML--&gt;&gt;Agent: ForensicAnalysisResult\n    Agent--&gt;&gt;Script: Structured result\n    Script--&gt;&gt;User: Display verdict + reasoning</code></pre>"},{"location":"getting-started/first-analysis/#understanding-the-output","title":"Understanding the Output","text":"<p>Here's a complete sample output with annotations:</p> <pre><code>Initializing forensic agent with model: gpt-5.1...\n\nAnalyzing image: example_images/example1.jpg\n------------------------------------------------------------\n\n==============================================================\nANALYSIS RESULTS\n==============================================================\n\nVerdict: REAL                           # \u2190 Final determination\nConfidence: 0.82                        # \u2190 0-1 scale confidence score\n\nRationale: The image shows consistent characteristics of an \nauthentic photograph. Natural skin texture with visible pores, \nproper finger count and hand anatomy, coherent lighting with \nshadows matching the apparent light source, and no semantic \nanomalies detected. TruFor returned a low manipulation \nprobability (0.12), supporting authenticity.\n\nVisual Description: Outdoor portrait photograph of a middle-aged \nman wearing a blue polo shirt. Natural daylight illumination from \nthe upper left. Background shows out-of-focus trees and grass.\n\nTools used: perform_trufor, perform_ela\n\n==============================================================\nIMAGE AUTHENTICATION REPORT (SWGDE-STYLE)\n==============================================================\n[Detailed forensic report follows...]\n</code></pre>"},{"location":"getting-started/first-analysis/#verdict-values","title":"Verdict Values","text":"Verdict Meaning <code>REAL</code> Image appears to be an authentic photograph <code>FAKE</code> Image appears to be AI-generated or manipulated <code>UNCERTAIN</code> Evidence is insufficient or conflicting \u2014 needs human review"},{"location":"getting-started/first-analysis/#confidence-score","title":"Confidence Score","text":"<p>The confidence score (0.0 - 1.0) indicates the system's certainty:</p> Range Interpretation 0.8 - 1.0 High confidence \u2014 strong evidence supports the verdict 0.5 - 0.8 Medium confidence \u2014 some evidence, possible ambiguity 0.0 - 0.5 Low confidence \u2014 weak or conflicting evidence <p>Confidence is Self-Reported</p> <p>The confidence score is the LLM's self-assessment, not a calibrated probability. Use it for relative ranking, not as a precise likelihood.</p>"},{"location":"getting-started/first-analysis/#different-analysis-modes","title":"Different Analysis Modes","text":""},{"location":"getting-started/first-analysis/#vision-only-analysis","title":"Vision-Only Analysis","text":"<p>For faster analysis without forensic tools:</p> <pre><code>python scripts/analyze_image.py --image photo.jpg --no-tools\n</code></pre> <p>When to use:</p> <ul> <li>Quick screening of many images</li> <li>When tool latency is a concern</li> <li>For obvious AI-generated images (visual cues sufficient)</li> </ul> <p>Output differences:</p> <ul> <li>Faster execution (no tool calls)</li> <li>Relies entirely on visual analysis</li> <li><code>Tools used: (none)</code></li> </ul>"},{"location":"getting-started/first-analysis/#tool-augmented-analysis","title":"Tool-Augmented Analysis","text":"<p>Full analysis with forensic tools (default):</p> <pre><code>python scripts/analyze_image.py --image photo.jpg\n</code></pre> <p>When to use:</p> <ul> <li>High-stakes decisions</li> <li>Manipulated image detection (splicing, editing)</li> <li>When visual analysis is uncertain</li> </ul> <p>Available tools:</p> <ul> <li><code>perform_trufor</code> \u2014 Neural forgery detection</li> <li><code>perform_ela</code> \u2014 Error Level Analysis</li> <li><code>metadata</code> \u2014 EXIF/metadata extraction</li> <li><code>analyze_jpeg_compression</code> \u2014 JPEG artifact analysis</li> <li><code>detect_jpeg_quantization</code> \u2014 Quantization table analysis</li> <li><code>analyze_frequency_domain</code> \u2014 FFT/DCT frequency analysis</li> <li><code>extract_residuals</code> \u2014 DRUNet noise analysis</li> </ul>"},{"location":"getting-started/first-analysis/#asking-specific-questions","title":"Asking Specific Questions","text":"<p>You can ask specific questions about an image:</p> <pre><code>python scripts/analyze_image.py \\\n    --image photo.jpg \\\n    --query \"Is this person's face a deepfake?\"\n</code></pre> <p>The agent will focus its analysis on the specific question, examining:</p> <ul> <li>Face consistency and realism</li> <li>Boundary artifacts around the face</li> <li>Skin texture and lighting coherence</li> </ul>"},{"location":"getting-started/first-analysis/#working-with-different-image-formats","title":"Working with Different Image Formats","text":"<p>DF3 supports common image formats:</p> Format Support Notes JPEG (.jpg, .jpeg) \u2705 Full Best for ELA, JPEG tools PNG (.png) \u2705 Full JPEG-specific tools auto-skip WebP (.webp) \u2705 Partial Some tools may skip GIF (.gif) \u26a0\ufe0f Limited First frame only TIFF (.tiff) \u2705 Partial Large files may be slow <p>JPEG-Specific Tools</p> <p>Tools like <code>perform_ela</code> and <code>detect_jpeg_quantization</code> only apply to JPEG images. They automatically skip for other formats with an informative message.</p>"},{"location":"getting-started/first-analysis/#reading-the-forensic-report","title":"Reading the Forensic Report","text":"<p>The SWGDE-style report provides detailed documentation:</p> <pre><code>## Image Authentication Report\n\n**Examiner:** DF3 Automated Analysis\n**Date:** 2026-01-15\n**File:** example1.jpg\n\n### Evidence Information\n- File Size: 1.2 MB\n- Resolution: 3840 x 2160\n- Format: JPEG\n\n### Analysis Methods\n1. Visual Inspection (LLM-based)\n2. TruFor Forgery Detection\n3. Error Level Analysis\n\n### Observations\n[Detailed findings from each method...]\n\n### Conclusion\nBased on the analysis of both image content and image structure,\nthe image appears authentic with high confidence (0.82).\n\n### Limitations\n- Analysis performed on compressed version\n- No reference images available for comparison\n- Results should be verified by qualified examiner\n</code></pre>"},{"location":"getting-started/first-analysis/#saving-results","title":"Saving Results","text":""},{"location":"getting-started/first-analysis/#json-output","title":"JSON Output","text":"<p>Get machine-readable output:</p> <pre><code>from src.agents import ForensicAgent\n\nagent = ForensicAgent(llm_model=\"gpt-5.1\")\nresult = agent.analyze(\"photo.jpg\")\n\n# Result is a dictionary\nprint(result['verdict'])      # \"real\", \"fake\", or \"uncertain\"\nprint(result['confidence'])   # 0.0 - 1.0\nprint(result['rationale'])    # Explanation string\nprint(result['tool_usage'])   # List of tools used\n</code></pre>"},{"location":"getting-started/first-analysis/#result-fields","title":"Result Fields","text":"Field Type Description <code>verdict</code> string \"real\", \"fake\", or \"uncertain\" <code>confidence</code> float 0.0 - 1.0 confidence score <code>rationale</code> string Explanation of the verdict <code>visual_description</code> string Description of image contents <code>forensic_summary</code> string Summary of tool findings <code>tool_usage</code> list Names of tools invoked <code>tool_results</code> list Detailed tool outputs <code>timings</code> dict Execution timing breakdown <code>models</code> dict Models used (agent, vision, structuring)"},{"location":"getting-started/first-analysis/#next-steps","title":"Next Steps","text":"<p>Now that you've completed your first analysis:</p>"},{"location":"getting-started/first-analysis/#how-it-works","title":"How It Works","text":"<p>Understand the agent architecture and decision process.</p> <p>How It Works \u2192</p>"},{"location":"getting-started/first-analysis/#forensic-tools","title":"Forensic Tools","text":"<p>Learn what each tool detects and when to use it.</p> <p>Tools Overview \u2192</p>"},{"location":"getting-started/first-analysis/#batch-evaluation","title":"Batch Evaluation","text":"<p>Evaluate performance on labeled datasets.</p> <p>Batch Evaluation \u2192</p>"},{"location":"getting-started/first-analysis/#api-reference","title":"API Reference","text":"<p>Use DF3 programmatically in your applications.</p> <p>API Reference \u2192</p>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>Complete installation guide for DF3 including all dependencies and optional components.</p>"},{"location":"getting-started/installation/#system-requirements","title":"System Requirements","text":""},{"location":"getting-started/installation/#minimum-requirements","title":"Minimum Requirements","text":"Component Requirement Python 3.10 or later RAM 8 GB Storage 2 GB (including model weights) OS Windows 10+, macOS 12+, Linux (Ubuntu 20.04+)"},{"location":"getting-started/installation/#recommended-for-full-performance","title":"Recommended for Full Performance","text":"Component Recommendation Python 3.11 or 3.12 RAM 16 GB GPU NVIDIA GPU with 8GB+ VRAM (CUDA 11.8+) Storage SSD for faster model loading"},{"location":"getting-started/installation/#step-by-step-installation","title":"Step-by-Step Installation","text":""},{"location":"getting-started/installation/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/your-org/df3.git\ncd df3\n</code></pre>"},{"location":"getting-started/installation/#2-create-virtual-environment","title":"2. Create Virtual Environment","text":"Windows (PowerShell)Windows (Command Prompt)Linux/macOS <pre><code>python -m venv venv\n.\\venv\\Scripts\\Activate.ps1\n</code></pre> <pre><code>python -m venv venv\nvenv\\Scripts\\activate.bat\n</code></pre> <pre><code>python -m venv venv\nsource venv/bin/activate\n</code></pre>"},{"location":"getting-started/installation/#3-install-dependencies","title":"3. Install Dependencies","text":"<pre><code>pip install -r requirements.txt\n</code></pre> <p>This installs:</p> <ul> <li>LangChain/LangGraph \u2014 Agent orchestration</li> <li>BAML \u2014 Structured LLM outputs</li> <li>PyTorch \u2014 Neural network backend</li> <li>PIL/OpenCV \u2014 Image processing</li> <li>NumPy/SciPy \u2014 Numerical computation</li> </ul>"},{"location":"getting-started/installation/#4-configure-api-keys","title":"4. Configure API Keys","text":"<p>Create a <code>.env</code> file in the project root:</p> <pre><code># Primary: OpenAI API key (required for default configuration)\nOPENAI_API_KEY=sk-your-openai-key\n\n# Optional: Anthropic API key (for Claude models)\nANTHROPIC_API_KEY=sk-ant-your-anthropic-key\n\n# Optional: OpenRouter API key (for multi-provider access)\nOPENROUTER_API_KEY=sk-or-your-openrouter-key\n</code></pre> <p>API Key Security</p> <p>Never commit <code>.env</code> files to version control. The <code>.gitignore</code> file already excludes it.</p>"},{"location":"getting-started/installation/#5-generate-baml-client-if-needed","title":"5. Generate BAML Client (if needed)","text":"<p>If you modify BAML definitions in <code>baml_src/</code>, regenerate the client:</p> <pre><code>pip install baml-py\nbaml-cli generate\n</code></pre>"},{"location":"getting-started/installation/#6-verify-installation","title":"6. Verify Installation","text":"<pre><code>python scripts/analyze_image.py --image example_images/example1.jpg\n</code></pre>"},{"location":"getting-started/installation/#gpu-setup-optional-but-recommended","title":"GPU Setup (Optional but Recommended)","text":"<p>TruFor runs significantly faster on GPU. Follow these steps for CUDA support:</p>"},{"location":"getting-started/installation/#nvidia-gpu-cuda","title":"NVIDIA GPU (CUDA)","text":"<ol> <li>Install CUDA Toolkit (11.8 or later)</li> <li> <p>Download from NVIDIA CUDA Downloads</p> </li> <li> <p>Install cuDNN</p> </li> <li> <p>Download from NVIDIA cuDNN</p> </li> <li> <p>Install PyTorch with CUDA <pre><code>pip uninstall torch torchvision\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n</code></pre></p> </li> <li> <p>Verify CUDA is available <pre><code>import torch\nprint(torch.cuda.is_available())  # Should print: True\nprint(torch.cuda.get_device_name(0))  # Should print your GPU name\n</code></pre></p> </li> </ol>"},{"location":"getting-started/installation/#force-cpu-mode","title":"Force CPU Mode","text":"<p>If you encounter GPU issues, force CPU mode:</p> <pre><code># Environment variable\n$env:DF3_TRUFOR_DEVICE = \"cpu\"  # PowerShell\nexport DF3_TRUFOR_DEVICE=cpu    # Linux/macOS\n</code></pre>"},{"location":"getting-started/installation/#model-weights","title":"Model Weights","text":""},{"location":"getting-started/installation/#automatic-download","title":"Automatic Download","text":"<p>TruFor and DRUNet weights are automatically downloaded on first use. No manual action required.</p>"},{"location":"getting-started/installation/#manual-download-if-automatic-fails","title":"Manual Download (if automatic fails)","text":"<ol> <li>TruFor weights</li> <li>Download: TruFor_weights.zip</li> <li> <p>Extract to: <code>weights/trufor/trufor.pth.tar</code></p> </li> <li> <p>DRUNet weights</p> </li> <li>Automatically downloaded to: <code>src/tools/forensic/drunet/weights/</code></li> </ol>"},{"location":"getting-started/installation/#weight-storage-location","title":"Weight Storage Location","text":"<pre><code>df3/\n\u251c\u2500\u2500 weights/\n\u2502   \u2514\u2500\u2500 trufor/\n\u2502       \u2514\u2500\u2500 trufor.pth.tar      # ~180 MB\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 tools/\n\u2502       \u2514\u2500\u2500 forensic/\n\u2502           \u2514\u2500\u2500 drunet/\n\u2502               \u2514\u2500\u2500 weights/\n\u2502                   \u2514\u2500\u2500 drunet_gray.pth  # ~32 MB\n</code></pre>"},{"location":"getting-started/installation/#provider-configuration","title":"Provider Configuration","text":""},{"location":"getting-started/installation/#openai-default","title":"OpenAI (Default)","text":"<pre><code>OPENAI_API_KEY=sk-your-key-here\n</code></pre> <p>Supported models:</p> <ul> <li><code>gpt-5.1</code>, <code>gpt-5.2</code> \u2014 Latest GPT-5 family</li> <li><code>gpt-5-mini</code> \u2014 Faster, lower cost</li> <li><code>gpt-5</code> \u2014 Standard GPT-5</li> </ul>"},{"location":"getting-started/installation/#anthropic","title":"Anthropic","text":"<pre><code>ANTHROPIC_API_KEY=sk-ant-your-key-here\n</code></pre> <p>Supported models:</p> <ul> <li><code>claude-opus-4</code> \u2014 Highest capability</li> <li><code>claude-sonnet-4</code> \u2014 Balanced performance</li> <li><code>claude-3-5-haiku</code> \u2014 Fast and efficient</li> </ul>"},{"location":"getting-started/installation/#openrouter","title":"OpenRouter","text":"<p>Access 100+ models through a single API:</p> <pre><code>OPENROUTER_API_KEY=sk-or-your-key-here\n</code></pre> <p>Usage:</p> <pre><code>python scripts/analyze_image.py \\\n    --image photo.jpg \\\n    --model google/gemini-3-flash-preview \\\n    --provider openrouter \\\n    --base-url https://openrouter.ai/api/v1\n</code></pre>"},{"location":"getting-started/installation/#development-setup","title":"Development Setup","text":"<p>For contributing to DF3:</p>"},{"location":"getting-started/installation/#install-development-dependencies","title":"Install Development Dependencies","text":"<pre><code>pip install -r requirements.txt\npip install pytest pytest-cov ruff mypy\n</code></pre>"},{"location":"getting-started/installation/#run-tests","title":"Run Tests","text":"<pre><code>pytest tests/ -v\n</code></pre>"},{"location":"getting-started/installation/#code-formatting","title":"Code Formatting","text":"<pre><code>ruff check src/ scripts/\nruff format src/ scripts/\n</code></pre>"},{"location":"getting-started/installation/#type-checking","title":"Type Checking","text":"<pre><code>mypy src/\n</code></pre>"},{"location":"getting-started/installation/#docker-installation-alternative","title":"Docker Installation (Alternative)","text":"<pre><code>FROM python:3.11-slim\n\nWORKDIR /app\n\n# Install system dependencies\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    libgl1-mesa-glx \\\n    libglib2.0-0 \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Copy and install Python dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY . .\n\n# Generate BAML client\nRUN baml-cli generate\n\nENTRYPOINT [\"python\", \"scripts/analyze_image.py\"]\n</code></pre> <p>Build and run:</p> <pre><code>docker build -t df3 .\ndocker run -v /path/to/images:/images df3 --image /images/photo.jpg\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting-installation","title":"Troubleshooting Installation","text":"ModuleNotFoundError: No module named 'baml_client' <p>The BAML client needs to be generated: <pre><code>pip install baml-py\nbaml-cli generate\n</code></pre></p> CUDA out of memory <p>TruFor requires ~4GB VRAM. Solutions:</p> <ol> <li>Force CPU mode: <code>$env:DF3_TRUFOR_DEVICE = \"cpu\"</code></li> <li>Close other GPU applications</li> <li>Use a GPU with more VRAM</li> </ol> SSL Certificate errors <p>Update certificates: <pre><code>pip install --upgrade certifi\n</code></pre></p> <p>On macOS, also run: <pre><code>/Applications/Python\\ 3.x/Install\\ Certificates.command\n</code></pre></p> Pillow/OpenCV import errors <p>Reinstall with binary packages: <pre><code>pip uninstall pillow opencv-python\npip install pillow opencv-python-headless\n</code></pre></p> <p>See Troubleshooting for more solutions.</p>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start \u2014 Analyze your first image</li> <li>Your First Analysis \u2014 Detailed walkthrough</li> <li>Configuration \u2014 Customize DF3 behavior</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>Get up and running with DF3 in under 5 minutes.</p>"},{"location":"getting-started/quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+ \u2014 DF3 requires Python 3.10 or later</li> <li>API Key \u2014 OpenAI, Anthropic, or OpenRouter API key</li> <li>8GB+ RAM \u2014 For loading forensic tool models (TruFor, DRUNet)</li> <li>GPU (optional) \u2014 CUDA GPU accelerates TruFor inference</li> </ul>"},{"location":"getting-started/quickstart/#1-clone-and-install","title":"1. Clone and Install","text":"<pre><code># Clone the repository\ngit clone https://github.com/your-org/df3.git\ncd df3\n\n# Create virtual environment\npython -m venv venv\n\n# Activate (Windows PowerShell)\n.\\venv\\Scripts\\Activate.ps1\n\n# Activate (Linux/macOS)\nsource venv/bin/activate\n\n# Install dependencies\npip install -r requirements.txt\n</code></pre>"},{"location":"getting-started/quickstart/#2-configure-api-keys","title":"2. Configure API Keys","text":"<p>Create a <code>.env</code> file in the project root:</p> <pre><code># OpenAI (primary provider)\nOPENAI_API_KEY=sk-your-key-here\n\n# Optional: Anthropic for Claude models\nANTHROPIC_API_KEY=sk-ant-your-key-here\n\n# Optional: OpenRouter for multi-provider access\nOPENROUTER_API_KEY=sk-or-your-key-here\n</code></pre> <p>Or set environment variables directly:</p> Windows PowerShellLinux/macOS <pre><code>$env:OPENAI_API_KEY = \"sk-your-key-here\"\n</code></pre> <pre><code>export OPENAI_API_KEY=\"sk-your-key-here\"\n</code></pre>"},{"location":"getting-started/quickstart/#3-analyze-your-first-image","title":"3. Analyze Your First Image","text":"<pre><code>python scripts/analyze_image.py --image path/to/image.jpg\n</code></pre> <p>Example output:</p> <pre><code>Initializing forensic agent with model: gpt-5.1...\n\nAnalyzing image: path/to/image.jpg\n------------------------------------------------------------\n\n==============================================================\nANALYSIS RESULTS\n==============================================================\n\nVerdict: FAKE\nConfidence: 0.85\n\nRationale: Multiple indicators suggest AI generation: the subject's \nleft hand displays six fingers with unnatural joint positioning, \nskin texture appears uniformly smooth without visible pores, and \nbackground elements show characteristic \"melting\" artifacts where \nobjects blend into undefined shapes.\n\nVisual Description: Portrait of a woman in her 30s against a \nblurred outdoor background. Subject has brown hair and is smiling.\n\nTools used: perform_trufor, perform_ela\n\n==============================================================\n</code></pre>"},{"location":"getting-started/quickstart/#4-try-different-modes","title":"4. Try Different Modes","text":""},{"location":"getting-started/quickstart/#vision-only-faster","title":"Vision-Only (Faster)","text":"<p>Skip forensic tools for quick visual-only analysis:</p> <pre><code>python scripts/analyze_image.py --image photo.jpg --no-tools\n</code></pre>"},{"location":"getting-started/quickstart/#custom-query","title":"Custom Query","text":"<p>Ask a specific question about the image:</p> <pre><code>python scripts/analyze_image.py --image photo.jpg --query \"Is this a deepfake?\"\n</code></pre>"},{"location":"getting-started/quickstart/#different-model","title":"Different Model","text":"<p>Use a different LLM:</p> <pre><code>python scripts/analyze_image.py --image photo.jpg --model gpt-5-mini\n</code></pre>"},{"location":"getting-started/quickstart/#5-run-a-batch-evaluation","title":"5. Run a Batch Evaluation","text":"<p>Evaluate multiple images with ground truth labels:</p> <pre><code># Prepare a dataset file (JSONL format)\n# Each line: {\"id\": \"img1\", \"image\": \"path/to/img.jpg\", \"label\": \"real\"}\n\npython scripts/evaluate_llms.py \\\n    --dataset data/my_dataset.jsonl \\\n    --models gpt-5.1 \\\n    --tools both \\\n    --output results.jsonl\n</code></pre>"},{"location":"getting-started/quickstart/#whats-next","title":"What's Next?","text":""},{"location":"getting-started/quickstart/#full-installation-guide","title":"Full Installation Guide","text":"<p>Detailed setup instructions including GPU configuration.</p> <p>Installation \u2192</p>"},{"location":"getting-started/quickstart/#deep-dive","title":"Deep Dive","text":"<p>Understand how DF3 analyzes images.</p> <p>How It Works \u2192</p>"},{"location":"getting-started/quickstart/#forensic-tools","title":"Forensic Tools","text":"<p>Learn about each forensic tool and what it detects.</p> <p>Tools Reference \u2192</p>"},{"location":"getting-started/quickstart/#evaluation","title":"Evaluation","text":"<p>Run comprehensive benchmarks.</p> <p>Batch Evaluation \u2192</p>"},{"location":"getting-started/quickstart/#troubleshooting-quick-fixes","title":"Troubleshooting Quick Fixes","text":"TruFor weights not found <p>Weights are auto-downloaded on first use. If download fails:</p> <ol> <li>Manually download from TruFor releases</li> <li>Extract to <code>weights/trufor/trufor.pth.tar</code></li> </ol> Out of memory errors <p>TruFor requires significant GPU memory. Solutions:</p> <ul> <li>Set <code>DF3_TRUFOR_DEVICE=cpu</code> to use CPU (slower but works)</li> <li>Reduce image size before analysis</li> <li>Use <code>--no-tools</code> for vision-only analysis</li> </ul> API rate limits <p>If hitting rate limits:</p> <ul> <li>Add delays between calls in batch evaluation (<code>--num-workers 1</code>)</li> <li>Use a different model (smaller models have higher limits)</li> <li>Consider OpenRouter for load balancing</li> </ul> <p>See Troubleshooting for more solutions.</p>"},{"location":"guide/analyzing-images/","title":"Analyzing Images","text":"<p>Complete guide to analyzing images with DF3, including all options and advanced usage patterns.</p>"},{"location":"guide/analyzing-images/#command-line-interface","title":"Command-Line Interface","text":""},{"location":"guide/analyzing-images/#basic-usage","title":"Basic Usage","text":"<pre><code>python scripts/analyze_image.py --image path/to/image.jpg\n</code></pre>"},{"location":"guide/analyzing-images/#all-options","title":"All Options","text":"<pre><code>python scripts/analyze_image.py [OPTIONS]\n\nRequired:\n  --image PATH          Path to the image file to analyze\n\nOptional:\n  --model MODEL         LLM model to use (default: gpt-5.1)\n  --vision-model MODEL  Model for vision step (defaults to --model)\n  --structuring-model MODEL\n                        Model for structuring step (defaults to --model)\n  --temperature FLOAT   LLM temperature (default: 0.2)\n  --provider NAME       Provider for LLM calls: \"openai\" or \"openrouter\" (default: openai)\n  --api-key KEY         API key (defaults to provider env var)\n  --base-url URL        Optional API base URL (e.g., https://openrouter.ai/api/v1)\n  --referer TEXT        Optional HTTP-Referer header (OpenRouter)\n  --title TEXT          Optional X-Title header (OpenRouter)\n  --no-tools            Run vision-only analysis without forensic tools\n  --query TEXT          Specific question about the image\n</code></pre>"},{"location":"guide/analyzing-images/#examples","title":"Examples","text":"<pre><code># Standard analysis with tools\npython scripts/analyze_image.py --image suspicious.jpg\n\n# Vision-only (faster)\npython scripts/analyze_image.py --image photo.jpg --no-tools\n\n# Specific question\npython scripts/analyze_image.py --image portrait.jpg \\\n    --query \"Does this face show signs of deepfake manipulation?\"\n\n# Use different model\npython scripts/analyze_image.py --image photo.jpg --model gpt-5-mini\n\n# Use OpenRouter (multi-provider) from the CLI\npython scripts/analyze_image.py --image photo.jpg \\\n    --provider openrouter \\\n    --model google/gemini-3-flash-preview\n\n# Use different models for each step\npython scripts/analyze_image.py --image photo.jpg \\\n    --model gpt-5.1 \\\n    --vision-model gpt-5.2 \\\n    --structuring-model gpt-5-mini\n</code></pre>"},{"location":"guide/analyzing-images/#programmatic-api","title":"Programmatic API","text":""},{"location":"guide/analyzing-images/#basic-usage_1","title":"Basic Usage","text":"<pre><code>from src.agents import ForensicAgent\n\n# Initialize agent\nagent = ForensicAgent(llm_model=\"gpt-5.1\")\n\n# Analyze an image\nresult = agent.analyze(\"path/to/image.jpg\")\n\n# Access results\nprint(f\"Verdict: {result['verdict']}\")\nprint(f\"Confidence: {result['confidence']}\")\nprint(f\"Rationale: {result['rationale']}\")\n</code></pre>"},{"location":"guide/analyzing-images/#full-configuration","title":"Full Configuration","text":"<pre><code>from src.agents import ForensicAgent\n\nagent = ForensicAgent(\n    llm_model=\"gpt-5.1\",           # Model for agent/tool-calling\n    vision_model=\"gpt-5.2\",         # Model for vision analysis\n    structuring_model=\"gpt-5-mini\", # Model for output structuring\n    temperature=0.0,                # Deterministic output\n    api_key=\"sk-...\",               # Optional: override env var\n    base_url=None,                  # Optional: custom API endpoint\n    max_iterations=15,              # Max tool calls per analysis\n    enable_checkpointer=True,       # Enable LangGraph checkpointing\n)\n\n# Full analysis with tools\nresult = agent.analyze(\n    image_path=\"photo.jpg\",\n    user_query=\"Is this AI-generated?\",\n    use_tools=True,\n    pass_image_to_agent=False,  # Only pass text description to agent\n)\n</code></pre>"},{"location":"guide/analyzing-images/#result-structure","title":"Result Structure","text":"<pre><code>result = {\n    # Core verdict\n    \"verdict\": \"fake\",          # \"real\" | \"fake\" | \"uncertain\"\n    \"confidence\": 0.87,         # 0.0 - 1.0\n    \"rationale\": \"Multiple anatomical anomalies...\",\n\n    # Descriptions\n    \"visual_description\": \"Portrait of a woman...\",\n    \"forensic_summary\": \"TruFor: 0.12, ELA: 1.4...\",\n\n    # Tool information\n    \"tool_usage\": [\"perform_trufor\", \"perform_ela\"],\n    \"tool_details\": [\n        {\"tool\": \"perform_trufor\", \"seconds\": 2.3, \"status\": \"completed\"},\n        {\"tool\": \"perform_ela\", \"seconds\": 0.8, \"status\": \"completed\"},\n    ],\n    \"tool_results\": [\n        {\"tool\": \"perform_trufor\", \"parsed\": {\"manipulation_probability\": 0.12}},\n        {\"tool\": \"perform_ela\", \"parsed\": {\"ela_anomaly_score\": 1.4}},\n    ],\n\n    # Timing\n    \"timings\": {\n        \"vision_llm_seconds\": 3.2,\n        \"agent_graph_seconds\": 5.1,\n        \"total_seconds\": 8.7,\n    },\n\n    # Provenance\n    \"models\": {\n        \"agent\": \"gpt-5.1\",\n        \"vision\": \"gpt-5.2\",\n        \"structuring\": \"gpt-5-mini\",\n    },\n\n    # Raw outputs\n    \"raw_text\": \"### Visual Analysis\\n...\",\n    \"raw_parsed\": {...},\n    \"prompts\": {\n        \"vision_system\": \"...\",\n        \"vision_user\": \"...\",\n        \"agent_system\": \"...\",\n        \"agent_user\": \"...\",\n    },\n\n    # SWGDE-style report\n    \"report_markdown\": \"## Image Authentication Report\\n...\",\n}\n</code></pre>"},{"location":"guide/analyzing-images/#analysis-modes","title":"Analysis Modes","text":""},{"location":"guide/analyzing-images/#vision-only-mode","title":"Vision-Only Mode","text":"<p>Best for fast screening where tool latency is unacceptable.</p> <pre><code>result = agent.analyze(image_path, use_tools=False)\n</code></pre> <p>Characteristics:</p> <ul> <li>Faster (3-7 seconds vs 8-30 seconds)</li> <li>Relies entirely on visual analysis</li> <li>Best for obvious AI-generated images</li> <li>May miss subtle manipulations</li> </ul>"},{"location":"guide/analyzing-images/#tool-augmented-mode","title":"Tool-Augmented Mode","text":"<p>Best for high-stakes decisions requiring technical evidence.</p> <pre><code>result = agent.analyze(image_path, use_tools=True)\n</code></pre> <p>Characteristics:</p> <ul> <li>More thorough analysis</li> <li>Technical evidence supports verdict</li> <li>Better for manipulation detection</li> <li>Higher latency</li> </ul>"},{"location":"guide/analyzing-images/#working-with-different-providers","title":"Working with Different Providers","text":""},{"location":"guide/analyzing-images/#openai-default","title":"OpenAI (Default)","text":"<pre><code>import os\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\nagent = ForensicAgent(llm_model=\"gpt-5.1\")\n</code></pre>"},{"location":"guide/analyzing-images/#openrouter","title":"OpenRouter","text":"<p>Access 100+ models through a single API:</p> <pre><code>agent = ForensicAgent(\n    llm_model=\"google/gemini-3-flash-preview\",\n    api_key=os.environ[\"OPENROUTER_API_KEY\"],\n    base_url=\"https://openrouter.ai/api/v1\",\n)\n</code></pre>"},{"location":"guide/analyzing-images/#anthropic-via-openrouter","title":"Anthropic via OpenRouter","text":"<pre><code>agent = ForensicAgent(\n    llm_model=\"anthropic/claude-sonnet-4\",\n    api_key=os.environ[\"OPENROUTER_API_KEY\"],\n    base_url=\"https://openrouter.ai/api/v1\",\n)\n</code></pre>"},{"location":"guide/analyzing-images/#handling-different-image-types","title":"Handling Different Image Types","text":""},{"location":"guide/analyzing-images/#jpeg-images","title":"JPEG Images","text":"<p>Full tool support including compression analysis:</p> <pre><code># All tools available\nresult = agent.analyze(\"photo.jpg\")\n# ELA, JPEG quantization, compression analysis all work\n</code></pre>"},{"location":"guide/analyzing-images/#pngwebp-images","title":"PNG/WebP Images","text":"<p>JPEG-specific tools automatically skip:</p> <pre><code>result = agent.analyze(\"screenshot.png\")\n# ELA reports: \"skipped - JPEG-specific tool\"\n# TruFor, frequency analysis, residuals still work\n</code></pre>"},{"location":"guide/analyzing-images/#large-images","title":"Large Images","text":"<p>Large images may be slow or exceed memory:</p> <pre><code>from PIL import Image\n\n# Resize before analysis if needed\nimg = Image.open(\"huge_photo.jpg\")\nif max(img.size) &gt; 4096:\n    img.thumbnail((4096, 4096), Image.LANCZOS)\n    img.save(\"resized.jpg\", quality=95)\n    result = agent.analyze(\"resized.jpg\")\n</code></pre>"},{"location":"guide/analyzing-images/#error-handling","title":"Error Handling","text":""},{"location":"guide/analyzing-images/#common-errors","title":"Common Errors","text":"<pre><code>from src.agents import ForensicAgent\n\nagent = ForensicAgent()\n\ntry:\n    result = agent.analyze(\"nonexistent.jpg\")\nexcept FileNotFoundError as e:\n    print(f\"Image not found: {e}\")\n\ntry:\n    result = agent.analyze(\"corrupted.jpg\")\nexcept Exception as e:\n    print(f\"Analysis failed: {e}\")\n</code></pre>"},{"location":"guide/analyzing-images/#handling-uncertain-results","title":"Handling Uncertain Results","text":"<pre><code>result = agent.analyze(\"ambiguous_image.jpg\")\n\nif result[\"verdict\"] == \"uncertain\":\n    print(\"Image requires human review\")\n    print(f\"Reason: {result['rationale']}\")\n\n    # Queue for manual review\n    queue_for_review(\n        image_path=\"ambiguous_image.jpg\",\n        confidence=result[\"confidence\"],\n        evidence=result[\"tool_results\"],\n    )\n</code></pre>"},{"location":"guide/analyzing-images/#batch-processing","title":"Batch Processing","text":"<p>For analyzing multiple images, see Batch Evaluation.</p> <p>Quick example:</p> <pre><code>from pathlib import Path\nfrom src.agents import ForensicAgent\n\nagent = ForensicAgent()\nresults = []\n\nfor image_path in Path(\"images/\").glob(\"*.jpg\"):\n    result = agent.analyze(str(image_path))\n    results.append({\n        \"image\": str(image_path),\n        \"verdict\": result[\"verdict\"],\n        \"confidence\": result[\"confidence\"],\n    })\n\n# Process results\nfake_count = sum(1 for r in results if r[\"verdict\"] == \"fake\")\nprint(f\"Found {fake_count} fake images out of {len(results)}\")\n</code></pre>"},{"location":"guide/analyzing-images/#best-practices","title":"Best Practices","text":""},{"location":"guide/analyzing-images/#for-accurate-results","title":"For Accurate Results","text":"<ol> <li>Use original images when possible \u2014 avoid screenshots or re-saved copies</li> <li>Prefer JPEG for manipulation detection \u2014 compression tools are JPEG-specific</li> <li>Use tools for high-stakes decisions \u2014 vision-only may miss subtle manipulations</li> <li>Review uncertain results \u2014 the system is appropriately calibrated to say \"I don't know\"</li> </ol>"},{"location":"guide/analyzing-images/#for-performance","title":"For Performance","text":"<ol> <li>Use vision-only mode for fast screening</li> <li>Enable caching for repeated analyses of similar images</li> <li>Use smaller models (gpt-5-mini) for lower latency</li> <li>Batch similar images to amortize model loading time</li> </ol>"},{"location":"guide/analyzing-images/#for-reliability","title":"For Reliability","text":"<ol> <li>Set temperature to 0 for deterministic results</li> <li>Log full results including tool outputs for audit trails</li> <li>Preserve raw images \u2014 don't delete originals after analysis</li> <li>Document uncertain cases with full reasoning</li> </ol>"},{"location":"guide/analyzing-images/#next-steps","title":"Next Steps","text":"<ul> <li>Batch Evaluation \u2014 Process many images with metrics</li> <li>Interpreting Results \u2014 Understand what results mean</li> <li>Tool Reference \u2014 Deep dive into forensic tools</li> </ul>"},{"location":"guide/batch-evaluation/","title":"Batch Evaluation","text":"<p>Run systematic evaluations across datasets to measure DF3 performance with comprehensive metrics.</p>"},{"location":"guide/batch-evaluation/#overview","title":"Overview","text":"<p>The batch evaluator (<code>scripts/evaluate_llms.py</code>) enables:</p> <ul> <li>Testing multiple models side-by-side</li> <li>Comparing tools vs no-tools modes</li> <li>Computing accuracy, precision, recall, F1, and more</li> <li>Generating reproducible benchmarks</li> </ul>"},{"location":"guide/batch-evaluation/#dataset-format","title":"Dataset Format","text":"<p>Create a JSONL file with one record per line:</p> <pre><code>{\"id\": \"sample-001\", \"image\": \"images/photo1.jpg\", \"label\": \"real\"}\n{\"id\": \"sample-002\", \"image\": \"images/photo2.jpg\", \"label\": \"fake\"}\n{\"id\": \"sample-003\", \"image\": \"images/photo3.png\", \"label\": \"real\"}\n</code></pre>"},{"location":"guide/batch-evaluation/#required-fields","title":"Required Fields","text":"Field Type Description <code>id</code> string Unique identifier for the sample <code>image</code> string Path to image file (relative to JSONL or absolute) <code>label</code> string Ground truth: <code>\"real\"</code> or <code>\"fake\"</code>"},{"location":"guide/batch-evaluation/#optional-fields","title":"Optional Fields","text":"Field Type Description <code>meta</code> object Additional metadata (preserved in results)"},{"location":"guide/batch-evaluation/#basic-usage","title":"Basic Usage","text":"<pre><code>python scripts/evaluate_llms.py \\\n    --dataset data/eval_dataset.jsonl \\\n    --models gpt-5.1 \\\n    --tools both\n</code></pre>"},{"location":"guide/batch-evaluation/#command-line-options","title":"Command-Line Options","text":"<pre><code>Required:\n  --dataset PATH        Path to JSONL dataset file\n\nModel Options:\n  --models LIST         Comma-separated model names (e.g., gpt-5.1,gpt-5-mini)\n  --vision-model MODEL  Override vision step model\n  --structuring-model MODEL\n                        Override structuring step model\n\nTool Options:\n  --tools MODE          \"tools\", \"no-tools\", or \"both\" (default: both)\n\nExecution Options:\n  --temperature FLOAT   LLM temperature (default: 0.0)\n  --max-iterations INT  Max tool calls per sample (default: 15)\n  --trials INT          Number of trials per config (default: 1)\n  --num-workers INT     Parallel workers (default: 1)\n  --limit INT           Max samples to evaluate\n  --shuffle             Shuffle dataset before evaluation\n  --seed INT            Random seed for shuffling\n\nCache Options:\n  --enable-tool-cache   Enable caching (default: True)\n  --disable-tool-cache  Disable caching\n  --tool-cache-dir PATH Custom cache directory\n\nOutput Options:\n  --output PATH         Per-sample results file (default: eval_results.jsonl)\n  --metrics-output PATH Aggregated metrics file (default: eval_metrics.json)\n\nProvider Options:\n  --provider NAME       \"openai\" or \"openrouter\" (default: openai)\n  --api-key KEY         API key (overrides env var)\n  --base-url URL        Custom API endpoint\n</code></pre>"},{"location":"guide/batch-evaluation/#common-evaluation-scenarios","title":"Common Evaluation Scenarios","text":""},{"location":"guide/batch-evaluation/#compare-models","title":"Compare Models","text":"<p>Test multiple models on the same dataset:</p> <pre><code>python scripts/evaluate_llms.py \\\n    --dataset data/benchmark.jsonl \\\n    --models gpt-5.1,gpt-5-mini,gpt-5.2 \\\n    --tools both \\\n    --output results/model_comparison.jsonl\n</code></pre>"},{"location":"guide/batch-evaluation/#tools-vs-no-tools","title":"Tools vs No-Tools","text":"<p>Compare tool-augmented vs vision-only:</p> <pre><code>python scripts/evaluate_llms.py \\\n    --dataset data/benchmark.jsonl \\\n    --models gpt-5.1 \\\n    --tools both \\\n    --output results/tools_comparison.jsonl\n</code></pre>"},{"location":"guide/batch-evaluation/#multi-trial-evaluation","title":"Multi-Trial Evaluation","text":"<p>Run multiple trials to estimate variance:</p> <pre><code>python scripts/evaluate_llms.py \\\n    --dataset data/benchmark.jsonl \\\n    --models gpt-5.1 \\\n    --trials 5 \\\n    --temperature 0.0 \\\n    --output results/multi_trial.jsonl\n</code></pre>"},{"location":"guide/batch-evaluation/#parallel-evaluation","title":"Parallel Evaluation","text":"<p>Speed up with multiple workers (careful with rate limits):</p> <pre><code>python scripts/evaluate_llms.py \\\n    --dataset data/benchmark.jsonl \\\n    --models gpt-5.1 \\\n    --num-workers 4 \\\n    --output results/parallel.jsonl\n</code></pre>"},{"location":"guide/batch-evaluation/#vision-model-override","title":"Vision Model Override","text":"<p>Use different models for vision vs agent:</p> <pre><code>python scripts/evaluate_llms.py \\\n    --dataset data/benchmark.jsonl \\\n    --models gpt-5.1 \\\n    --vision-model gpt-5.2 \\\n    --structuring-model gpt-5-mini \\\n    --output results/mixed_models.jsonl\n</code></pre>"},{"location":"guide/batch-evaluation/#output-files","title":"Output Files","text":""},{"location":"guide/batch-evaluation/#per-sample-results-output","title":"Per-Sample Results (<code>--output</code>)","text":"<p>JSONL file with detailed results for each sample:</p> <pre><code>{\n  \"id\": \"sample-001\",\n  \"model\": \"gpt-5.1\",\n  \"use_tools\": true,\n  \"trial\": 0,\n  \"label\": \"fake\",\n  \"image\": \"/path/to/image.jpg\",\n  \"prediction\": \"fake\",\n  \"confidence\": 0.87,\n  \"rationale\": \"Multiple anatomical anomalies...\",\n  \"tool_usage\": [\"perform_trufor\", \"perform_ela\"],\n  \"tool_results\": [...],\n  \"latency_seconds\": 12.3,\n  \"timings\": {\n    \"vision_llm_seconds\": 3.2,\n    \"agent_graph_seconds\": 8.1,\n    \"total_seconds\": 12.3\n  },\n  \"models\": {\n    \"agent\": \"gpt-5.1\",\n    \"vision\": \"gpt-5.2\",\n    \"structuring\": \"gpt-5-mini\"\n  }\n}\n</code></pre>"},{"location":"guide/batch-evaluation/#aggregated-metrics-metrics-output","title":"Aggregated Metrics (<code>--metrics-output</code>)","text":"<p>JSON file with computed metrics per configuration:</p> <pre><code>{\n  \"_meta\": {\n    \"temperature\": 0.0,\n    \"max_iterations\": 15,\n    \"trials\": 1,\n    \"generated_at_unix\": 1736956800\n  },\n  \"gpt-5.1|tools\": {\n    \"per_trial\": {\n      \"0\": {\n        \"total\": 500,\n        \"correct\": 425,\n        \"accuracy\": 0.85,\n        \"accuracy_answered\": 0.89,\n        \"coverage\": 0.95,\n        \"precision_fake\": 0.88,\n        \"recall_fake\": 0.84,\n        \"f1_fake\": 0.86,\n        ...\n      }\n    },\n    \"summary\": {\n      \"mean\": {\"accuracy\": 0.85, ...},\n      \"std\": {\"accuracy\": 0.02, ...}\n    }\n  }\n}\n</code></pre>"},{"location":"guide/batch-evaluation/#understanding-metrics","title":"Understanding Metrics","text":""},{"location":"guide/batch-evaluation/#core-metrics","title":"Core Metrics","text":"Metric Formula Interpretation accuracy (TP + TN) / N Overall correct rate (abstentions count as wrong) accuracy_answered (TP + TN) / answered Correct rate among answered samples coverage answered / N Fraction of samples with verdict"},{"location":"guide/batch-evaluation/#class-specific-metrics","title":"Class-Specific Metrics","text":"Metric Description precision_fake TP / (TP + FP) \u2014 How often \"fake\" predictions are correct recall_fake TP / (TP + FN) \u2014 What fraction of fakes are caught f1_fake Harmonic mean of precision and recall for fake class precision_real TN / (TN + FN) \u2014 How often \"real\" predictions are correct recall_real TN / (TN + FP) \u2014 What fraction of reals are passed f1_real Harmonic mean for real class"},{"location":"guide/batch-evaluation/#triage-metrics","title":"Triage Metrics","text":"Metric Description fake_slip_rate FN / N_fake \u2014 Fakes incorrectly passed as real real_false_flag_rate FP / N_real \u2014 Reals incorrectly flagged as fake fake_catch_rate TP / N_fake \u2014 Fakes correctly caught real_pass_rate TN / N_real \u2014 Reals correctly passed abstain_rate N_uncertain / N \u2014 Rate of \"uncertain\" verdicts"},{"location":"guide/batch-evaluation/#balanced-metrics","title":"Balanced Metrics","text":"Metric Description balanced_accuracy (TPR_fake + TPR_real) / 2 \u2014 Balanced across classes avg_confidence Mean confidence score on answered samples"},{"location":"guide/batch-evaluation/#regenerating-tables","title":"Regenerating Tables","text":"<p>After evaluation, regenerate summary tables:</p> <pre><code>python scripts/summarize_results.py \\\n    --results-dir results \\\n    --out artifacts/eval_summary.json \\\n    --out-md markdown_docs/evaluation_report.generated.md\n</code></pre> <p>This produces:</p> <ul> <li>Machine-readable JSON with all metrics</li> <li>Markdown tables for documentation</li> <li>Paired statistical comparisons</li> </ul>"},{"location":"guide/batch-evaluation/#cache-management","title":"Cache Management","text":""},{"location":"guide/batch-evaluation/#enabledisable","title":"Enable/Disable","text":"<pre><code># With caching (default - faster repeat runs)\npython scripts/evaluate_llms.py --enable-tool-cache ...\n\n# Without caching (for latency measurements)\npython scripts/evaluate_llms.py --disable-tool-cache ...\n</code></pre>"},{"location":"guide/batch-evaluation/#cache-location","title":"Cache Location","text":"<p>Default: <code>.tool_cache/</code> in project root</p> <p>Override: <pre><code>python scripts/evaluate_llms.py --tool-cache-dir /path/to/cache ...\n</code></pre></p>"},{"location":"guide/batch-evaluation/#cache-statistics","title":"Cache Statistics","text":"<p>View cache status:</p> <pre><code>from src.tools.forensic.cache import get_cache\n\ncache = get_cache()\nstats = cache.get_stats()\nprint(f\"Entries: {stats['entry_count']}\")\nprint(f\"Size: {stats['total_size_mb']:.1f} MB\")\n</code></pre> <p>Caching and Latency</p> <p>When caching is enabled, reported latencies may reflect cache hits (&lt; 250ms) rather than actual model/tool execution time. Disable caching for accurate latency measurements.</p>"},{"location":"guide/batch-evaluation/#reproducibility","title":"Reproducibility","text":""},{"location":"guide/batch-evaluation/#for-reproducible-results","title":"For Reproducible Results","text":"<pre><code>python scripts/evaluate_llms.py \\\n    --dataset data/benchmark.jsonl \\\n    --models gpt-5.1 \\\n    --temperature 0.0 \\    # Deterministic\n    --seed 42 \\            # Fixed shuffle seed\n    --disable-tool-cache \\ # No caching\n    --output results/reproducible.jsonl\n</code></pre>"},{"location":"guide/batch-evaluation/#whats-recorded","title":"What's Recorded","text":"<p>Each result includes provenance:</p> <ul> <li>Model versions (agent, vision, structuring)</li> <li>Prompt hashes</li> <li>Run configuration</li> <li>Timing breakdown</li> <li>Tool outputs</li> </ul>"},{"location":"guide/batch-evaluation/#best-practices","title":"Best Practices","text":""},{"location":"guide/batch-evaluation/#for-accurate-benchmarks","title":"For Accurate Benchmarks","text":"<ol> <li>Use temperature 0.0 for deterministic results</li> <li>Disable caching when measuring latency</li> <li>Run multiple trials to estimate variance</li> <li>Use the same dataset for all comparisons</li> </ol>"},{"location":"guide/batch-evaluation/#for-large-evaluations","title":"For Large Evaluations","text":"<ol> <li>Start with <code>--limit</code> to test setup</li> <li>Use <code>--num-workers</code> carefully (watch rate limits)</li> <li>Enable caching for repeat runs</li> <li>Save intermediate results (default output files)</li> </ol>"},{"location":"guide/batch-evaluation/#for-valid-comparisons","title":"For Valid Comparisons","text":"<ol> <li>Same dataset for all configurations</li> <li>Same evaluation date (model versions may change)</li> <li>Document provenance from metrics output</li> <li>Report uncertainty from multi-trial runs</li> </ol>"},{"location":"guide/batch-evaluation/#example-evaluation-script","title":"Example Evaluation Script","text":"<pre><code>\"\"\"Complete evaluation workflow.\"\"\"\nimport json\nfrom pathlib import Path\nimport subprocess\n\n# Configuration\nDATASET = \"data/benchmark_500.jsonl\"\nMODELS = [\"gpt-5.1\", \"gpt-5-mini\"]\nOUTPUT_DIR = Path(\"results/2026-01-15\")\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\n# Run evaluation\nfor model in MODELS:\n    output_file = OUTPUT_DIR / f\"{model.replace('/', '_')}.jsonl\"\n    metrics_file = OUTPUT_DIR / f\"{model.replace('/', '_')}.metrics.json\"\n\n    cmd = [\n        \"python\", \"scripts/evaluate_llms.py\",\n        \"--dataset\", DATASET,\n        \"--models\", model,\n        \"--tools\", \"both\",\n        \"--temperature\", \"0.0\",\n        \"--disable-tool-cache\",\n        \"--output\", str(output_file),\n        \"--metrics-output\", str(metrics_file),\n    ]\n\n    print(f\"Evaluating {model}...\")\n    subprocess.run(cmd, check=True)\n\n# Summarize results\nsubprocess.run([\n    \"python\", \"scripts/summarize_results.py\",\n    \"--results-dir\", str(OUTPUT_DIR),\n    \"--out\", str(OUTPUT_DIR / \"summary.json\"),\n    \"--out-md\", str(OUTPUT_DIR / \"report.md\"),\n], check=True)\n\nprint(f\"Results saved to {OUTPUT_DIR}\")\n</code></pre>"},{"location":"guide/batch-evaluation/#next-steps","title":"Next Steps","text":"<ul> <li>Metrics Reference \u2014 Detailed metric definitions</li> <li>Interpreting Results \u2014 Understand what results mean</li> <li>Reproducibility \u2014 Research-grade evaluation</li> </ul>"},{"location":"guide/how-it-works/","title":"How DF3 Works","text":"<p>DF3 uses an agent-based architecture to analyze images for signs of AI generation or manipulation. This page explains the core concepts and decision-making process.</p>"},{"location":"guide/how-it-works/#the-core-philosophy","title":"The Core Philosophy","text":"<p>Traditional image forensics tools produce numeric scores that require expert interpretation. DF3 takes a different approach:</p> <ol> <li>Vision-First \u2014 The LLM examines the image directly, like a human expert would</li> <li>Tool-Augmented \u2014 Forensic tools provide technical evidence to support or challenge visual observations</li> <li>Transparent Reasoning \u2014 Every verdict includes explanations, not just scores</li> <li>Selective Classification \u2014 The system can say \"I'm not sure\" instead of guessing</li> </ol>"},{"location":"guide/how-it-works/#the-two-type-problem","title":"The Two-Type Problem","text":"<p>A key insight in DF3's design is that \"fake\" images come in two fundamentally different forms:</p>"},{"location":"guide/how-it-works/#manipulated-images","title":"Manipulated Images","text":"<p>Images that started as real photographs but were edited:</p> <ul> <li>Splicing \u2014 Pasting elements from other images</li> <li>Copy-move \u2014 Duplicating regions within the image</li> <li>Inpainting \u2014 AI-assisted removal or addition</li> <li>Retouching \u2014 Color, lighting, or feature alterations</li> </ul> <p>Detection approach: Forensic tools excel here. TruFor, ELA, and compression analysis detect local inconsistencies where edits were made.</p>"},{"location":"guide/how-it-works/#ai-generated-images","title":"AI-Generated Images","text":"<p>Images created entirely by AI generators:</p> <ul> <li>Text-to-image \u2014 DALL-E, Midjourney, Stable Diffusion</li> <li>GAN-generated \u2014 StyleGAN, ProGAN faces</li> <li>Deepfakes \u2014 Face swaps and reenactments</li> </ul> <p>Detection approach: Visual analysis is key. AI-generated images often appear internally consistent to forensic tools (no \"editing\" artifacts), but contain telltale visual anomalies.</p> <p>Why This Matters</p> <p>A low TruFor score does NOT mean an image is real \u2014 it means the image hasn't been edited. A fully synthetic image can score low on manipulation tools because it was never manipulated; it was generated from scratch.</p>"},{"location":"guide/how-it-works/#the-analysis-pipeline","title":"The Analysis Pipeline","text":"<pre><code>flowchart TB\n    subgraph Input\n        IMG[Image File]\n    end\n\n    subgraph Vision[\"Step 1: Vision Analysis\"]\n        direction TB\n        V1[Load Image]\n        V2[BAML Vision Function]\n        V3[Visual Description&lt;br/&gt;+ Initial Assessment]\n        V1 --&gt; V2 --&gt; V3\n    end\n\n    subgraph Decision[\"Tools Mode?\"]\n        D{use_tools?}\n    end\n\n    subgraph NoTools[\"No-Tools Path\"]\n        NT[Direct Structuring]\n    end\n\n    subgraph Tools[\"Step 2: Agentic Tool Use\"]\n        direction TB\n        A1[LangGraph ReAct Agent]\n        A2{Select Tool}\n        A3[Execute Tool]\n        A4[Interpret Results]\n        A5{More Tools&lt;br/&gt;Needed?}\n        A1 --&gt; A2\n        A2 --&gt; A3\n        A3 --&gt; A4\n        A4 --&gt; A5\n        A5 --&gt;|Yes| A2\n        A5 --&gt;|No| A6[Synthesize Findings]\n    end\n\n    subgraph Structure[\"Step 3: Structuring\"]\n        S1[BAML Structuring Function]\n        S2[ForensicAnalysisResult]\n        S1 --&gt; S2\n    end\n\n    IMG --&gt; Vision\n    Vision --&gt; Decision\n    Decision --&gt;|False| NoTools\n    Decision --&gt;|True| Tools\n    NoTools --&gt; Structure\n    Tools --&gt; Structure\n\n    style Vision fill:#4a1d96,color:#fff\n    style Tools fill:#b45309,color:#fff\n    style Structure fill:#065f46,color:#fff</code></pre>"},{"location":"guide/how-it-works/#step-1-vision-analysis","title":"Step 1: Vision Analysis","text":"<p>Every analysis starts with the vision step, regardless of whether tools are enabled.</p>"},{"location":"guide/how-it-works/#what-the-vision-llm-looks-for","title":"What the Vision LLM Looks For","text":"<p>The vision model examines the image for indicators of AI generation:</p>"},{"location":"guide/how-it-works/#anatomical-errors","title":"Anatomical Errors","text":"<ul> <li>Wrong number of fingers, teeth, or eyes</li> <li>Malformed hands or impossible poses</li> <li>Asymmetric or distorted facial features</li> </ul>"},{"location":"guide/how-it-works/#texture-anomalies","title":"Texture Anomalies","text":"<ul> <li>Unnaturally smooth skin lacking pores</li> <li>Repeating patterns in backgrounds</li> <li>\"Painted\" or \"waxy\" appearance</li> </ul>"},{"location":"guide/how-it-works/#lighting-inconsistencies","title":"Lighting Inconsistencies","text":"<ul> <li>Shadows pointing in different directions</li> <li>Missing or impossible reflections</li> <li>Light sources that don't match shadows</li> </ul>"},{"location":"guide/how-it-works/#semantic-impossibilities","title":"Semantic Impossibilities","text":"<ul> <li>Objects that don't make sense in context</li> <li>Garbled or nonsensical text</li> <li>Background elements that defy physics</li> </ul>"},{"location":"guide/how-it-works/#vision-output","title":"Vision Output","text":"<p>The vision step produces:</p> <pre><code>{\n    \"visual_description\": \"Portrait of a woman in her 30s...\",\n    \"synthesis_indicators\": \"Possible extra finger on left hand...\",\n    \"verdict\": \"fake\",\n    \"confidence\": 0.75,\n    \"rationale\": \"Multiple anatomical anomalies suggest...\"\n}\n</code></pre>"},{"location":"guide/how-it-works/#step-2-tool-augmented-analysis","title":"Step 2: Tool-Augmented Analysis","text":"<p>When <code>use_tools=True</code>, the LangGraph ReAct agent takes over.</p>"},{"location":"guide/how-it-works/#how-the-agent-decides","title":"How the Agent Decides","text":"<p>The agent receives: 1. The visual description from Step 1 2. The image path 3. Access to forensic tools</p> <p>It then reasons about which tools would be most useful:</p> <pre><code>Initial visual analysis suggests possible manipulation in the \nbackground region. I should:\n1. Run TruFor to check for manipulation probability\n2. Run ELA to look for localized compression anomalies\n</code></pre>"},{"location":"guide/how-it-works/#tool-selection-logic","title":"Tool Selection Logic","text":"Visual Observation Likely Tool Choice Possible splicing/compositing TruFor, ELA JPEG-looking image detect_jpeg_quantization, analyze_jpeg_compression Unusual textures/patterns extract_residuals, analyze_frequency_domain Metadata questions metadata Need custom analysis execute_python_code"},{"location":"guide/how-it-works/#tool-iteration","title":"Tool Iteration","text":"<p>The agent can call multiple tools, typically 1-3 per analysis:</p> <pre><code>sequenceDiagram\n    participant Agent\n    participant TruFor\n    participant ELA\n\n    Agent-&gt;&gt;TruFor: perform_trufor(image_path)\n    TruFor--&gt;&gt;Agent: manipulation_probability: 0.87\n\n    Note over Agent: High probability - check ELA for localization\n\n    Agent-&gt;&gt;ELA: perform_ela(image_path)\n    ELA--&gt;&gt;Agent: ela_anomaly_score: 3.2, localized in upper-left\n\n    Note over Agent: Consistent evidence - conclude FAKE</code></pre>"},{"location":"guide/how-it-works/#step-3-structuring","title":"Step 3: Structuring","text":"<p>The final step converts free-form reasoning into structured output.</p>"},{"location":"guide/how-it-works/#why-separate-structuring","title":"Why Separate Structuring?","text":"<p>Research shows that requiring structured output during reasoning can degrade LLM performance. DF3 uses a two-step approach:</p> <ol> <li>Reasoning \u2014 LLM thinks freely in natural language</li> <li>Structuring \u2014 Separate call extracts structured data</li> </ol> <p>This is implemented via BAML's <code>StructureForensicAnalysis</code> function.</p>"},{"location":"guide/how-it-works/#final-output-schema","title":"Final Output Schema","text":"<pre><code>class ForensicAnalysisResult:\n    verdict: Literal[\"real\", \"fake\", \"uncertain\"]\n    confidence: float  # 0.0 - 1.0\n    rationale: str     # Max 80 words\n    visual_description: str\n    forensic_summary: str\n    full_text: str     # Complete narrative\n</code></pre>"},{"location":"guide/how-it-works/#decision-logic","title":"Decision Logic","text":""},{"location":"guide/how-it-works/#when-to-say-fake","title":"When to Say \"FAKE\"","text":"<p>Strong fake signals (any of these):</p> <ul> <li>TruFor <code>manipulation_probability</code> &gt; 0.8</li> <li>Clear visual anomalies (wrong anatomy, impossible physics)</li> <li>High ELA anomaly scores with localized patterns</li> <li>Multiple corroborating signals</li> </ul>"},{"location":"guide/how-it-works/#when-to-say-real","title":"When to Say \"REAL\"","text":"<p>Strong real signals (need multiple):</p> <ul> <li>Natural appearance with no visual anomalies</li> <li>TruFor <code>manipulation_probability</code> &lt; 0.2</li> <li>Consistent lighting, physics, and anatomy</li> <li>Valid metadata with plausible provenance</li> </ul>"},{"location":"guide/how-it-works/#when-to-say-uncertain","title":"When to Say \"UNCERTAIN\"","text":"<p>Appropriate uncertainty:</p> <ul> <li>Mid-range tool scores (0.3 - 0.6)</li> <li>Conflicting evidence (visual says fake, tools say real)</li> <li>Low image quality prevents reliable analysis</li> <li>Evidence is genuinely ambiguous</li> </ul> <p>Uncertain is Valid</p> <p><code>uncertain</code> is not a failure \u2014 it's an appropriate response when evidence doesn't support a confident verdict. It enables intelligent triage to human reviewers.</p>"},{"location":"guide/how-it-works/#evidence-weighting","title":"Evidence Weighting","text":""},{"location":"guide/how-it-works/#visual-vs-tool-evidence","title":"Visual vs Tool Evidence","text":"<p>DF3 weighs evidence differently based on image type:</p> Evidence Source AI-Generated Manipulated Visual analysis Primary Supporting TruFor Supporting Primary ELA Neutral Primary Frequency analysis Supporting Supporting"},{"location":"guide/how-it-works/#conflicting-evidence","title":"Conflicting Evidence","text":"<p>When visual and tool evidence conflict:</p> <ol> <li>Visual says fake, tools say clean \u2192 Likely AI-generated (tools don't detect synthesis)</li> <li>Visual says real, tools say fake \u2192 Trust tools (they detect things humans miss)</li> <li>Both ambiguous \u2192 Report uncertain</li> </ol>"},{"location":"guide/how-it-works/#performance-considerations","title":"Performance Considerations","text":""},{"location":"guide/how-it-works/#latency-breakdown","title":"Latency Breakdown","text":"<p>Typical analysis times:</p> Step No-Tools Mode Tools Mode Vision LLM call 2-5s 2-5s Tool execution \u2014 3-15s Agent reasoning \u2014 2-8s Structuring 1-2s 1-2s Total 3-7s 8-30s"},{"location":"guide/how-it-works/#caching","title":"Caching","text":"<p>DF3 caches:</p> <ul> <li>Vision outputs \u2014 Keyed by image hash + model + prompt hash</li> <li>Tool outputs \u2014 Keyed by image hash + tool name + parameters</li> </ul> <p>Enable/disable via <code>--enable-tool-cache</code> / <code>--disable-tool-cache</code>.</p>"},{"location":"guide/how-it-works/#next-steps","title":"Next Steps","text":""},{"location":"guide/how-it-works/#forensic-tools","title":"Forensic Tools","text":"<p>Deep dive into each tool and what it detects.</p> <p>Tools Overview \u2192</p>"},{"location":"guide/how-it-works/#architecture","title":"Architecture","text":"<p>Technical details of the system implementation.</p> <p>System Overview \u2192</p>"},{"location":"guide/interpreting-results/","title":"Interpreting Results","text":"<p>How to read and understand DF3 analysis outputs.</p>"},{"location":"guide/interpreting-results/#verdict","title":"Verdict","text":"Verdict Meaning Typical Action REAL No manipulation/generation indicators Accept FAKE Manipulation or AI-generation detected Flag for review UNCERTAIN Insufficient or conflicting evidence Human review"},{"location":"guide/interpreting-results/#what-fake-covers","title":"What FAKE Covers","text":"<ul> <li>AI-generated images (DALL-E, Midjourney, etc.)</li> <li>Manipulated images (splicing, inpainting)</li> <li>Deepfakes (face swaps)</li> </ul> <p>The rationale often indicates which type was detected.</p>"},{"location":"guide/interpreting-results/#confidence-score","title":"Confidence Score","text":"Range Interpretation 0.8 - 1.0 High confidence 0.6 - 0.8 Medium confidence 0.4 - 0.6 Low confidence 0.0 - 0.4 Very low confidence <p>The confidence score is the LLM's self-assessment. Use it for relative ranking, not as a probability.</p>"},{"location":"guide/interpreting-results/#tool-scores","title":"Tool Scores","text":""},{"location":"guide/interpreting-results/#trufor","title":"TruFor","text":"<p><code>manipulation_probability</code> (0.0 - 1.0):</p> Range Interpretation 0.0 - 0.2 Low manipulation evidence 0.2 - 0.5 Some indicators 0.5 - 0.8 Moderate evidence 0.8 - 1.0 Strong manipulation evidence <p>Note: Low TruFor \u2260 authentic. AI-generated images score low because they weren't edited.</p>"},{"location":"guide/interpreting-results/#ela","title":"ELA","text":"<p><code>ela_anomaly_score</code> (z-score):</p> Range Interpretation 0.0 - 1.5 Normal 1.5 - 2.5 Slight anomalies 2.5 - 4.0 Moderate anomalies 4.0+ Strong anomalies <p>ELA detects regions with different compression. High scores suggest localized edits.</p>"},{"location":"guide/interpreting-results/#jpeg-quality","title":"JPEG Quality","text":"Quality Interpretation 90-100 High quality, minimal compression 70-90 Good quality 50-70 Moderate compression &lt; 50 Heavy compression"},{"location":"guide/interpreting-results/#frequency-analysis","title":"Frequency Analysis","text":"<p>Frequency domain results are corroborating evidence: - Periodic peaks may indicate AI-generated regularities - Unusual distributions can suggest processing</p>"},{"location":"guide/interpreting-results/#residual-analysis","title":"Residual Analysis","text":"<p>Noise statistics from DRUNet: - Very low residuals may indicate AI generation - Unusual distributions can suggest processing</p>"},{"location":"guide/interpreting-results/#common-scenarios","title":"Common Scenarios","text":""},{"location":"guide/interpreting-results/#high-visual-confidence-low-trufor-score","title":"High Visual Confidence, Low TruFor Score","text":"<p>Likely interpretation: AI-generated image</p> <p>TruFor detects editing, not generation. AI images are internally consistent.</p>"},{"location":"guide/interpreting-results/#low-visual-confidence-high-trufor-score","title":"Low Visual Confidence, High TruFor Score","text":"<p>Likely interpretation: Subtle manipulation</p> <p>TruFor detects things humans miss.</p>"},{"location":"guide/interpreting-results/#everything-uncertain","title":"Everything Uncertain","text":"<p>Interpretation: Genuine ambiguity</p> <ul> <li>Edge case</li> <li>New generation technique</li> <li>Unusual but legitimate image</li> </ul>"},{"location":"guide/interpreting-results/#reading-the-rationale","title":"Reading the Rationale","text":"<p>A good rationale includes:</p> <ol> <li>Visual observations</li> <li>Tool evidence</li> <li>How evidence was weighed</li> <li>Why the verdict was chosen</li> </ol> <p>Strong example:</p> <p>The image shows a portrait with anatomical anomalies: 6 fingers on left hand, uniformly smooth skin without visible pores. TruFor returned low manipulation probability (0.12), consistent with AI-generation rather than editing. Visual synthesis artifacts with absence of manipulation signatures supports verdict of FAKE (AI-generated).</p>"},{"location":"guide/interpreting-results/#batch-analysis","title":"Batch Analysis","text":"<p>For dataset evaluations, focus on:</p> <ul> <li>Accuracy when answered: Quality of committed predictions</li> <li>Coverage: How often it gives an answer vs uncertain</li> <li>Fake slip rate: Fakes incorrectly passed as real</li> <li>Real false flag rate: Reals incorrectly flagged as fake</li> </ul>"},{"location":"guide/interpreting-results/#see-also","title":"See Also","text":"<ul> <li>Tools Overview</li> <li>Metrics Reference</li> <li>SWGDE Best Practices</li> </ul>"},{"location":"reference/configuration/","title":"Configuration Guide","text":"<p>Complete reference for configuring DF3 behavior.</p>"},{"location":"reference/configuration/#configuration-methods","title":"Configuration Methods","text":"<p>DF3 accepts configuration through:</p> <ol> <li>Command-line arguments \u2014 Highest priority</li> <li>Environment variables \u2014 See Environment Variables</li> <li>Code defaults \u2014 Fallback values</li> </ol>"},{"location":"reference/configuration/#agent-configuration","title":"Agent Configuration","text":""},{"location":"reference/configuration/#model-selection","title":"Model Selection","text":"Parameter CLI Flag Default Description Agent Model <code>--model</code> <code>gpt-5.1</code> Primary model for reasoning and tools Vision Model <code>--vision-model</code> Same as agent Model for initial vision analysis Structuring Model <code>--structuring-model</code> Same as agent Model for BAML structuring <p>Example: Split models for cost optimization</p> <pre><code>python scripts/analyze_image.py ^\n    --image photo.jpg ^\n    --model gpt-5.1 ^\n    --vision-model gemini-3-flash ^\n    --structuring-model gpt-5-mini\n</code></pre>"},{"location":"reference/configuration/#temperature","title":"Temperature","text":"Parameter CLI Flag Default Range Temperature <code>--temperature</code> <code>0.2</code> 0.0 - 2.0 <ul> <li><code>0.0</code> \u2014 Most deterministic, recommended for evaluation</li> <li><code>0.2</code> \u2014 Default, slight variation</li> <li>Higher \u2014 More creative but less consistent</li> </ul>"},{"location":"reference/configuration/#agent-behavior","title":"Agent Behavior","text":"Parameter CLI Flag Default Description Max Iterations <code>--max-iterations</code> <code>15</code> Maximum tool-calling iterations Tools Enabled <code>--no-tools</code> (flag) Tools enabled Disable tool-calling Reasoning Effort <code>--reasoning-effort</code> None Provider-specific reasoning control"},{"location":"reference/configuration/#caching-configuration","title":"Caching Configuration","text":""},{"location":"reference/configuration/#cache-control","title":"Cache Control","text":"Parameter CLI Flag Default Description Disable Cache <code>--disable-tool-cache</code> Enabled Turn off all caching Cache Directory <code>--tool-cache-dir</code> <code>.tool_cache</code> Custom cache location"},{"location":"reference/configuration/#cache-behavior","title":"Cache Behavior","text":"<p>When enabled, DF3 caches:</p> <ul> <li>Tool outputs \u2014 Same image + tool = cached result</li> <li>Vision model outputs \u2014 Same image + model = cached description</li> </ul>"},{"location":"reference/configuration/#clearing-cache","title":"Clearing Cache","text":"<pre><code># Remove entire cache\nRemove-Item -Recurse -Force .tool_cache\n\n# Remove specific tool cache\nRemove-Item -Recurse -Force .tool_cache\\perform_trufor\n</code></pre> <p>Cache and Reproducibility</p> <p>For valid latency measurements, disable caching: <pre><code>--disable-tool-cache\n</code></pre></p>"},{"location":"reference/configuration/#evaluation-configuration","title":"Evaluation Configuration","text":""},{"location":"reference/configuration/#dataset-options","title":"Dataset Options","text":"Parameter CLI Flag Default Description Dataset <code>--dataset</code> Required Path to JSONL dataset Limit <code>--limit</code> All Process only first N samples Shuffle <code>--shuffle</code> No Randomize sample order Seed <code>--seed</code> None Random seed for shuffling"},{"location":"reference/configuration/#output-options","title":"Output Options","text":"Parameter CLI Flag Default Description Output <code>--output</code> <code>eval_results.jsonl</code> Per-sample results Metrics Output <code>--metrics-output</code> <code>eval_metrics.json</code> Aggregated metrics"},{"location":"reference/configuration/#parallel-processing","title":"Parallel Processing","text":"Parameter CLI Flag Default Description Workers <code>--num-workers</code> <code>1</code> Parallel worker threads Trials <code>--trials</code> <code>1</code> Repeat runs for variance <p>Example: Fast evaluation</p> <pre><code>python scripts/evaluate_llms.py ^\n    --dataset data/benchmark.jsonl ^\n    --models gpt-5.1 ^\n    --num-workers 4 ^\n    --tools both\n</code></pre>"},{"location":"reference/configuration/#tool-configuration","title":"Tool Configuration","text":""},{"location":"reference/configuration/#trufor-settings","title":"TruFor Settings","text":"<p>TruFor automatically:</p> <ul> <li>Downloads weights on first use (to <code>weights/trufor/</code>)</li> <li>Selects GPU if available, falls back to CPU</li> <li>Caches results by image hash</li> </ul>"},{"location":"reference/configuration/#drunet-residuals-settings","title":"DRUNet (Residuals) Settings","text":"<p>DRUNet weights stored in <code>src/tools/forensic/drunet/weights/</code>.</p>"},{"location":"reference/configuration/#custom-tool-directory","title":"Custom Tool Directory","text":"<p>To add custom tools, extend <code>src/tools/forensic_tools.py</code>.</p>"},{"location":"reference/configuration/#baml-configuration","title":"BAML Configuration","text":""},{"location":"reference/configuration/#client-configuration","title":"Client Configuration","text":"<p>BAML clients are defined in <code>baml_src/clients.baml</code>:</p> <pre><code>client&lt;llm&gt; DynamicForensicClient {\n    provider \"openai\"\n    options {\n        model \"gpt-5.1\"\n        temperature 0.2\n    }\n}\n</code></pre>"},{"location":"reference/configuration/#runtime-override","title":"Runtime Override","text":"<p>The Python code in <code>src/agents/baml_forensic.py</code> overrides the client at runtime:</p> <pre><code>cr = baml_py_core.ClientRegistry()\ncr.add_llm_client(\n    name=\"DynamicForensicClient\",\n    provider=provider,\n    options={\"model\": model_name, ...}\n)\n</code></pre>"},{"location":"reference/configuration/#regenerating-baml-client","title":"Regenerating BAML Client","text":"<p>After modifying <code>.baml</code> files:</p> <pre><code>baml-cli generate\n</code></pre>"},{"location":"reference/configuration/#provider-configuration","title":"Provider Configuration","text":""},{"location":"reference/configuration/#openai","title":"OpenAI","text":"<pre><code># .env\nOPENAI_API_KEY=sk-...\n</code></pre>"},{"location":"reference/configuration/#openrouter","title":"OpenRouter","text":"<pre><code># .env\nOPENROUTER_API_KEY=sk-or-v1-...\n</code></pre> <p>Access multiple providers through a single API.</p>"},{"location":"reference/configuration/#anthropic","title":"Anthropic","text":"<pre><code># .env\nANTHROPIC_API_KEY=sk-ant-...\n</code></pre>"},{"location":"reference/configuration/#google-gemini","title":"Google (Gemini)","text":"<pre><code># .env\nGOOGLE_API_KEY=...\n</code></pre>"},{"location":"reference/configuration/#logging-configuration","title":"Logging Configuration","text":""},{"location":"reference/configuration/#verbosity","title":"Verbosity","text":"Level Description Default Standard output only Verbose Include tool call details Debug Full LLM prompts/responses <p>Controlled via logging module:</p> <pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n</code></pre>"},{"location":"reference/configuration/#resource-limits","title":"Resource Limits","text":""},{"location":"reference/configuration/#memory","title":"Memory","text":"<ul> <li>TruFor model: ~2GB GPU memory (or RAM on CPU)</li> <li>DRUNet model: ~500MB</li> <li>Image processing: Varies by image size</li> </ul>"},{"location":"reference/configuration/#rate-limits","title":"Rate Limits","text":"<p>Consider provider rate limits when:</p> <ul> <li>Running batch evaluation</li> <li>Using multiple workers</li> <li>Processing large datasets</li> </ul>"},{"location":"reference/configuration/#configuration-examples","title":"Configuration Examples","text":""},{"location":"reference/configuration/#minimal-analysis","title":"Minimal Analysis","text":"<pre><code>python scripts/analyze_image.py --image photo.jpg\n</code></pre>"},{"location":"reference/configuration/#production-evaluation","title":"Production Evaluation","text":"<pre><code>python scripts/evaluate_llms.py ^\n    --dataset data/benchmark.jsonl ^\n    --models gpt-5.1 ^\n    --tools both ^\n    --temperature 0.0 ^\n    --trials 3 ^\n    --num-workers 4 ^\n    --disable-tool-cache ^\n    --output results/eval.jsonl ^\n    --metrics-output results/metrics.json\n</code></pre>"},{"location":"reference/configuration/#cost-optimized-analysis","title":"Cost-Optimized Analysis","text":"<pre><code>python scripts/analyze_image.py ^\n    --image photo.jpg ^\n    --model gpt-5-mini ^\n    --vision-model gpt-5.1 ^\n    --structuring-model gpt-5-mini\n</code></pre>"},{"location":"reference/configuration/#quick-validation","title":"Quick Validation","text":"<pre><code>python scripts/evaluate_llms.py ^\n    --dataset data/benchmark.jsonl ^\n    --models gpt-5-mini ^\n    --limit 50 ^\n    --tools no-tools\n</code></pre>"},{"location":"reference/configuration/#see-also","title":"See Also","text":"<ul> <li>Environment Variables \u2014 API keys and secrets</li> <li>Troubleshooting \u2014 Common issues</li> <li>CLI Reference \u2014 Command-line usage</li> </ul>"},{"location":"reference/environment/","title":"Environment Variables","text":"<p>API keys and configuration via environment.</p>"},{"location":"reference/environment/#quick-setup","title":"Quick Setup","text":"<pre><code># Copy template\nCopy-Item .env.example .env\n\n# Edit with your keys\nnotepad .env\n</code></pre>"},{"location":"reference/environment/#required-variables","title":"Required Variables","text":""},{"location":"reference/environment/#llm-provider-keys","title":"LLM Provider Keys","text":"<p>You need at least one API key for an LLM provider:</p> Variable Provider Required <code>OPENAI_API_KEY</code> OpenAI Yes (if using OpenAI) <code>OPENROUTER_API_KEY</code> OpenRouter Yes (if using OpenRouter) <code>ANTHROPIC_API_KEY</code> Anthropic Optional <code>GOOGLE_API_KEY</code> Google AI Optional"},{"location":"reference/environment/#example-env","title":"Example <code>.env</code>","text":"<pre><code># Primary LLM provider\nOPENAI_API_KEY=sk-proj-...\n\n# Alternative: OpenRouter for multiple providers\nOPENROUTER_API_KEY=sk-or-v1-...\n\n# Optional providers\nANTHROPIC_API_KEY=sk-ant-...\nGOOGLE_API_KEY=AIza...\n</code></pre>"},{"location":"reference/environment/#variable-reference","title":"Variable Reference","text":""},{"location":"reference/environment/#openai_api_key","title":"OPENAI_API_KEY","text":"<p>OpenAI API access key</p> <ul> <li>Format: <code>sk-proj-...</code> or <code>sk-...</code></li> <li>Source: OpenAI Platform</li> <li>Required for: GPT models, default analysis</li> </ul>"},{"location":"reference/environment/#openrouter_api_key","title":"OPENROUTER_API_KEY","text":"<p>OpenRouter unified API key</p> <ul> <li>Format: <code>sk-or-v1-...</code></li> <li>Source: OpenRouter</li> <li>Required for: Multi-provider access, some models</li> </ul>"},{"location":"reference/environment/#anthropic_api_key","title":"ANTHROPIC_API_KEY","text":"<p>Anthropic API key for Claude models</p> <ul> <li>Format: <code>sk-ant-...</code></li> <li>Source: Anthropic Console</li> <li>Required for: Claude models</li> </ul>"},{"location":"reference/environment/#google_api_key","title":"GOOGLE_API_KEY","text":"<p>Google AI Studio key for Gemini</p> <ul> <li>Format: <code>AIza...</code></li> <li>Source: Google AI Studio</li> <li>Required for: Gemini models</li> </ul>"},{"location":"reference/environment/#loading-variables","title":"Loading Variables","text":""},{"location":"reference/environment/#automatic-loading","title":"Automatic Loading","text":"<p>DF3 automatically loads <code>.env</code> using <code>python-dotenv</code>:</p> <pre><code>from dotenv import load_dotenv\nload_dotenv()  # Loads .env from project root\n</code></pre>"},{"location":"reference/environment/#manual-setting-powershell","title":"Manual Setting (PowerShell)","text":"<pre><code># Session only\n$env:OPENAI_API_KEY = \"sk-...\"\n\n# Or use setx for persistence (new sessions only)\nsetx OPENAI_API_KEY \"sk-...\"\n</code></pre>"},{"location":"reference/environment/#manual-setting-bashzsh","title":"Manual Setting (Bash/Zsh)","text":"<pre><code>export OPENAI_API_KEY=\"sk-...\"\n\n# Or add to ~/.bashrc / ~/.zshrc\necho 'export OPENAI_API_KEY=\"sk-...\"' &gt;&gt; ~/.bashrc\n</code></pre>"},{"location":"reference/environment/#provider-selection","title":"Provider Selection","text":""},{"location":"reference/environment/#how-df3-selects-a-provider","title":"How DF3 selects a provider","text":"<p>For the CLI (<code>scripts/analyze_image.py</code> / <code>scripts/evaluate_llms.py</code>), the provider is selected via:</p> <ul> <li><code>--provider</code> (<code>openai</code> or <code>openrouter</code>)</li> <li><code>--base-url</code> (optional override; defaults to <code>https://openrouter.ai/api/v1</code> when <code>--provider openrouter</code>)</li> </ul> <p>Important: the provider is not inferred from the model name. For example, <code>openai/gpt-5.1</code> is an OpenRouter model identifier, but you still need <code>--provider openrouter</code> (or an OpenRouter <code>--base-url</code>) to route requests to OpenRouter.</p>"},{"location":"reference/environment/#openai-default","title":"OpenAI (default)","text":"<ul> <li>Set <code>OPENAI_API_KEY</code></li> <li>Use OpenAI model names like <code>gpt-5.1</code></li> </ul> <pre><code>python scripts/analyze_image.py --image photo.jpg --model gpt-5.1\n</code></pre>"},{"location":"reference/environment/#openrouter-multi-provider","title":"OpenRouter (multi-provider)","text":"<ul> <li>Set <code>OPENROUTER_API_KEY</code></li> <li>Use OpenRouter model IDs like <code>google/gemini-3-flash-preview</code>, <code>anthropic/claude-sonnet-4-20250514</code>, <code>openai/gpt-5.1</code>, etc.</li> </ul> <pre><code>python scripts/analyze_image.py --image photo.jpg `\n    --provider openrouter `\n    --model google/gemini-3-flash-preview\n</code></pre>"},{"location":"reference/environment/#optional-openrouter-headers","title":"Optional OpenRouter headers","text":"<p>Some OpenRouter setups recommend passing identification headers:</p> <ul> <li><code>--referer</code> \u2192 <code>HTTP-Referer</code></li> <li><code>--title</code> \u2192 <code>X-Title</code></li> </ul>"},{"location":"reference/environment/#troubleshooting","title":"Troubleshooting","text":""},{"location":"reference/environment/#api-key-not-found","title":"\"API key not found\"","text":"<p>Cause: Environment variable not set or <code>.env</code> not loaded.</p> <p>Fix:</p> <pre><code># Verify variable is set\n$env:OPENAI_API_KEY\n\n# If empty, set it\n$env:OPENAI_API_KEY = \"sk-...\"\n</code></pre>"},{"location":"reference/environment/#invalid-api-key","title":"\"Invalid API key\"","text":"<p>Cause: Malformed or revoked key.</p> <p>Fix:</p> <ol> <li>Check key format (no extra spaces)</li> <li>Verify key is active in provider dashboard</li> <li>Generate new key if needed</li> </ol>"},{"location":"reference/environment/#rate-limit-exceeded","title":"\"Rate limit exceeded\"","text":"<p>Cause: Too many API calls.</p> <p>Fix:</p> <ol> <li>Reduce <code>--num-workers</code></li> <li>Add delays between requests</li> <li>Upgrade API plan</li> </ol>"},{"location":"reference/environment/#model-not-found","title":"\"Model not found\"","text":"<p>Cause: Model name incorrect or not available to your key.</p> <p>Fix:</p> <ol> <li>Check model name spelling</li> <li>Verify model access in provider dashboard</li> <li>Try alternative model</li> </ol>"},{"location":"reference/environment/#template-env-file","title":"Template .env File","text":"<p>Create <code>.env.example</code> for team reference:</p> <pre><code># DF3 Environment Configuration\n# Copy this file to .env and fill in your keys\n\n# Required: At least one LLM provider\nOPENAI_API_KEY=your-openai-key-here\n\n# Optional: Alternative providers\n# OPENROUTER_API_KEY=your-openrouter-key-here\n# ANTHROPIC_API_KEY=your-anthropic-key-here\n# GOOGLE_API_KEY=your-google-key-here\n\n# Optional: Custom settings\n# DF3_CACHE_DIR=.tool_cache\n# DF3_LOG_LEVEL=INFO\n</code></pre>"},{"location":"reference/environment/#see-also","title":"See Also","text":"<ul> <li>Configuration Guide \u2014 Full configuration reference</li> <li>Troubleshooting \u2014 Common issues</li> <li>Installation \u2014 Setup guide</li> </ul>"},{"location":"reference/faq/","title":"FAQ","text":""},{"location":"reference/faq/#general","title":"General","text":""},{"location":"reference/faq/#what-is-df3","title":"What is DF3?","text":"<p>DF3 is an agentic forensic image analysis system that combines vision-capable LLMs with specialized forensic tools to classify images as <code>real</code>, <code>fake</code>, or <code>uncertain</code>.</p>"},{"location":"reference/faq/#what-does-the-verdict-mean","title":"What does the verdict mean?","text":"Verdict Meaning <code>real</code> No manipulation or generation indicators detected <code>fake</code> Manipulation or AI-generation indicators detected <code>uncertain</code> Evidence insufficient or conflicting"},{"location":"reference/faq/#what-types-of-fakes-does-df3-target","title":"What types of fakes does DF3 target?","text":"<ul> <li>AI-generated images (DALL-E, Midjourney, Stable Diffusion, etc.)</li> <li>Manipulated images (splicing, inpainting, copy-move)</li> <li>Face swaps and identity manipulations</li> </ul>"},{"location":"reference/faq/#technical","title":"Technical","text":""},{"location":"reference/faq/#which-llms-are-supported","title":"Which LLMs are supported?","text":"<p>DF3 uses LangChain and BAML libraries for model API integration, standardized on either OpenRouter-style or OpenAI-style API calls. Any vision-capable model that supports tool calling (at least in prompt format) is supported, including:</p> <ul> <li>Models accessible via OpenAI-compatible endpoints (cloud or local)</li> <li>Models accessible via OpenRouter</li> <li>Locally run LLMs (via OpenAI-compatible local servers or LangChain's local integrations)</li> <li>Any provider supported by LangChain's model integrations</li> </ul> <p>The system does not require specific model implementations\u2014it works with any model that can process images and handle tool/function calling semantics, whether hosted remotely or running locally.</p>"},{"location":"reference/faq/#what-forensic-tools-are-included","title":"What forensic tools are included?","text":"Tool Function TruFor Neural forgery detection ELA JPEG compression anomaly detection JPEG Analysis Quantization table analysis Frequency Analysis DCT/FFT pattern detection Residual Extraction DRUNet noise analysis Metadata EXIF/XMP/C2PA extraction Code Execution Custom Python analysis"},{"location":"reference/faq/#is-gpu-required","title":"Is GPU required?","text":"<p>No. TruFor and DRUNet run on CPU but are faster with CUDA.</p>"},{"location":"reference/faq/#what-image-formats-are-supported","title":"What image formats are supported?","text":"<p>JPEG, PNG, BMP, TIFF, WEBP</p>"},{"location":"reference/faq/#usage","title":"Usage","text":""},{"location":"reference/faq/#single-image-analysis","title":"Single image analysis","text":"<pre><code>python scripts/analyze_image.py --image photo.jpg\n</code></pre>"},{"location":"reference/faq/#vision-only-mode-no-tools","title":"Vision-only mode (no tools)","text":"<pre><code>python scripts/analyze_image.py --image photo.jpg --no-tools\n</code></pre>"},{"location":"reference/faq/#batch-evaluation","title":"Batch evaluation","text":"<pre><code>python scripts/evaluate_llms.py --dataset data/eval.jsonl --models gpt-5.1 --tools both\n</code></pre>"},{"location":"reference/faq/#different-vision-and-agent-models","title":"Different vision and agent models","text":"<pre><code>python scripts/analyze_image.py --image photo.jpg \\\n    --model gpt-5.1 \\\n    --vision-model gemini-3-flash \\\n    --structuring-model gpt-5-mini\n</code></pre>"},{"location":"reference/faq/#results","title":"Results","text":""},{"location":"reference/faq/#what-does-the-confidence-score-mean","title":"What does the confidence score mean?","text":"<p>The LLM's self-reported certainty (0-1). Use for triage ranking, not as a calibrated probability.</p>"},{"location":"reference/faq/#why-does-the-model-output-uncertain","title":"Why does the model output UNCERTAIN?","text":"<ul> <li>Conflicting evidence between visual analysis and tools</li> <li>Low internal confidence</li> <li>Image quality insufficient for analysis</li> <li>Ambiguous content</li> </ul>"},{"location":"reference/faq/#trufor-says-no-manipulation-but-image-looks-ai-generated","title":"TruFor says \"no manipulation\" but image looks AI-generated?","text":"<p>Expected. TruFor detects manipulation (editing), not generation. AI-generated images are internally consistent and score low on manipulation detectors.</p>"},{"location":"reference/faq/#integration","title":"Integration","text":""},{"location":"reference/faq/#library-usage","title":"Library usage","text":"<pre><code>from src.agents.forensic_agent import ForensicAgent\n\nagent = ForensicAgent(llm_model=\"gpt-5.1\")\nresult = agent.analyze(\"photo.jpg\")\nprint(result[\"verdict\"])\n</code></pre>"},{"location":"reference/faq/#adding-custom-tools","title":"Adding custom tools","text":"<p>Implement in <code>src/tools/forensic/</code> and register in <code>forensic_tools.py</code>.</p>"},{"location":"reference/faq/#data","title":"Data","text":""},{"location":"reference/faq/#are-images-sent-externally","title":"Are images sent externally?","text":"<p>Images are sent to the configured LLM provider (OpenAI, Anthropic, Google, etc.). Forensic tools run locally.</p>"},{"location":"reference/faq/#local-caching","title":"Local caching","text":"<p>Tool outputs and vision results can be cached in <code>.tool_cache/</code>. Disable with <code>--disable-tool-cache</code>.</p>"},{"location":"reference/faq/#see-also","title":"See Also","text":"<ul> <li>Quickstart</li> <li>Troubleshooting</li> <li>Limitations</li> </ul>"},{"location":"reference/prompts/","title":"Prompts Reference","text":"<p>Complete documentation of all prompts used throughout the DF3 forensic image analysis system.</p>"},{"location":"reference/prompts/#overview","title":"Overview","text":"<p>The DF3 system uses multiple prompts at different stages of the analysis pipeline:</p> <ol> <li>Vision Analysis Prompts - Initial image analysis without tools</li> <li>Agent System Prompt - Main agent reasoning and tool usage guidance</li> <li>Agent User Prompt - Context-specific instructions for tool-based analysis</li> <li>BAML Prompts - Structured output extraction and vision-only analysis</li> </ol> <p>All prompts are centralized in <code>src/agents/prompts.py</code> (Python prompts) and <code>baml_src/forensic_analysis.baml</code> (BAML prompts).</p>"},{"location":"reference/prompts/#python-prompts-srcagentspromptspy","title":"Python Prompts (<code>src/agents/prompts.py</code>)","text":""},{"location":"reference/prompts/#system-prompt-get_system_prompt","title":"System Prompt (<code>get_system_prompt()</code>)","text":"<p>The main system prompt used by the LangGraph agent for tool-based analysis. This prompt includes SWGDE best practices when available.</p> <pre><code>def get_system_prompt() -&gt; str:\n</code></pre> <p>Full Prompt:</p> <pre><code>You are a forensic image analysis agent specializing in detecting AI-generated or manipulated images.\n\nYOUR PRIMARY TASK: Determine if an image is REAL (authentic photograph), FAKE (AI-generated/synthetic/manipulated), or UNCERTAIN (inconclusive; route to human/manual review).\n\nIMPORTANT DISTINCTION (DO NOT CONFUSE THESE):\n- \"No evidence of manipulation\" (from tools like TruFor/ELA) is NOT the same as \"not synthetic\".\n- Fully AI-generated images can sometimes score low on manipulation tools. Treat low manipulation signals as *neutral* for synthetic detection.\n\n## How to Analyze (in order):\n\n### 1. VISUAL INSPECTION (Most Important)\nLook carefully for AI-generation artifacts. These are YOUR PRIMARY SIGNALS, right after your initial reasonings and insights:\n\n**Anatomical &amp; Biological Errors:**\n- Wrong number of fingers, teeth, eyes, or limbs\n- Hands with merged, extra, or missing fingers\n- Asymmetric or malformed ears, eyes, or facial features\n- Unnatural body proportions or impossible poses\n- Hair that merges with background or defies physics\n- Teeth that are too uniform, blurry, or incorrectly shaped\n\n**Texture &amp; Surface Anomalies:**\n- Skin that looks too smooth, waxy, or plastic-like\n- Repeating patterns or textures (especially in backgrounds, fabric, grass, crowds)\n- Inconsistent level of detail (sharp face but blurry ears)\n- \"Painted\" or \"airbrushed\" appearance\n- Text or writing that is garbled, misspelled, or nonsensical\n\n**Lighting &amp; Physics Violations:**\n- Shadows pointing in different directions\n- Missing or impossible reflections\n- Light sources that don't match the shadows\n- Objects floating or defying gravity\n- Impossible perspective or depth\n\n**Semantic Impossibilities:**\n- Objects that don't make sense in context\n- Backgrounds that blend incorrectly with subjects\n- Watermarks or signatures that look AI-generated\n- Uncanny valley effect - something feels \"off\" even if you can't pinpoint it\n\n### 2. FORENSIC TOOLS (Important for Detecting Manipulation)\nTools are CRITICAL for detecting manipulated/edited images (splicing, photoshopping, compositing):\n\n**How to interpret tool results:**\n- **TruFor manipulation_probability near 1.0** \u2192 Strong evidence of manipulation. Take this seriously!\n- **TruFor manipulation_probability near 0.0** \u2192 Little evidence of post-hoc editing/splicing. This does NOT rule out fully synthetic generation.\n- **ELA showing localized anomalies** \u2192 Suggests specific regions were edited\n- **Unusual frequency patterns / residual statistics / JPEG inconsistencies** \u2192 Can sometimes support synthetic detection, but are not definitive alone\n\n**Two types of fakes require different approaches:**\n1. **Manipulated images (editing, photoshop, splicing)**: Tools like TruFor and ELA excel here. High tool scores = strong fake signal.\n2. **AI-generated images**: Tools may show \"no manipulation\" because AI mostly creates consistent images. Visual analysis is key here. However even if the image looks perfect, it may be AI-generated, you need to use your judgment and evaluate with the forensics tools when needed.\nYou need to think like a expert forensic investigator and use your judgment to determine if the image is AI-generated or not, based on evidence, explanation and reasoning.\n\n**Decision logic:**\n- Strong manipulation-tool signal (e.g., TruFor near 1.0, strong localized ELA issues) \u2192 Likely FAKE (manipulated), even if it looks visually convincing\n- Visual anomalies consistent with synthesis (anatomy/texture/lighting/semantics) + weak manipulation-tool signal \u2192 Likely FAKE (fully synthetic)\n- Clean visuals + weak manipulation-tool signal \u2192 Could be REAL or a high-quality synthetic. Do NOT treat this as proof of real; state limitations and weigh other evidence.\n\nAvailable tools:\n- metadata: Extract EXIF/XMP/ICC metadata and detect C2PA / Content Credentials. Input: string path or JSON {\"path\": \"...\"}\n- analyze_jpeg_compression: JPEG compression artifacts &amp; quantization. Input: plain string path.\n- detect_jpeg_quantization: JPEG quant tables, quality estimation. Input: string path or JSON {\"path\": \"...\"}\n- analyze_frequency_domain: DCT/FFT frequency anomalies. Input: plain string path.\n- extract_residuals: DRUNet residual statistics for noise cues. Input: plain string path.\n- perform_ela: Error Level Analysis for localized inconsistencies. Input: string path or JSON {\"path\": \"...\"}\n- perform_trufor: AI-driven forgery detection. HIGH SCORES ARE MEANINGFUL - don't dismiss! Input: string path or JSON {\"path\": \"...\"}\n- execute_python_code: Custom Python analysis. Input: JSON {\"code\": \"...\", \"image_path\": \"...\"}\n  Pre-loaded: img (PIL), img_array (numpy), np, Path, artifacts_dir. Can import: cv2, scipy. Save files to artifacts_dir\n\nTool guidelines:\n- Use 1-3 tools to check for manipulation\n- RESPECT high confidence scores from tools - they detect things humans miss\n- If tools and visual analysis conflict, investigate further rather than dismissing either\n\n**IMPORTANT: Do NOT invent new thresholds for tool outputs.**\n- TruFor manipulation_probability: 0-1 scale. Near 0 = likely authentic, near 1 = likely manipulated\n- ELA anomaly_score: relative z-score. Higher = more anomalous. Interpret in context, don't apply arbitrary cutoffs\n- Other metrics: interpret relatively (higher/lower than typical) rather than using made-up thresholds\n- When uncertain about a score's meaning, describe what it suggests rather than claiming specific thresholds\n\n### 3. REACH A VERDICT (3-way triage)\nChoose exactly one outcome:\n- **FAKE**: Sufficient evidence the image is AI-generated or manipulated.\n- **REAL**: Sufficient evidence the image is an authentic photograph.\n- **UNCERTAIN**: Evidence is insufficient or conflicting \u2192 recommend manual review / more data.\n\n**FAKE signals (any of these is strong evidence):**\n- TruFor manipulation_probability near 1.0 \u2192 likely manipulated/edited\n- Strong visual anomalies (anatomical errors, impossible physics) \u2192 likely AI-generated\n- ELA showing high anomaly scores or localized inconsistencies \u2192 likely edited\n\n**REAL signals:**\n- Natural appearance with no visual anomalies\n- TruFor manipulation_probability near 0.0 is supportive for \"not manipulated\", but is NOT sufficient to rule out fully synthetic generation\n- Consistent lighting, physics, and anatomy\n\n**How to weigh conflicting evidence:**\n- High TruFor scores should NOT be dismissed as \"false positives\" without strong counter-evidence\n- If visuals look clean but TruFor is high \u2192 trust the tool, it detects things humans miss\n- If visuals show AI artifacts but tools are low \u2192 trust your eyes, AI images pass tool checks\n- Both visual AND tool evidence pointing to fake \u2192 definitely FAKE\n- Mid-range tool scores (e.g., 0.3-0.6) \u2192 look for corroborating evidence from other tools or visual analysis\n\n**When to use UNCERTAIN (valid outcome):**\n- The image is too low quality / too compressed to assess reliably\n- Evidence is genuinely mixed or weak after considering both visual cues and tool evidence\n- Tool outputs are neutral/contradictory and visual cues are not decisive\n\n\n## Output Format\nRespond naturally in MARKDOWN. Include these sections but reason freely within them:\n\n### Visual Analysis\nDescribe what you see. Note anything suspicious or confirming authenticity. Think out loud.\n\n### Tool Evidence (if used)\nBrief summary of what tools found. Explain how this supports or conflicts with visual findings.\n\n### Reasoning &amp; Verdict\nExplain your reasoning. Weigh the evidence. Then state clearly:\n**Verdict: real** or **Verdict: fake** or **Verdict: uncertain**\n\n### Confidence\n**Confidence (0-1): X.XX** - justify briefly\n\nCite specific evidence for your conclusion.\n</code></pre> <p>SWGDE Best Practices:</p> <p>When <code>docs/sw.md</code> is available, the system prompt is automatically appended with SWGDE Image Authentication best practices (sections 6-9). This is loaded dynamically at runtime via <code>_load_swgde_best_practices_text()</code>.</p>"},{"location":"reference/prompts/#vision-system-prompt-get_vision_system_prompt","title":"Vision System Prompt (<code>get_vision_system_prompt()</code>)","text":"<p>Used for the initial vision-only analysis step (before tools).</p> <pre><code>def get_vision_system_prompt() -&gt; str:\n</code></pre> <p>Full Prompt:</p> <pre><code>You are an expert at detecting AI-generated and manipulated images. Analyze images carefully for signs of AI generation or manipulation. Trust your visual analysis - look for anatomical errors, texture anomalies, lighting inconsistencies, and anything that feels 'off'. Return a verdict: real, fake, or uncertain (uncertain means inconclusive \u2192 manual review). Prefer uncertain over guessing. Return ONLY a JSON object.\n</code></pre>"},{"location":"reference/prompts/#vision-user-prompt-get_vision_user_prompt","title":"Vision User Prompt (<code>get_vision_user_prompt()</code>)","text":"<p>The user prompt for vision-only analysis, requesting structured JSON output.</p> <pre><code>def get_vision_user_prompt() -&gt; str:\n</code></pre> <p>Full Prompt:</p> <pre><code>Analyze this image and determine if it is REAL (authentic photograph) or FAKE (AI-generated, synthetic, manipulated, or deepfake).\n\nLook carefully for:\n- Anatomical errors: wrong number of fingers/teeth/eyes, malformed hands, asymmetric features\n- Texture issues: too-smooth skin, repeating patterns, inconsistent detail levels, waxy/plastic appearance\n- Lighting problems: mismatched shadows, impossible reflections, inconsistent light sources\n- Semantic oddities: objects that don't make sense, garbled text, uncanny valley feeling\n- Any detail that feels \"off\" even if you can't explain why\n\nIMPORTANT: You may return \"uncertain\" (inconclusive) if evidence is insufficient or conflicting. Prefer \"uncertain\" over guessing.\n\nReturn ONLY a JSON object with these keys:\n- visual_description: string describing scene, subjects, and any anomalies you notice\n- synthesis_indicators: string listing specific evidence for/against AI generation\n- verdict: \"real\" | \"fake\" | \"uncertain\"\n- confidence: float between 0 and 1\n- rationale: your reasoning in 2-3 sentences explaining why you reached this verdict\n- full_text: a narrative combining all your observations and reasoning\n</code></pre>"},{"location":"reference/prompts/#agent-prompt-builder-build_agent_prompt","title":"Agent Prompt Builder (<code>build_agent_prompt()</code>)","text":"<p>Builds the user prompt for the agent reasoning step, incorporating the visual analysis summary.</p> <pre><code>def build_agent_prompt(visual_summary: str, image_path: str) -&gt; str:\n</code></pre> <p>Full Prompt Template:</p> <pre><code>Your initial visual analysis:\n{visual_summary}\n\nImage path: {image_path}\n\nNow use forensic tools to check for manipulation. Remember:\n- **Two types of fakes**: Manipulated images (tools detect well) vs AI-generated (visual analysis detects better)\n- **High tool scores matter**: TruFor near 1.0 is strong evidence of manipulation - don't dismiss it!\n- **Low tool scores don't mean real**: AI-generated images pass tool checks because they're internally consistent\n- **Interpret scores relatively**: Don't invent specific thresholds. TruFor near 0 = likely real, near 1 = likely fake\n- **Both matter**: Use BOTH visual analysis AND tool evidence to reach your verdict\n\nTool input formats:\n- metadata: \"{image_path}\" or {{\"path\": \"{image_path}\"}}\n- analyze_jpeg_compression: \"{image_path}\" (plain string)\n- detect_jpeg_quantization: \"{image_path}\" or {{\"path\": \"{image_path}\"}}\n- analyze_frequency_domain: \"{image_path}\" (plain string)\n- extract_residuals: \"{image_path}\" (plain string)\n- perform_ela: \"{image_path}\" or {{\"path\": \"{image_path}\"}}\n- perform_trufor: \"{image_path}\" or {{\"path\": \"{image_path}\"}}\n- execute_python_code: {{\"code\": \"...\", \"image_path\": \"{image_path}\"}} (img, img_array, np, Path, artifacts_dir available; can import cv2, scipy; save to artifacts_dir)\n\nUse 1-3 tools for supporting evidence, reason, then reach your verdict.\n\nCRITICAL: Output one verdict: real, fake, or uncertain. \"Uncertain\" is a valid outcome meaning inconclusive \u2192 manual review. Prefer uncertain over guessing; do NOT output \"real\" unless you have affirmative reasons.\n\nRespond naturally in MARKDOWN:\n\n### Observations\nDescribe what is visibly in the image. Be concrete (subjects, scene, lighting, text). Do not speculate beyond what is visible.\n\n### Tool Outputs\nSummarize tool outputs with ONLY the key fields and numbers (no long dumps).\nKeep this section short (aim: &lt;= 8 bullets total).\nClearly mark any tool as not applicable (e.g., JPEG-specific tools on PNG/WEBP).\n\n### Interpretation\nInterpret what the observations + tool outputs suggest. IMPORTANT: low manipulation-tool evidence is NOT proof of \"real\" for fully synthetic images.\n\n### Limitations\nList limitations/caveats as bullets (e.g., image quality, tool applicability, model uncertainty, synthetic vs manipulation ambiguity).\n\n### Conclusion\nState clearly: **Verdict: real** or **Verdict: fake** or **Verdict: uncertain** and justify briefly.\nAlso include a line: **Confidence (0-1): X.XX**\n\nImportant: Do NOT output a final JSON object here. Write a clean markdown analysis.\nYour output will be structured downstream, so focus on correct reasoning and clear evidence.\n\nHard limit: Keep your entire response under ~500 words to avoid timeouts.\n</code></pre>"},{"location":"reference/prompts/#retry-prompt-build_retry_prompt","title":"Retry Prompt (<code>build_retry_prompt()</code>)","text":"<p>Used when the agent's response is missing required sections and needs regeneration.</p> <pre><code>def build_retry_prompt(visual_output: str, previous_output: str) -&gt; str:\n</code></pre> <p>Full Prompt Template:</p> <pre><code>The previous response was missing the visual analysis section.\n\nRewrite with this structure:\n1) ### Visual Analysis: your observations about the image (use provided context)\n2) ### Tool Evidence: tools used and findings (or \"No tools used\")\n3) ### Reasoning &amp; Verdict: explain your thinking, then state **Verdict: real** or **Verdict: fake** or **Verdict: uncertain**\n4) ### Confidence: **Confidence (0-1): X.XX**\n\nContext from initial analysis:\n{visual_output}\n\nPrevious response:\n{previous_output}\n\nRegenerate now, and provide a verdict (real/fake/uncertain).\n</code></pre>"},{"location":"reference/prompts/#baml-prompts-baml_srcforensic_analysisbaml","title":"BAML Prompts (<code>baml_src/forensic_analysis.baml</code>)","text":"<p>BAML prompts are used for structured output extraction and vision-only analysis. They follow a multi-step approach to avoid reasoning degradation.</p>"},{"location":"reference/prompts/#analyzeimagevisiononly","title":"AnalyzeImageVisionOnly","text":"<p>Unstructured vision-only analysis function. Returns free-form markdown text.</p> <pre><code>function AnalyzeImageVisionOnly(image: image) -&gt; string\n</code></pre> <p>Full Prompt:</p> <pre><code>You are a forensic image analyst specializing in detecting AI-generated or manipulated images.\n\nCRITICAL: You MUST always start your analysis by describing what is actually in the image - the subjects, scene, objects, people, animals, environment, etc. Do NOT skip directly to forensic metrics.\n\nAnalyze this image and decide one of three outcomes:\n- real: authentic photograph\n- fake: AI-generated, synthetic, or manipulated\n- uncertain: inconclusive \u2192 route to human/manual review\n\nSafety rule: Do NOT output \"real\" unless you have affirmative reasons it is authentic. Prefer \"uncertain\" over guessing.\n\nProvide your analysis in MARKDOWN format with this structure:\n### Visual Description\n- Describe what is visibly in the image (subjects, scene, objects, people/animals, environment, colors, composition)\n- Analyze lighting: sources, direction, intensity, shadows, reflections, consistency\n- Check physics: perspective, shadows, reflections, physical interactions, textures\n- Note any visual anomalies or inconsistencies you observe\n\n### Forensic Analysis\n- Vision-only pass; note \"No tools used\" and list any visual cues for/against synthesis\n\n### Conclusion\n- State if the image looks synthetic/AI vs natural, and why (refer to observations above)\n- Include a line \"Verdict: real | fake | uncertain\" (uncertain means inconclusive/manual review)\n\n### Confidence\n- State High / Medium / Low with a brief justification\n- Include \"Confidence (0-1): &lt;value between 0 and 1&gt;\"\n\nThink through your reasoning step by step. Do not constrain your thinking - provide detailed analysis.\n\n{{ _.role(\"user\") }} {{ image }}\n</code></pre>"},{"location":"reference/prompts/#structureforensicanalysis","title":"StructureForensicAnalysis","text":"<p>Extracts structured data from unstructured reasoning output. This separation prevents reasoning degradation.</p> <pre><code>function StructureForensicAnalysis(reasoning_output: string) -&gt; ForensicAnalysisResult\n</code></pre> <p>Full Prompt:</p> <pre><code>Extract structured information from this forensic analysis reasoning output.\n\nThe analysis may contain:\n- A visual description of the image\n- Forensic tool results or summaries\n- A conclusion with a verdict (real/fake/uncertain)\n- A confidence level and value\n\nExtract the following information:\n- verdict: The final verdict (real, fake, or uncertain). Treat \"inconclusive\" / \"cannot determine\" as \"uncertain\".\n- confidence: A float between 0 and 1\n- rationale: A brief justification (max 80 words)\n- visual_description: Description of what's in the image\n- forensic_summary: Summary of tools used or \"No tools used\"\n- full_text: The complete formatted narrative from the reasoning output\n\nReasoning output:\n{{ reasoning_output }}\n\n{{ ctx.output_format }}\n</code></pre>"},{"location":"reference/prompts/#analyzeimagevisiononlystructured","title":"AnalyzeImageVisionOnlyStructured","text":"<p>Convenience function that combines vision analysis and structuring in one step. May cause reasoning degradation in complex cases.</p> <pre><code>function AnalyzeImageVisionOnlyStructured(image: image) -&gt; ForensicAnalysisResult\n</code></pre> <p>Full Prompt:</p> <pre><code>You are a forensic image analyst. Analyze this image and assess whether it appears AI-generated, synthetic, or a deepfake.\n\nCRITICAL: You MUST always start your analysis by describing what is actually in the image - the subjects, scene, objects, people, animals, environment, etc.\n\nThink through your reasoning step by step. Consider:\n1. Visual description: What is in the image, lighting, physics\n2. Synthesis indicators: Visual cues for/against synthesis\n3. Verdict: real, fake, or uncertain (uncertain means inconclusive/manual review)\n4. Confidence: A value between 0 and 1\n5. Rationale: Brief justification (max 80 words)\n\nSafety rule: Do NOT output \"real\" unless you have affirmative reasons it is authentic. Prefer \"uncertain\" over guessing.\n\nAfter reasoning through these points, provide your structured answer.\n\n{{ _.role(\"user\") }} {{ image }}\n\n{{ ctx.output_format }}\n</code></pre>"},{"location":"reference/prompts/#prompt-usage-flow","title":"Prompt Usage Flow","text":""},{"location":"reference/prompts/#with-tools-use_toolstrue","title":"With Tools (<code>use_tools=True</code>)","text":"<ol> <li>Vision Analysis \u2192 <code>get_vision_system_prompt()</code> + <code>get_vision_user_prompt()</code> (via BAML <code>AnalyzeImageVisionOnlyStructured</code>)</li> <li>Agent Reasoning \u2192 <code>get_system_prompt()</code> + <code>build_agent_prompt(visual_summary, image_path)</code></li> <li>Structuring \u2192 <code>StructureForensicAnalysis</code> (BAML prompt)</li> </ol>"},{"location":"reference/prompts/#without-tools-use_toolsfalse","title":"Without Tools (<code>use_tools=False</code>)","text":"<ol> <li>Vision Analysis \u2192 <code>get_vision_system_prompt()</code> + <code>get_vision_user_prompt()</code> (via BAML <code>AnalyzeImageVisionOnlyStructured</code>)</li> <li>Structuring \u2192 Already structured from step 1</li> </ol>"},{"location":"reference/prompts/#prompt-design-principles","title":"Prompt Design Principles","text":""},{"location":"reference/prompts/#1-separation-of-reasoning-and-structuring","title":"1. Separation of Reasoning and Structuring","text":"<p>To avoid reasoning degradation, the system separates: - Reasoning phase: Unstructured, free-form markdown output - Structuring phase: Dedicated extraction of structured fields</p>"},{"location":"reference/prompts/#2-explicit-verdict-guidance","title":"2. Explicit Verdict Guidance","text":"<p>All prompts emphasize: - Three-way verdict: <code>real</code>, <code>fake</code>, or <code>uncertain</code> - <code>uncertain</code> is a valid outcome (inconclusive \u2192 manual review) - Prefer <code>uncertain</code> over guessing - Do not output <code>real</code> without affirmative evidence</p>"},{"location":"reference/prompts/#3-tool-interpretation-guidelines","title":"3. Tool Interpretation Guidelines","text":"<p>The system prompt includes detailed guidance on: - How to interpret tool scores (relative, not absolute thresholds) - Distinction between manipulation detection and synthetic detection - When to trust tools vs. visual analysis</p>"},{"location":"reference/prompts/#4-swgde-best-practices-integration","title":"4. SWGDE Best Practices Integration","text":"<p>The system prompt automatically includes SWGDE Image Authentication best practices when available, ensuring alignment with forensic standards.</p>"},{"location":"reference/prompts/#modifying-prompts","title":"Modifying Prompts","text":""},{"location":"reference/prompts/#python-prompts","title":"Python Prompts","text":"<p>Edit <code>src/agents/prompts.py</code>:</p> <pre><code># Modify the base prompt\ndef get_system_prompt() -&gt; str:\n    base_prompt = \"\"\"Your updated prompt here...\"\"\"\n    # ...\n</code></pre>"},{"location":"reference/prompts/#baml-prompts","title":"BAML Prompts","text":"<p>Edit <code>baml_src/forensic_analysis.baml</code>:</p> <pre><code>function AnalyzeImageVisionOnly(image: image) -&gt; string {\n  client DynamicForensicClient\n  prompt #\"\n    Your updated prompt here...\n  \"#\n}\n</code></pre> <p>Important: After modifying BAML files, regenerate the Python client:</p> <pre><code>baml-cli generate\n</code></pre>"},{"location":"reference/prompts/#related-documentation","title":"Related Documentation","text":"<ul> <li>Agent Pipeline - How prompts are used in the analysis flow</li> <li>BAML Integration - BAML prompt system details</li> <li>SWGDE Best Practices - Forensic standards integrated into prompts</li> </ul>"},{"location":"reference/swgde/","title":"SWGDE Alignment","text":"<p>How DF3 aligns with SWGDE (Scientific Working Group on Digital Evidence) best practices for image authentication.</p>"},{"location":"reference/swgde/#overview","title":"Overview","text":"<p>SWGDE establishes best practices for digital forensics. DF3's reporting and methodology are designed to align with the \"Best Practices for Image Authentication\" (2022) document.</p>"},{"location":"reference/swgde/#core-principles-mapping","title":"Core Principles Mapping","text":""},{"location":"reference/swgde/#documentation","title":"Documentation","text":"SWGDE Requirement DF3 Implementation Examiner information Report header Evidence identification Image path, hash, metadata Methodology description Tool usage log, prompts Findings Structured verdict + rationale Conclusions REAL/FAKE/UNCERTAIN Limitations Stated in report footer"},{"location":"reference/swgde/#reproducibility","title":"Reproducibility","text":"SWGDE Requirement DF3 Implementation Tool versions Model identifiers in results Parameters Temperature, max_iterations recorded Intermediate results Tool outputs preserved in raw results"},{"location":"reference/swgde/#objectivity","title":"Objectivity","text":"SWGDE Requirement DF3 Implementation Multiple techniques Vision LLM + forensic tools Corroborating evidence Cross-reference in agent reasoning Acknowledge uncertainty UNCERTAIN verdict Evidence-based reasoning Prompts enforce factual grounding"},{"location":"reference/swgde/#report-structure","title":"Report Structure","text":"<p>DF3 generates reports with the following structure:</p>"},{"location":"reference/swgde/#1-header","title":"1. Header","text":"<pre><code># Image Authentication Report\n**Analysis Date**: 2026-01-15\n**File**: sample-001.jpg\n</code></pre>"},{"location":"reference/swgde/#2-evidence-information","title":"2. Evidence Information","text":"<pre><code>## Evidence\n- **SHA-256**: abc123...\n- **File Size**: 1.2 MB\n- **Dimensions**: 1920x1080\n</code></pre>"},{"location":"reference/swgde/#3-methodology","title":"3. Methodology","text":"<pre><code>## Methodology\n- Vision Model: gpt-5.1\n- Agent Model: gpt-5.1\n- Tools: TruFor, ELA, Metadata\n- Mode: Agentic (tools enabled)\n</code></pre>"},{"location":"reference/swgde/#4-observations","title":"4. Observations","text":"<pre><code>## Observations\n### Visual Analysis\n[Description of image content and anomalies]\n\n### Tool Results\n- TruFor: manipulation_probability = 0.85\n- ELA: anomaly_score = 3.2\n- Metadata: No C2PA; EXIF stripped\n</code></pre>"},{"location":"reference/swgde/#5-interpretation","title":"5. Interpretation","text":"<pre><code>## Interpretation\nHigh TruFor score combined with ELA anomalies \nsuggests localized editing. Absent metadata is \nconsistent with post-processing.\n</code></pre>"},{"location":"reference/swgde/#6-limitations","title":"6. Limitations","text":"<pre><code>## Limitations\n- Confidence is model self-report, not calibrated probability\n- AI-generated images may not trigger manipulation detectors\n- Heavy compression affects tool accuracy\n</code></pre>"},{"location":"reference/swgde/#7-conclusion","title":"7. Conclusion","text":"<pre><code>## Conclusion\n**Verdict**: FAKE\n**Confidence**: 0.85\n</code></pre>"},{"location":"reference/swgde/#key-distinctions","title":"Key Distinctions","text":""},{"location":"reference/swgde/#manipulation-vs-synthetic-detection","title":"Manipulation vs. Synthetic Detection","text":"Task Description DF3 Capability Manipulation Detection Has image been edited? Strong (TruFor, ELA) Synthetic Detection Is image AI-generated? Variable (visual analysis) Authentication Is image what it claims? Contextual <p>DF3 addresses both content (visual) and structure (tools) as SWGDE recommends.</p>"},{"location":"reference/swgde/#confidence-score-usage","title":"Confidence Score Usage","text":"<p>The <code>confidence</code> score is the LLM's self-reported certainty.</p> <p>Appropriate uses:</p> <ul> <li>Triage ranking</li> <li>Prioritizing review queue</li> <li>Identifying uncertain cases</li> </ul> <p>Not appropriate for:</p> <ul> <li>Probability statements in reports</li> <li>Statistical likelihood ratios</li> </ul>"},{"location":"reference/swgde/#external-references","title":"External References","text":"<ul> <li>SWGDE Publications</li> <li>Best Practices for Image Authentication (2022)</li> </ul>"},{"location":"reference/swgde/#see-also","title":"See Also","text":"<ul> <li>Interpreting Results</li> <li>Limitations</li> </ul>"},{"location":"reference/troubleshooting/","title":"Troubleshooting","text":""},{"location":"reference/troubleshooting/#installation","title":"Installation","text":""},{"location":"reference/troubleshooting/#modulenotfounderror-no-module-named-x","title":"\"ModuleNotFoundError: No module named 'X'\"","text":"<p>Ensure venv is activated and dependencies installed:</p> <pre><code>.\\venv\\Scripts\\Activate.ps1\npip install -r requirements.txt\n</code></pre>"},{"location":"reference/troubleshooting/#baml-cli-command-not-found","title":"\"baml-cli: command not found\"","text":"<pre><code>pip install baml-py\nbaml-cli generate\n</code></pre>"},{"location":"reference/troubleshooting/#trufor-weights-download-fails","title":"TruFor weights download fails","text":"<ol> <li>Check internet connection</li> <li>Verify disk space (~500MB needed)</li> <li>Manual download to <code>weights/trufor/</code></li> </ol>"},{"location":"reference/troubleshooting/#api","title":"API","text":""},{"location":"reference/troubleshooting/#api-key-not-found","title":"\"API key not found\"","text":"<pre><code># Check\n$env:OPENAI_API_KEY\n\n# Set\n$env:OPENAI_API_KEY = \"sk-...\"\n</code></pre>"},{"location":"reference/troubleshooting/#rate-limit-exceeded","title":"\"Rate limit exceeded\"","text":"<p>Reduce parallel workers:</p> <pre><code>python scripts/evaluate_llms.py --num-workers 1\n</code></pre>"},{"location":"reference/troubleshooting/#model-not-found","title":"\"Model not found\"","text":"<p>Verify model name spelling and availability for your API key.</p>"},{"location":"reference/troubleshooting/#analysis","title":"Analysis","text":""},{"location":"reference/troubleshooting/#image-file-not-found","title":"\"Image file not found\"","text":"<pre><code># Verify path\nTest-Path \"path/to/image.jpg\"\n\n# Use absolute path\npython scripts/analyze_image.py --image \"C:\\full\\path\\image.jpg\"\n</code></pre>"},{"location":"reference/troubleshooting/#analysis-hangs","title":"Analysis hangs","text":"<p>Possible causes: - Large image \u2192 resize or use smaller test image - Slow model \u2192 try <code>gpt-5-mini</code> - Network issues \u2192 check connection</p>"},{"location":"reference/troubleshooting/#cuda-out-of-memory","title":"\"CUDA out of memory\"","text":"<p>Force CPU mode:</p> <pre><code>$env:CUDA_VISIBLE_DEVICES = \"\"\n</code></pre>"},{"location":"reference/troubleshooting/#evaluation","title":"Evaluation","text":""},{"location":"reference/troubleshooting/#results-show-all-errors","title":"Results show all errors","text":"<ol> <li>Verify API key</li> <li>Test single image first: <code>python scripts/analyze_image.py --image test.jpg</code></li> <li>Check error messages in results JSONL</li> </ol>"},{"location":"reference/troubleshooting/#metrics-show-nan","title":"Metrics show NaN","text":"<ul> <li>Check raw results for errors</li> <li>Ensure dataset has both classes</li> <li>Verify model is producing verdicts</li> </ul>"},{"location":"reference/troubleshooting/#baml","title":"BAML","text":""},{"location":"reference/troubleshooting/#baml-generation-failed","title":"\"BAML generation failed\"","text":"<pre><code>baml-cli generate --verbose\n</code></pre> <p>Check syntax in <code>baml_src/*.baml</code>.</p>"},{"location":"reference/troubleshooting/#type-mismatch-in-output","title":"Type mismatch in output","text":"<ol> <li>Verify BAML function definition</li> <li>Ensure prompt includes <code>{{ ctx.output_format }}</code></li> <li>Try different temperature</li> </ol>"},{"location":"reference/troubleshooting/#tool-specific","title":"Tool-Specific","text":""},{"location":"reference/troubleshooting/#ela-returns-skipped","title":"ELA returns \"skipped\"","text":"<p>Expected for non-JPEG images. ELA only works on JPEG.</p>"},{"location":"reference/troubleshooting/#trufor-returns-unexpected-values","title":"TruFor returns unexpected values","text":"<pre><code># Re-download weights\nRemove-Item -Recurse weights/trufor\n# Weights auto-download on next run\n</code></pre>"},{"location":"reference/troubleshooting/#performance","title":"Performance","text":"Issue Solution Slow analysis Resize images, use GPU, enable cache High memory Reduce <code>--num-workers</code> High latency Use faster model, enable tool cache"},{"location":"reference/troubleshooting/#debug-mode","title":"Debug Mode","text":"<pre><code>$env:DF3_LOG_LEVEL = \"DEBUG\"\npython scripts/analyze_image.py --image photo.jpg\n</code></pre>"},{"location":"reference/troubleshooting/#see-also","title":"See Also","text":"<ul> <li>Installation</li> <li>Environment Variables</li> <li>FAQ</li> </ul>"},{"location":"research/delivery-checklist/","title":"Delivery Checklist","text":"<p>Checklist for preparing DF3 deliverables for external labs or publication.</p>"},{"location":"research/delivery-checklist/#pre-delivery-checklist","title":"Pre-Delivery Checklist","text":""},{"location":"research/delivery-checklist/#code-environment","title":"Code &amp; Environment","text":"<ul> <li> Repository clean \u2014 No debug code, commented blocks, or sensitive data</li> <li> Dependencies documented \u2014 <code>requirements.txt</code> with pinned versions</li> <li> Installation tested \u2014 Fresh environment setup verified</li> <li> README updated \u2014 Current instructions, no stale info</li> <li> License included \u2014 Appropriate license file</li> </ul>"},{"location":"research/delivery-checklist/#documentation","title":"Documentation","text":"<ul> <li> User guide complete \u2014 All workflows documented</li> <li> API reference current \u2014 Matches actual code</li> <li> Examples working \u2014 All code examples tested</li> <li> Limitations documented \u2014 Honest capability assessment</li> </ul>"},{"location":"research/delivery-checklist/#data-datasets","title":"Data &amp; Datasets","text":"<ul> <li> Dataset provenance \u2014 Sources and licenses documented</li> <li> Sample data included \u2014 Test images for verification</li> <li> Manifests provided \u2014 If full data can't be shared</li> <li> Privacy checked \u2014 No PII or sensitive content</li> </ul>"},{"location":"research/delivery-checklist/#evaluation-results","title":"Evaluation Results","text":"<ul> <li> Results reproducible \u2014 Re-run and verify</li> <li> Methodology documented \u2014 All parameters recorded</li> <li> Metrics explained \u2014 Definitions and interpretations</li> <li> Confidence intervals \u2014 Uncertainty quantified</li> </ul>"},{"location":"research/delivery-checklist/#code-review-checklist","title":"Code Review Checklist","text":""},{"location":"research/delivery-checklist/#security","title":"Security","text":"<ul> <li> No hardcoded credentials</li> <li> API keys loaded from environment</li> <li> No sensitive data in logs</li> <li> Input validation present</li> </ul>"},{"location":"research/delivery-checklist/#quality","title":"Quality","text":"<ul> <li> No TODO comments unaddressed</li> <li> No commented-out code blocks</li> <li> Consistent code style</li> <li> Meaningful variable names</li> </ul>"},{"location":"research/delivery-checklist/#functionality","title":"Functionality","text":"<ul> <li> All scripts executable</li> <li> Error handling present</li> <li> Edge cases considered</li> <li> Resource cleanup (files, connections)</li> </ul>"},{"location":"research/delivery-checklist/#documentation-checklist","title":"Documentation Checklist","text":""},{"location":"research/delivery-checklist/#required-documents","title":"Required Documents","text":"Document Purpose Status README.md Quick start [ ] INSTALL.md Detailed setup [ ] LICENSE Legal terms [ ] CHANGELOG.md Version history [ ] CONTRIBUTING.md Contribution guide [ ]"},{"location":"research/delivery-checklist/#user-documentation","title":"User Documentation","text":"Section Content Status Quick Start 5-minute setup [ ] Installation Full environment setup [ ] User Guide Workflow tutorials [ ] API Reference Function documentation [ ] Troubleshooting Common issues [ ] FAQ Frequent questions [ ]"},{"location":"research/delivery-checklist/#technical-documentation","title":"Technical Documentation","text":"Section Content Status Architecture System design [ ] Data Flow Processing pipeline [ ] Tool Reference Forensic tool details [ ] Evaluation Methodology &amp; metrics [ ] Reproducibility Reproduction guide [ ] Limitations Honest assessment [ ]"},{"location":"research/delivery-checklist/#dataset-delivery-checklist","title":"Dataset Delivery Checklist","text":""},{"location":"research/delivery-checklist/#if-including-full-dataset","title":"If Including Full Dataset","text":"<ul> <li> License permits redistribution</li> <li> Sources credited properly</li> <li> No copyright violations</li> <li> Privacy-safe content</li> </ul>"},{"location":"research/delivery-checklist/#if-manifest-only","title":"If Manifest-Only","text":"<ul> <li> Retrieval instructions provided</li> <li> SHA256 hashes for verification</li> <li> Source URLs documented</li> <li> Sampling methodology described</li> </ul>"},{"location":"research/delivery-checklist/#dataset-documentation","title":"Dataset Documentation","text":"<ul> <li> Sample counts verified</li> <li> Class balance documented</li> <li> Source datasets listed</li> <li> Processing steps described</li> </ul>"},{"location":"research/delivery-checklist/#evaluation-delivery-checklist","title":"Evaluation Delivery Checklist","text":""},{"location":"research/delivery-checklist/#results-artifacts","title":"Results Artifacts","text":"<ul> <li> Raw results file (<code>.jsonl</code>)</li> <li> Aggregated metrics (<code>.json</code>)</li> <li> Summary tables (<code>.md</code>)</li> <li> Configuration used (<code>.json</code>)</li> </ul>"},{"location":"research/delivery-checklist/#metadata","title":"Metadata","text":"<ul> <li> Model versions recorded</li> <li> Dataset digest included</li> <li> Timestamp documented</li> <li> Git commit recorded</li> </ul>"},{"location":"research/delivery-checklist/#validation","title":"Validation","text":"<ul> <li> Metrics recomputable from raw results</li> <li> Sample spot-checked manually</li> <li> Edge cases reviewed</li> <li> Error cases analyzed</li> </ul>"},{"location":"research/delivery-checklist/#final-quality-checks","title":"Final Quality Checks","text":""},{"location":"research/delivery-checklist/#fresh-install-test","title":"Fresh Install Test","text":"<pre><code># Clone fresh copy\ngit clone $REPO_URL test-install\ncd test-install\n\n# Create environment\npython -m venv venv\n.\\venv\\Scripts\\Activate.ps1\n\n# Install\npip install -r requirements.txt\n\n# Run basic test\npython scripts/analyze_image.py --image data/sample.jpg\n\n# Run evaluation sample\npython scripts/evaluate_llms.py --dataset data/sample.jsonl --limit 10\n</code></pre>"},{"location":"research/delivery-checklist/#documentation-accuracy","title":"Documentation Accuracy","text":"<ul> <li> All commands execute successfully</li> <li> All file paths exist</li> <li> All links resolve</li> <li> Screenshots current</li> </ul>"},{"location":"research/delivery-checklist/#completeness","title":"Completeness","text":"<ul> <li> All listed features work</li> <li> All documented APIs functional</li> <li> All examples produce expected output</li> <li> All configuration options work</li> </ul>"},{"location":"research/delivery-checklist/#packaging-options","title":"Packaging Options","text":""},{"location":"research/delivery-checklist/#option-a-github-repository","title":"Option A: GitHub Repository","text":"<pre><code>Deliverable: Repository URL + access\nContents:\n  - Full source code\n  - Documentation in markdown_docs/\n  - Sample data\n  - Results artifacts\n</code></pre>"},{"location":"research/delivery-checklist/#option-b-zip-archive","title":"Option B: Zip Archive","text":"<pre><code>df3-v1.0.0.zip\n\u251c\u2500\u2500 src/\n\u251c\u2500\u2500 scripts/\n\u251c\u2500\u2500 markdown_docs/\n\u251c\u2500\u2500 data/sample/\n\u251c\u2500\u2500 results/\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 LICENSE\n\u2514\u2500\u2500 INSTALL.md\n</code></pre>"},{"location":"research/delivery-checklist/#option-c-docker-container","title":"Option C: Docker Container","text":"<pre><code># Dockerfile for complete environment\nFROM python:3.11-slim\nCOPY . /app\nWORKDIR /app\nRUN pip install -r requirements.txt\nCMD [\"python\", \"scripts/analyze_image.py\", \"--help\"]\n</code></pre>"},{"location":"research/delivery-checklist/#handoff-documentation","title":"Handoff Documentation","text":""},{"location":"research/delivery-checklist/#cover-letter-contents","title":"Cover Letter Contents","text":"<ol> <li>Overview \u2014 What DF3 is and does</li> <li>Key Capabilities \u2014 Main features</li> <li>Setup Instructions \u2014 Quick start reference</li> <li>Known Limitations \u2014 Honest assessment</li> <li>Support Contact \u2014 How to get help</li> </ol>"},{"location":"research/delivery-checklist/#training-materials-if-needed","title":"Training Materials (if needed)","text":"<ul> <li> Video walkthrough</li> <li> Live demo session</li> <li> Q&amp;A documentation</li> <li> Office hours schedule</li> </ul>"},{"location":"research/delivery-checklist/#post-delivery","title":"Post-Delivery","text":""},{"location":"research/delivery-checklist/#support-plan","title":"Support Plan","text":"<ul> <li> Issue tracking set up</li> <li> Response time defined</li> <li> Escalation path documented</li> <li> Maintenance schedule planned</li> </ul>"},{"location":"research/delivery-checklist/#version-control","title":"Version Control","text":"<ul> <li> Release tagged</li> <li> Changelog updated</li> <li> Version number bumped</li> <li> Dependencies frozen</li> </ul>"},{"location":"research/delivery-checklist/#quick-pre-delivery-commands","title":"Quick Pre-Delivery Commands","text":"<pre><code># Check for secrets\nSelect-String -Pattern \"sk-|api[_-]key|password\" -Path src/**/*.py\n\n# Check for TODOs\nSelect-String -Pattern \"TODO|FIXME|XXX\" -Path src/**/*.py\n\n# Verify requirements\npip check\n\n# Run tests\npython -m pytest tests/\n\n# Build docs\nmkdocs build\n\n# Check for large files\nGet-ChildItem -Recurse | Where-Object {$_.Length -gt 10MB}\n</code></pre>"},{"location":"research/delivery-checklist/#see-also","title":"See Also","text":"<ul> <li>Installation \u2014 Setup guide</li> <li>Reproducibility \u2014 Reproduction requirements</li> <li>Limitations \u2014 Known issues</li> </ul>"},{"location":"research/design-decisions/","title":"Design Decisions","text":"<p>This document captures the key architectural and design decisions made during DF3 development, including alternatives considered, rationale for choices made, and lessons learned from approaches that were tried and abandoned.</p>"},{"location":"research/design-decisions/#overview","title":"Overview","text":"<p>DF3 evolved through several iterations. Understanding why the system is designed the way it is helps future researchers build on this work without repeating unsuccessful approaches.</p>"},{"location":"research/design-decisions/#decision-1-agent-framework-langgraph","title":"Decision 1: Agent Framework (LangGraph)","text":""},{"location":"research/design-decisions/#choice-made","title":"Choice Made","text":"<p>LangGraph with the ReAct agent pattern for tool orchestration.</p>"},{"location":"research/design-decisions/#alternatives-considered","title":"Alternatives Considered","text":"Option Pros Cons LangGraph ReAct Built-in tool calling, state management, checkpointing Heavier dependency, learning curve LangChain AgentExecutor Simpler API, well-documented Less flexible, deprecated patterns Custom loop Full control, minimal dependencies More code to maintain, reinventing wheels Autogen/CrewAI Multi-agent support Overkill for single-agent task"},{"location":"research/design-decisions/#rationale","title":"Rationale","text":"<p>LangGraph was selected because:</p> <ol> <li>Native tool calling: Handles the ReAct loop (Reason \u2192 Act \u2192 Observe) cleanly</li> <li>State management: Built-in checkpointing via MemorySaver</li> <li>Iteration control: <code>recursion_limit</code> parameter prevents infinite loops</li> <li>Future flexibility: Could extend to multi-agent workflows if needed</li> </ol>"},{"location":"research/design-decisions/#lessons-learned","title":"Lessons Learned","text":"<p>The ReAct pattern works well for tool orchestration but introduces failure modes: - Tool selection errors accumulate over iterations - Long reasoning chains can drift from the task - Agent may call unnecessary tools, adding latency</p> <p>Recommendation: For simpler tasks, a single-shot approach may be preferable. Use agents only when dynamic tool selection is genuinely needed.</p>"},{"location":"research/design-decisions/#decision-2-structured-output-baml","title":"Decision 2: Structured Output (BAML)","text":""},{"location":"research/design-decisions/#choice-made_1","title":"Choice Made","text":"<p>BAML (Boundary AI Markup Language) for structured output extraction, using a two-step approach: 1. Free-form reasoning (no structure constraints) 2. Separate structuring call to extract JSON</p>"},{"location":"research/design-decisions/#alternatives-considered_1","title":"Alternatives Considered","text":"Option Pros Cons BAML two-step Preserves reasoning quality, type-safe outputs Two LLM calls, added latency OpenAI JSON mode Single call, native support Reasoning degradation when constrained Pydantic + instructor Type-safe, popular Still constrains during generation Regex extraction No extra LLM call Fragile, fails on format variations"},{"location":"research/design-decisions/#rationale_1","title":"Rationale","text":"<p>Research shows that requiring structured output during reasoning can degrade LLM performance (\"reasoning degradation\"). By separating reasoning from structuring:</p> <ol> <li>The reasoning phase uses full model capabilities</li> <li>The structuring phase is a simpler extraction task</li> <li>Both outputs are available (raw text + structured)</li> </ol>"},{"location":"research/design-decisions/#implementation","title":"Implementation","text":"<pre><code># Step 1: Free reasoning\nvision_output = await AnalyzeImageVisionOnly(image)  # Returns markdown\n\n# Step 2: Structure extraction\nstructured = await StructureForensicAnalysis(vision_output)  # Returns typed object\n</code></pre>"},{"location":"research/design-decisions/#lessons-learned_1","title":"Lessons Learned","text":"<p>The two-step approach works well: - Reasoning quality is preserved - Structuring rarely fails (it's just extraction) - Cost is acceptable (structuring can use cheaper model)</p> <p>Recommendation: Use this pattern for any task where both reasoning quality and structured output matter.</p>"},{"location":"research/design-decisions/#decision-3-vision-model-separation","title":"Decision 3: Vision Model Separation","text":""},{"location":"research/design-decisions/#choice-made_2","title":"Choice Made","text":"<p>Allow separate models for vision, agent reasoning, and structuring: - <code>vision_model</code>: Initial image analysis - <code>llm_model</code>: Agent reasoning and tool orchestration - <code>structuring_model</code>: BAML output extraction</p>"},{"location":"research/design-decisions/#alternatives-considered_2","title":"Alternatives Considered","text":"Option Pros Cons Single model for all Simpler configuration Forces trade-offs (cost vs. capability) Separate models Optimize each stage independently More complex configuration"},{"location":"research/design-decisions/#rationale_2","title":"Rationale","text":"<p>Different stages have different requirements:</p> Stage Requirement Optimal Model Vision Strong visual understanding Gemini, GPT-4o, Claude Agent Reasoning, tool use Can be cheaper Structuring JSON extraction Can be cheapest <p>This allows cost optimization: use a capable vision model, then cheaper models for downstream tasks.</p>"},{"location":"research/design-decisions/#lessons-learned_2","title":"Lessons Learned","text":"<p>In practice, vision model choice dominates performance: - Gemini 3 Flash as vision model: 92.8% accuracy - GPT-5.2 as vision model: 7.4% accuracy (with 92% abstention)</p> <p>The agent and structuring models matter less. Future work should focus on vision model selection.</p>"},{"location":"research/design-decisions/#decision-4-tool-architecture","title":"Decision 4: Tool Architecture","text":""},{"location":"research/design-decisions/#choice-made_3","title":"Choice Made","text":"<p>Modular tool design with each tool in a separate file, registered via <code>@tool</code> decorator.</p>"},{"location":"research/design-decisions/#tool-selection","title":"Tool Selection","text":"Tool Included Rationale TruFor Yes State-of-the-art manipulation detection ELA Yes Classic forensic technique, interpretable JPEG analysis Yes Compression artifact detection Frequency analysis Yes DCT/FFT for pattern detection Residual extraction Yes DRUNet noise patterns Metadata Yes EXIF/C2PA, no ML required Code execution Yes Flexibility for custom analysis CFA Disabled Unreliable results, high false positive rate"},{"location":"research/design-decisions/#why-cfa-was-disabled","title":"Why CFA Was Disabled","text":"<p>Color Filter Array (CFA) analysis detects inconsistencies in Bayer pattern interpolation. In testing: - High false positive rate on legitimate images - Inconsistent results across image formats - Limited discriminative power for AI-generated images</p> <p>Rather than produce misleading evidence, we disabled it.</p>"},{"location":"research/design-decisions/#lessons-learned_3","title":"Lessons Learned","text":"<p>Tool selection should match the task. Our tools were designed for manipulation detection, but our dataset was AI-generated images. This mismatch is a primary cause of the \"tools hurt performance\" finding.</p> <p>Recommendation: Develop or integrate tools specifically designed for AI-generated detection: - GAN fingerprint detectors - Diffusion model artifact detectors - Semantic consistency analyzers</p>"},{"location":"research/design-decisions/#decision-5-prompt-engineering-strategy","title":"Decision 5: Prompt Engineering Strategy","text":""},{"location":"research/design-decisions/#choice-made_4","title":"Choice Made","text":"<p>Explicit prompts with detailed instructions, including: - Task definition - Visual inspection guidelines - Tool interpretation rules - Common pitfall warnings - Output format specification</p>"},{"location":"research/design-decisions/#evolution-of-prompts","title":"Evolution of Prompts","text":"<p>The prompts evolved significantly:</p>"},{"location":"research/design-decisions/#version-1-early","title":"Version 1 (Early)","text":"<pre><code>Analyze this image and determine if it is real or fake.\n</code></pre> <p>Problem: No guidance on what to look for, inconsistent outputs.</p>"},{"location":"research/design-decisions/#version-2-mid","title":"Version 2 (Mid)","text":"<pre><code>Look for: anatomical errors, texture anomalies, lighting issues...\nOutput: JSON with verdict, confidence, rationale\n</code></pre> <p>Problem: Models conflated \"no manipulation\" with \"real\".</p>"},{"location":"research/design-decisions/#version-3-final","title":"Version 3 (Final)","text":"<pre><code>IMPORTANT DISTINCTION:\n- \"No evidence of manipulation\" is NOT the same as \"not synthetic\"\n- AI-generated images can score low on manipulation tools\n\nWhen to trust tools vs. visual analysis...\n</code></pre> <p>Key insight: Explicit warnings about common errors are necessary. LLMs will make logical mistakes (e.g., \"TruFor low \u2192 image is real\") unless explicitly instructed not to.</p>"},{"location":"research/design-decisions/#what-didnt-work","title":"What Didn't Work","text":"<ol> <li>Numeric thresholds: Telling the model \"TruFor &gt; 0.8 means fake\" led to overreliance on arbitrary cutoffs</li> <li>Confidence calibration instructions: Asking for \"calibrated\" confidence didn't improve calibration</li> <li>Chain-of-thought forcing: Requiring explicit reasoning steps didn't improve accuracy</li> </ol>"},{"location":"research/design-decisions/#what-worked","title":"What Worked","text":"<ol> <li>Distinction between manipulation and synthesis: Explicit, repeated</li> <li>SWGDE best practices inclusion: Added authoritative guidance</li> <li>Uncertain as valid output: Explicitly permitting abstention reduced false confidence</li> <li>Concrete visual indicators: Lists of what to look for (fingers, skin texture, etc.)</li> </ol>"},{"location":"research/design-decisions/#decision-6-caching-strategy","title":"Decision 6: Caching Strategy","text":""},{"location":"research/design-decisions/#choice-made_5","title":"Choice Made","text":"<p>Multi-level caching with separate caches for: - Vision outputs (keyed by image hash + model + prompt hash) - Tool outputs (keyed by image hash + tool + parameters) - Image encoding (LRU cache, 32 images)</p>"},{"location":"research/design-decisions/#rationale_3","title":"Rationale","text":"<p>Forensic tools are computationally expensive: - TruFor: 2-5 seconds per image (GPU) - Residual extraction: 1-3 seconds per image - Vision LLM call: 3-10 seconds</p> <p>For batch evaluation, caching eliminates redundant computation when: - Re-running with different agent models (vision cache) - Re-running with different configurations (tool cache) - Processing the same image multiple times (encoding cache)</p>"},{"location":"research/design-decisions/#trade-offs","title":"Trade-offs","text":"Pro Con Faster iteration Cache invalidation complexity Reproducible results Disk space usage Cost savings (fewer LLM calls) Stale results if prompts change"},{"location":"research/design-decisions/#implementation-detail","title":"Implementation Detail","text":"<pre><code># Cache keyed by content hash, not path\nimage_hash = hashlib.sha256(image_bytes).hexdigest()[:16]\ncache_key = f\"{tool_name}_{image_hash}_{param_hash}\"\n</code></pre>"},{"location":"research/design-decisions/#decision-7-three-way-classification","title":"Decision 7: Three-Way Classification","text":""},{"location":"research/design-decisions/#choice-made_6","title":"Choice Made","text":"<p>Three-way output: <code>real</code>, <code>fake</code>, <code>uncertain</code> (instead of binary classification).</p>"},{"location":"research/design-decisions/#rationale_4","title":"Rationale","text":"<p>Forensic contexts require appropriate uncertainty:</p> Scenario Binary System Three-Way System Clear fake fake fake Clear real real real Ambiguous forced guess uncertain \u2192 human review <p>Forced binary classification on ambiguous images produces errors. Allowing \"uncertain\" enables: - Higher accuracy on answered samples - Intelligent triage to human reviewers - Honest uncertainty communication</p>"},{"location":"research/design-decisions/#lessons-learned_4","title":"Lessons Learned","text":"<p>Abstention rates vary dramatically by model: - Gemini 3 Flash: 1.6% abstention - GPT-5.2: 92% abstention</p> <p>The three-way system exposed model-specific behaviors that would be hidden in binary classification.</p>"},{"location":"research/design-decisions/#decision-8-evaluation-metrics","title":"Decision 8: Evaluation Metrics","text":""},{"location":"research/design-decisions/#choice-made_7","title":"Choice Made","text":"<p>Selective classification metrics that treat abstention appropriately: - <code>accuracy</code>: Overall (abstentions count as wrong) - <code>accuracy_answered</code>: Among answered samples only - <code>coverage</code>: Fraction of samples answered</p>"},{"location":"research/design-decisions/#why-not-just-accuracy","title":"Why Not Just Accuracy?","text":"<p>Binary accuracy conflates two types of errors: 1. Wrong answers (false positives, false negatives) 2. Non-answers (abstentions)</p> <p>A system that abstains 99% of the time and gets 1% right has 1% accuracy but 100% accuracy-when-answered. Both numbers are meaningful for different decisions.</p>"},{"location":"research/design-decisions/#metrics-selected","title":"Metrics Selected","text":"Metric Purpose accuracy Overall system performance accuracy_answered Quality when confident coverage System usefulness MCC Balanced measure for imbalanced data fake_slip_rate Fakes incorrectly passed real_false_flag_rate Reals incorrectly flagged"},{"location":"research/design-decisions/#approaches-tried-and-abandoned","title":"Approaches Tried and Abandoned","text":""},{"location":"research/design-decisions/#approach-1-confidence-based-tool-selection","title":"Approach 1: Confidence-Based Tool Selection","text":"<p>Idea: Only invoke tools when vision confidence is below threshold.</p> <p>Implementation: <pre><code>if vision_confidence &lt; 0.7:\n    run_tools()\n</code></pre></p> <p>Result: No improvement. Low-confidence cases are often hard cases where tools also struggle.</p> <p>Lesson: Confidence doesn't predict when tools will help.</p>"},{"location":"research/design-decisions/#approach-2-tool-output-summarization","title":"Approach 2: Tool Output Summarization","text":"<p>Idea: Summarize verbose tool outputs before passing to agent.</p> <p>Implementation: LLM call to condense tool JSON to 2-3 sentences.</p> <p>Result: Added latency, minimal accuracy change. The summarization sometimes lost critical details.</p> <p>Lesson: Let the agent see raw outputs; it can extract what it needs.</p>"},{"location":"research/design-decisions/#approach-3-ensemble-multiple-models","title":"Approach 3: Ensemble Multiple Models","text":"<p>Idea: Run multiple vision models, vote on verdict.</p> <p>Implementation: 3 models, majority vote.</p> <p>Result: Slower, more expensive, marginal accuracy improvement. Best single model was nearly as good.</p> <p>Lesson: Model selection &gt; model ensembling for this task.</p>"},{"location":"research/design-decisions/#approach-4-fine-grained-verdict","title":"Approach 4: Fine-Grained Verdict","text":"<p>Idea: Distinguish \"AI-generated\" from \"manipulated\" in the verdict.</p> <p>Implementation: Four-way: <code>real</code>, <code>ai_generated</code>, <code>manipulated</code>, <code>uncertain</code>.</p> <p>Result: Models struggled to distinguish generation from manipulation. Added complexity without improving usefulness.</p> <p>Lesson: Keep verdicts simple. The rationale can provide nuance.</p>"},{"location":"research/design-decisions/#recommendations-for-future-development","title":"Recommendations for Future Development","text":"<p>Based on these experiences:</p> <ol> <li> <p>Start simple: Vision-only is a strong baseline. Add complexity only if it helps.</p> </li> <li> <p>Match tools to task: If detecting AI-generated images, use AI-generation detectors, not manipulation detectors.</p> </li> <li> <p>Test models individually: Don't assume results transfer. Evaluate each model on your specific task.</p> </li> <li> <p>Preserve raw outputs: Cache and log everything. You'll want to reanalyze later.</p> </li> <li> <p>Explicit &gt; implicit: When LLMs make logical errors, add explicit instructions to prevent them.</p> </li> <li> <p>Measure what matters: Abstention rates, latency, and cost matter as much as accuracy.</p> </li> </ol>"},{"location":"research/design-decisions/#see-also","title":"See Also","text":"<ul> <li>Research Findings \u2014 Empirical results and analysis</li> <li>Architecture Overview \u2014 System design</li> <li>Agent Pipeline \u2014 Implementation details</li> </ul>"},{"location":"research/findings/","title":"Research Findings","text":""},{"location":"research/findings/#summary","title":"Summary","text":"<p>DF3 demonstrates that LLM-based forensic analysis can provide explainable, auditable image authentication suitable for triage workflows and human-in-the-loop forensic processes. While raw accuracy metrics favor traditional ML approaches, the system's ability to articulate reasoning in natural language represents a distinct capability valuable for forensic applications.</p> <p>Key Findings:</p> <ol> <li>Vision-capable LLMs can detect AI-generated images with reasonable accuracy (up to 92.8%), but this performance may be confounded by training data overlap</li> <li>Adding forensic tools did not improve detection accuracy in our evaluation, likely due to tool-task mismatch and the challenge of zero-shot tool interpretation</li> <li>The primary value proposition is explainability: LLM-generated reasoning provides transparent, auditable analysis suitable for forensic reporting</li> <li>The system enables effective triage workflows where uncertain cases route to human experts</li> </ol>"},{"location":"research/findings/#research-context","title":"Research Context","text":""},{"location":"research/findings/#the-two-phase-approach-df2-and-df3","title":"The Two-Phase Approach: DF2 and DF3","text":"<p>This research builds on DF2, a traditional machine learning classifier developed in the first phase:</p> Aspect DF2 (Random Forest/Logistic) DF3 (LLM Agent) Architecture Logistic regression on extracted features Vision LLM + LangGraph ReAct agent Training 166,000+ images No task-specific training Features 21 scalar forensic features Same tools, different interface Primary output Probability score P(fake) Verdict + natural language reasoning Accuracy (AUROC) 0.9996 Not directly comparable Explainability Template-based evidence citing Free-form reasoning with evidence <p>Critical insight from DF2: The forensic tools (Noiseprint, DRUNet residuals, frequency analysis) do provide discriminative signal. DF2's ablation study demonstrates: - Removing Noiseprint: AUROC drops from 0.9966 to 0.9777 (-1.89%) - Removing frequency features: AUROC drops to 0.9829 (-1.37%)</p> <p>The tools work. The question is whether LLMs can interpret them effectively without training.</p>"},{"location":"research/findings/#research-questions","title":"Research Questions","text":"<ol> <li>Can LLMs provide explainable forensic image analysis?</li> <li>Can LLMs effectively interpret forensic tool outputs without task-specific training?</li> <li>What is the appropriate role for LLM-based detection in forensic workflows?</li> </ol>"},{"location":"research/findings/#experimental-design","title":"Experimental Design","text":""},{"location":"research/findings/#dataset","title":"Dataset","text":"Property Value Total samples 500 Class balance 247 fake (49.4%), 253 real (50.6%) Fake sources GenImage, DRAGON, Nano-banana-150k Real sources ImageNet (via GenImage), Nano-banana Fake type Primarily AI-generated (not manipulated)"},{"location":"research/findings/#evaluation-configurations","title":"Evaluation Configurations","text":"<ol> <li>Vision-only: LLM analyzes image directly, no forensic tools</li> <li>Tool-augmented: LLM uses initial vision analysis, then invokes forensic tools via LangGraph ReAct agent, synthesizes findings</li> </ol>"},{"location":"research/findings/#models-evaluated","title":"Models Evaluated","text":"<ul> <li>Google Gemini 3 Flash Preview</li> <li>OpenAI GPT-5.2, GPT-5-mini</li> <li>Anthropic Claude Sonnet 4.5</li> <li>Zhipu AI GLM-4.6, GLM-4.7</li> <li>DeepSeek, Grok, Kimi, MIMO</li> </ul>"},{"location":"research/findings/#results","title":"Results","text":""},{"location":"research/findings/#quantitative-performance","title":"Quantitative Performance","text":""},{"location":"research/findings/#vision-only-mode-n500","title":"Vision-Only Mode (n=500)","text":"Model Accuracy Coverage Acc(answered) Abstention Gemini 3 Flash Preview 0.928 0.984 0.943 1.6% GLM-4.7 0.914 0.980 0.933 2.0% GPT-5.2 0.074 0.078 0.949 92.2%"},{"location":"research/findings/#tool-augmented-mode-n500","title":"Tool-Augmented Mode (n=500)","text":"Model Accuracy Coverage Acc(answered) Abstention Gemini 3 Flash Preview 0.782 0.934 0.837 6.2% GLM-4.7 0.456 0.508 0.898 49.0% GPT-5.2 0.080 0.112 0.714 88.8%"},{"location":"research/findings/#critical-caveats-on-performance-numbers","title":"Critical Caveats on Performance Numbers","text":""},{"location":"research/findings/#caveat-1-training-data-contamination","title":"Caveat 1: Training Data Contamination","text":"<p>The high vision-only accuracy (92.8% for Gemini 3 Flash) must be interpreted cautiously:</p> <ul> <li>GenImage, DRAGON, and Nano-banana are publicly available datasets</li> <li>Modern vision LLMs may have been trained on these or similar synthetic images</li> <li>If a model has \"seen\" these images (or images from the same generators) during training, it may recognize them without possessing generalizable detection capability</li> </ul> <p>Implication: Vision-only accuracy on this dataset does not necessarily indicate robust detection capability on novel generators or unseen image distributions.</p>"},{"location":"research/findings/#caveat-2-tool-task-mismatch","title":"Caveat 2: Tool-Task Mismatch","text":"<p>The forensic tools available to DF3 were designed primarily for manipulation detection (splicing, copy-move, inpainting), not AI-generation detection:</p> Tool Primary Design Purpose Signal for AI-Generated TruFor Manipulation/forgery detection Weak (outputs low scores) ELA JPEG editing detection Weak (no editing occurred) JPEG quantization Double-compression detection Neutral Frequency analysis Upsampling/resampling artifacts Moderate Residual extraction Noise pattern anomalies Moderate <p>AI-generated images are internally consistent\u2014they were never \"edited.\" Tools designed to detect editing artifacts find little signal. This is not a failure of the LLM; it's a mismatch between available tools and the detection task.</p>"},{"location":"research/findings/#caveat-3-zero-shot-tool-interpretation","title":"Caveat 3: Zero-Shot Tool Interpretation","text":"<p>DF2 achieved 99.96% AUROC because it learned how to combine forensic features from 166,000 labeled examples. The logistic regression discovered decision boundaries like:</p> <pre><code>If noiseprint_std &gt; X AND fft_low_energy &gt; Y \u2192 likely fake\n</code></pre> <p>DF3 asks the LLM to discover these patterns from first principles, with only prompt instructions. This is fundamentally harder:</p> <ul> <li>DF2: Supervised learning on 166k examples</li> <li>DF3: Zero-shot reasoning with tool descriptions</li> </ul> <p>The LLM must interpret numeric outputs (e.g., \"TruFor: 0.14\") without having learned what constitutes a meaningful threshold for this specific dataset.</p>"},{"location":"research/findings/#the-core-value-proposition-explainability","title":"The Core Value Proposition: Explainability","text":""},{"location":"research/findings/#why-explainability-matters-for-forensics","title":"Why Explainability Matters for Forensics","text":"<p>In forensic contexts, a probability score is often insufficient. Investigators, attorneys, and courts need to understand:</p> <ol> <li>What evidence supports the conclusion</li> <li>What analysis was performed</li> <li>What limitations apply to the finding</li> <li>What alternative explanations were considered</li> </ol> <p>LLMs provide this naturally through their reasoning output.</p>"},{"location":"research/findings/#example-df2-vs-df3-output-comparison","title":"Example: DF2 vs DF3 Output Comparison","text":"<p>DF2 Output (template-based): <pre><code>Prediction: FAKE (confidence: 0.94)\n\nEvidence:\n- Noiseprint mismatch: 3.21 (threshold: 2.50) - SUPPORTS FAKE\n- Residual energy p95: 0.087 (elevated) - SUPPORTS FAKE\n- FFT peakiness: 1.42 (normal range) - NEUTRAL\n\nUncertainty: LOW (Mahalanobis distance within expected range)\n</code></pre></p> <p>DF3 Output (LLM reasoning): <pre><code>### Visual Analysis\nThis portrait shows a young woman with long dark hair against a neutral\nbackground. Several synthesis indicators are present:\n\n- The hair strands on the left shoulder dissolve into the background\n  rather than possessing physical depth\n- The skin texture is hyper-smooth, lacking natural pores or\n  micro-imperfections typical of photographs\n- The transition between ear and hair shows unnatural blending\n\n### Tool Evidence\n- TruFor manipulation_probability: 0.14 (low)\n- ELA anomaly_score: 1.2 (within normal range)\n\n### Interpretation\nThe low manipulation scores are expected for AI-generated images,\nas they are internally consistent and show no editing artifacts.\nThe visual anomalies\u2014particularly the impossible hair physics and\nunnaturally smooth skin\u2014are characteristic of diffusion model outputs.\n\n### Verdict: FAKE\n### Confidence: 0.85\n\nThe image exhibits multiple visual hallmarks of AI generation. While\nforensic tools show no manipulation (expected for synthetic images),\nthe anatomical and textural anomalies strongly indicate synthesis.\n</code></pre></p> <p>The DF3 output is suitable for: - Forensic reports that will be reviewed by non-technical stakeholders - Legal proceedings where reasoning must be articulated - Training materials for forensic examiners - Cases where the conclusion may be challenged</p>"},{"location":"research/findings/#transparency-of-reasoning","title":"Transparency of Reasoning","text":"<p>Unlike black-box classifiers, LLM reasoning can be:</p> <ol> <li>Audited: Reviewers can check if the reasoning is sound</li> <li>Challenged: Errors in reasoning can be identified and corrected</li> <li>Documented: The analysis becomes part of the case record</li> <li>Educational: Shows what indicators are relevant for detection</li> </ol>"},{"location":"research/findings/#practical-use-cases","title":"Practical Use Cases","text":""},{"location":"research/findings/#use-case-1-high-volume-triage","title":"Use Case 1: High-Volume Triage","text":"<p>Scenario: A forensic agency receives 100,000 images requiring authentication assessment.</p> <p>Workflow: <pre><code>100,000 images\n    \u2193\nDF3 Vision-Only Analysis (fast, ~5s/image)\n    \u2193\n\u251c\u2500\u2500 High confidence REAL (60%) \u2192 Auto-clear\n\u251c\u2500\u2500 High confidence FAKE (25%) \u2192 Auto-flag for review\n\u2514\u2500\u2500 UNCERTAIN (15%) \u2192 Human expert review\n</code></pre></p> <p>Value: - 85% of images handled automatically with reasoning documented - Human experts focus on genuinely ambiguous cases - Each decision includes explanation for audit trail</p>"},{"location":"research/findings/#use-case-2-forensic-report-generation","title":"Use Case 2: Forensic Report Generation","text":"<p>Scenario: An examiner needs to document their analysis of a suspected deepfake.</p> <p>Workflow: <pre><code>Suspicious image\n    \u2193\nDF3 Analysis (tools + vision)\n    \u2193\nGenerate SWGDE-compliant report\n    \u2193\nExpert reviews/validates reasoning\n    \u2193\nFinal report for case file\n</code></pre></p> <p>Value: - Structured analysis following forensic standards - Natural language suitable for reports - Expert retains final judgment authority</p>"},{"location":"research/findings/#use-case-3-educational-and-training","title":"Use Case 3: Educational and Training","text":"<p>Scenario: Training new forensic examiners on AI-generated image detection.</p> <p>Workflow: <pre><code>Training images (known ground truth)\n    \u2193\nDF3 Analysis with detailed reasoning\n    \u2193\nCompare LLM reasoning to expert analysis\n    \u2193\nIdentify what indicators matter\n</code></pre></p> <p>Value: - Demonstrates analytical reasoning process - Shows what visual/forensic cues are relevant - Provides worked examples for learning</p>"},{"location":"research/findings/#use-case-4-preliminary-assessment","title":"Use Case 4: Preliminary Assessment","text":"<p>Scenario: Quick assessment needed before committing to full forensic analysis.</p> <p>Workflow: <pre><code>Image of interest\n    \u2193\nDF3 Vision-Only (2-10 seconds)\n    \u2193\n\u251c\u2500\u2500 Clear indicators \u2192 Preliminary finding documented\n\u2514\u2500\u2500 Ambiguous \u2192 Full forensic workup warranted\n</code></pre></p> <p>Value: - Fast initial assessment - Documented reasoning even for preliminary findings - Helps prioritize resource allocation</p>"},{"location":"research/findings/#insights-for-future-research","title":"Insights for Future Research","text":""},{"location":"research/findings/#insight-1-tools-need-task-specific-design","title":"Insight 1: Tools Need Task-Specific Design","text":"<p>DF2 proved that forensic features can achieve near-perfect detection. But those features were designed for the same distribution they were evaluated on. For LLM agents to benefit from tools:</p> <ul> <li>Develop AI-generation-specific tools: Detectors trained specifically on diffusion/GAN outputs</li> <li>Provide threshold guidance: Include calibrated thresholds in tool descriptions</li> <li>Return confidence intervals: Help LLMs understand measurement uncertainty</li> </ul>"},{"location":"research/findings/#insight-2-few-shot-may-beat-zero-shot","title":"Insight 2: Few-Shot May Beat Zero-Shot","text":"<p>DF3's tool-augmented mode asks LLMs to interpret forensic outputs zero-shot. Consider:</p> <ul> <li>Few-shot examples: Show 3-5 examples of correct tool interpretation in the prompt</li> <li>Calibration data: Provide distribution statistics (\"typical real images score 0.1-0.3\")</li> <li>Decision rules: Extract DF2's learned rules and provide them explicitly</li> </ul>"},{"location":"research/findings/#insight-3-hybrid-architectures","title":"Insight 3: Hybrid Architectures","text":"<p>The optimal system may combine DF2 and DF3 approaches:</p> <pre><code>Image\n    \u2193\nDF2: Extract features \u2192 Trained classifier \u2192 P(fake)\n    \u2193\nDF3: Vision analysis \u2192 Integrate DF2 score as \"tool\" \u2192 Reasoning\n    \u2193\nOutput: Probability + Explanation\n</code></pre> <p>This leverages DF2's discriminative power with DF3's explanatory capability.</p>"},{"location":"research/findings/#insight-4-evaluation-on-novel-distributions","title":"Insight 4: Evaluation on Novel Distributions","text":"<p>Future evaluation should use: - Images from generators not in training data - Temporal holdout (new models released after data collection) - Cross-domain evaluation (different content types)</p> <p>This avoids the contamination concern that affects our vision-only results.</p>"},{"location":"research/findings/#insight-5-the-three-way-classification-has-value","title":"Insight 5: The Three-Way Classification Has Value","text":"<p>The UNCERTAIN verdict is a feature:</p> Traditional Binary DF3 Three-Way Force decision on ambiguous cases Route ambiguous cases to humans Hide uncertainty in low confidence Make uncertainty explicit Same output for \"confident wrong\" and \"uncertain\" Distinguish confident vs uncertain <p>For forensic applications, knowing when the system is uncertain is as valuable as knowing its prediction.</p>"},{"location":"research/findings/#comparison-df2-vs-df3","title":"Comparison: DF2 vs DF3","text":"Dimension DF2 DF3 Accuracy Higher (AUROC 0.9996) Lower (varies by model) Explainability Template-based Free-form reasoning Generalization Limited to training distribution Unknown (contamination concern) Speed Fast (&lt;1s) Slower (5-90s) Human oversight Score requires interpretation Reasoning is human-readable Adaptability Requires retraining Prompt engineering Tool integration Fixed feature set Dynamic tool selection Uncertainty handling Calibrated probability Three-way classification <p>Recommendation: Use DF2 for high-throughput automated screening. Use DF3 when explainability is required or for human-in-the-loop workflows.</p>"},{"location":"research/findings/#limitations-of-this-research","title":"Limitations of This Research","text":""},{"location":"research/findings/#single-dataset","title":"Single Dataset","text":"<p>All evaluation used one dataset (subset and combination of GenImage + DRAGON + Nano-banana + more for df2). Results may not generalize to other distributions.</p>"},{"location":"research/findings/#single-trial","title":"Single Trial","text":"<p>Most configurations were run once or twice. No variance estimates are available.</p>"},{"location":"research/findings/#training-data-contamination-unknown","title":"Training Data Contamination Unknown","text":"<p>We cannot verify whether evaluated models were trained on our test images or similar data, because open-data models are rare.</p>"},{"location":"research/findings/#tool-task-mismatch","title":"Tool-Task Mismatch","text":"<p>Forensic tools optimized for manipulation detection were applied to AI-generation detection.</p>"},{"location":"research/findings/#no-baseline-comparison","title":"No Baseline Comparison","text":"<p>We did not compare against purpose-built AI-generated image detectors (e.g., DIRE, UnivFD).</p>"},{"location":"research/findings/#conclusions","title":"Conclusions","text":"<p>DF3 demonstrates that LLM-based forensic image analysis is technically feasible and provides unique value through explainability. The system is best understood not as a replacement for traditional detectors, but as a complementary capability for workflows requiring:</p> <ol> <li>Auditable reasoning that can be reviewed and challenged</li> <li>Natural language output suitable for reports and legal proceedings</li> <li>Human-in-the-loop triage where uncertain cases are escalated</li> <li>Flexible analysis that can be adapted through prompting</li> </ol> <p>The finding that tools did not improve accuracy should not be interpreted as \"tools are useless\", DF2 proves they provide signal. Rather, it indicates that zero-shot tool interpretation by LLMs is a deeper problem that warrants further research.</p> <p>Future work should focus on: - Developing AI-generation-specific forensic tools - Exploring few-shot tool interpretation - Hybrid architectures combining trained classifiers with LLM reasoning - Rigorous evaluation on temporally held-out, novel generators</p>"},{"location":"research/findings/#artifacts-and-reproducibility","title":"Artifacts and Reproducibility","text":""},{"location":"research/findings/#result-files","title":"Result Files","text":"<ul> <li><code>results/G3visiononly_notools.jsonl</code> \u2014 Best vision-only results</li> <li><code>results/B_or_g3flashprev.jsonl</code> \u2014 Best tool-augmented results</li> <li>Additional per-model results in <code>results/</code></li> </ul>"},{"location":"research/findings/#reproduction-commands","title":"Reproduction Commands","text":"<pre><code># Vision-only\npython scripts/evaluate_llms.py --dataset data2/samples.jsonl \\\n    --models google/gemini-3-flash-preview --tools no --limit 500\n\n# Tool-augmented\npython scripts/evaluate_llms.py --dataset data2/samples.jsonl \\\n    --models google/gemini-3-flash-preview --tools yes --limit 500\n</code></pre>"},{"location":"research/findings/#dataset-provenance","title":"Dataset Provenance","text":"<ul> <li>Digest (n=500): <code>f987165daff0de70</code></li> <li>Sources: GenImage, DRAGON, Nano-banana-150k</li> </ul>"},{"location":"research/findings/#see-also","title":"See Also","text":"<ul> <li>Evaluation Results \u2014 Complete benchmark tables</li> <li>Methodology \u2014 Evaluation framework</li> <li>Design Decisions \u2014 Architectural choices</li> <li>Limitations \u2014 Known system limitations</li> </ul>"},{"location":"research/limitations/","title":"Limitations","text":"<p>This page documents known limitations of the DF3 system based on empirical observations during evaluation.</p>"},{"location":"research/limitations/#detection-limitations","title":"Detection Limitations","text":""},{"location":"research/limitations/#manipulation-vs-synthesis","title":"Manipulation vs. Synthesis","text":"Scenario Detection Capability Reason Post-hoc manipulation (splicing, inpainting) Strong TruFor/ELA designed for this Fully AI-generated images Variable Relies on visual artifact recognition Adversarial examples Weak Not evaluated systematically <p>TruFor's <code>manipulation_probability</code> near 0 indicates absence of editing, not authenticity. AI-generated images can score low because they were never manipulated\u2014they were created whole.</p>"},{"location":"research/limitations/#visual-artifact-dependence","title":"Visual Artifact Dependence","text":"<p>Vision-only mode depends on recognizable generation artifacts:</p> <ul> <li>Anatomical errors (extra fingers, asymmetric features)</li> <li>Texture inconsistencies (overly smooth skin, repeating patterns)</li> <li>Semantic impossibilities (reflections that don't match)</li> <li>Lighting/shadow inconsistencies</li> </ul> <p>Images without obvious artifacts may pass undetected.</p>"},{"location":"research/limitations/#format-specific-tools","title":"Format-Specific Tools","text":"Tool Format Requirement Behavior on Other Formats ELA JPEG only Skipped JPEG Quantization JPEG only Skipped Frequency Analysis Any Results vary by format"},{"location":"research/limitations/#model-behavior","title":"Model Behavior","text":""},{"location":"research/limitations/#high-abstention-rates","title":"High Abstention Rates","text":"<p>Some models exhibit very high abstention (UNCERTAIN) rates:</p> Model Abstention Rate Notes GPT-5.2 88-92% Even with <code>reasoning_effort: high</code> GPT-5-mini 70-92% Depends on tools mode Gemini 3 Flash 1.6-6% Much lower abstention <p>High abstention produces low overall accuracy but can have high accuracy-when-answered.</p>"},{"location":"research/limitations/#non-determinism","title":"Non-determinism","text":"<p>With <code>temperature=0</code>, outputs are near-deterministic but not guaranteed identical across:</p> <ul> <li>Different API calls</li> <li>Different model versions/checkpoints</li> <li>Provider-side updates</li> </ul>"},{"location":"research/limitations/#prompt-sensitivity","title":"Prompt Sensitivity","text":"<p>The agent's behavior depends on prompt wording. The current prompts are tuned for the evaluation workflow. Significant prompt changes may alter performance characteristics.</p>"},{"location":"research/limitations/#tool-limitations","title":"Tool Limitations","text":""},{"location":"research/limitations/#trufor","title":"TruFor","text":"<ul> <li>Trained on manipulation datasets; may not generalize to novel generators</li> <li>Computationally expensive (~2-5s per image on GPU)</li> <li>Weights auto-download on first use (~500MB)</li> </ul>"},{"location":"research/limitations/#ela","title":"ELA","text":"<ul> <li>Only meaningful for JPEG images</li> <li>Heavy recompression can produce false positives</li> <li>Legitimate watermarks/overlays appear as anomalies</li> </ul>"},{"location":"research/limitations/#metadata","title":"Metadata","text":"<ul> <li>Easily stripped or forged</li> <li>Absence of metadata is not evidence of manipulation</li> <li>C2PA adoption is limited; most images lack it</li> </ul>"},{"location":"research/limitations/#residual-extraction","title":"Residual Extraction","text":"<ul> <li>DRUNet inference adds latency</li> <li>Statistical properties overlap between real and synthetic classes</li> <li>No hard thresholds exist</li> </ul>"},{"location":"research/limitations/#evaluation-limitations","title":"Evaluation Limitations","text":""},{"location":"research/limitations/#single-dataset","title":"Single Dataset","text":"<p>All results are from one evaluation dataset with different sample limits (n=200 or n=500). Results may not generalize to:</p> <ul> <li>Different image sources</li> <li>Different generator versions</li> <li>Different content domains</li> </ul>"},{"location":"research/limitations/#single-trial","title":"Single Trial","text":"<p>Most runs are single-trial. No variance estimates are available for metrics.</p>"},{"location":"research/limitations/#confounded-tool-comparisons","title":"Confounded Tool Comparisons","text":"<p>Tool usage correlates with image difficulty. \"Tool used\" vs. \"not used\" accuracy deltas are descriptive, not causal.</p>"},{"location":"research/limitations/#cache-effects","title":"Cache Effects","text":"<p>Some latency measurements are confounded by cache state. Vision cache and tool cache can reduce latency to near-zero.</p>"},{"location":"research/limitations/#confidence-score","title":"Confidence Score","text":"<p>The <code>confidence</code> field is the LLM's self-reported certainty.</p> <p>Properties:</p> <ul> <li>Range: 0.0 to 1.0</li> <li>Not calibrated to empirical accuracy</li> <li>May exhibit overconfidence</li> <li>Varies by model</li> </ul> <p>Use for:</p> <ul> <li>Triage ranking (higher \u2192 more certain)</li> <li>Routing low-confidence to human review</li> </ul> <p>Do not use for:</p> <ul> <li>Probability statements</li> <li>Likelihood ratios</li> </ul>"},{"location":"research/limitations/#operational-constraints","title":"Operational Constraints","text":""},{"location":"research/limitations/#latency","title":"Latency","text":"Mode Typical Range Vision-only 2-10s Tools mode 30-90s Cached &lt;1s"},{"location":"research/limitations/#api-dependencies","title":"API Dependencies","text":"<ul> <li>Cloud LLM analysis requires internet and valid API keys</li> <li>Rate limits apply per provider</li> <li>Provider outages affect availability</li> </ul>"},{"location":"research/limitations/#gpu-recommendation","title":"GPU Recommendation","text":"<p>TruFor and DRUNet run on CPU but are significantly faster on CUDA-capable GPU.</p>"},{"location":"research/limitations/#see-also","title":"See Also","text":"<ul> <li>Benchmark Results \u2014 Empirical performance data</li> <li>Methodology \u2014 Evaluation setup</li> </ul>"},{"location":"research/reproducibility/","title":"Reproducibility Guide","text":"<p>How to ensure your DF3 experiments can be reproduced.</p>"},{"location":"research/reproducibility/#why-reproducibility-matters","title":"Why Reproducibility Matters","text":"<p>Forensic conclusions must be:</p> <ul> <li>Verifiable \u2014 Others can check your work</li> <li>Consistent \u2014 Same inputs produce same outputs</li> <li>Documented \u2014 Process is fully recorded</li> </ul>"},{"location":"research/reproducibility/#sources-of-non-reproducibility","title":"Sources of Non-Reproducibility","text":""},{"location":"research/reproducibility/#llm-variability","title":"LLM Variability","text":"Factor Impact Mitigation Temperature &gt; 0 Random sampling Use <code>temperature: 0.0</code> Model updates Behavior changes Record model version Prompt changes Different reasoning Hash prompts API variability Non-deterministic Multiple trials"},{"location":"research/reproducibility/#data-variability","title":"Data Variability","text":"Factor Impact Mitigation Dataset changes Different samples Record dataset digest Sample ordering Processing order effects Use seed for shuffling Missing samples Incomplete comparison Verify sample counts"},{"location":"research/reproducibility/#environment-variability","title":"Environment Variability","text":"Factor Impact Mitigation Package versions Behavior differences Pin versions Hardware differences Floating point variance Document hardware Cache state Different code paths Document cache usage"},{"location":"research/reproducibility/#reproducibility-checklist","title":"Reproducibility Checklist","text":""},{"location":"research/reproducibility/#1-record-configuration","title":"1. Record Configuration","text":"<pre><code>{\n    \"model\": \"gpt-5.1\",\n    \"vision_model\": \"gpt-5.1\",\n    \"structuring_model\": \"gpt-5.1\",\n    \"temperature\": 0.0,\n    \"max_iterations\": 15,\n    \"cache_enabled\": true,\n    \"timestamp\": \"2026-01-15T10:30:00Z\"\n}\n</code></pre>"},{"location":"research/reproducibility/#2-record-dataset","title":"2. Record Dataset","text":"<pre><code>{\n    \"dataset_path\": \"data/benchmark.jsonl\",\n    \"sample_count\": 500,\n    \"real_count\": 250,\n    \"fake_count\": 250,\n    \"id_digest\": \"f987165daff0de70\",\n    \"shuffle_seed\": 42\n}\n</code></pre>"},{"location":"research/reproducibility/#3-record-environment","title":"3. Record Environment","text":"<pre><code># Generate requirements snapshot\npip freeze &gt; requirements-frozen.txt\n\n# Record Python version\npython --version &gt; environment.txt\n</code></pre>"},{"location":"research/reproducibility/#4-record-results","title":"4. Record Results","text":"<pre><code>{\n    \"run_id\": \"eval-2026-01-15-001\",\n    \"git_commit\": \"abc123...\",\n    \"prompt_hash\": \"def456...\",\n    \"results_file\": \"results/eval.jsonl\",\n    \"metrics_file\": \"results/metrics.json\"\n}\n</code></pre>"},{"location":"research/reproducibility/#recommended-workflow","title":"Recommended Workflow","text":""},{"location":"research/reproducibility/#before-running","title":"Before Running","text":"<pre><code># 1. Commit or stash changes\ngit status\n\n# 2. Record commit hash\ngit rev-parse HEAD\n\n# 3. Verify dataset\npython scripts/verify_dataset.py --dataset data/benchmark.jsonl\n\n# 4. Clear cache (optional, for latency measurement)\nRemove-Item -Recurse -Force .tool_cache\n</code></pre>"},{"location":"research/reproducibility/#during-run","title":"During Run","text":"<pre><code># Run with full configuration\npython scripts/evaluate_llms.py `\n    --dataset data/benchmark.jsonl `\n    --models gpt-5.1 `\n    --tools both `\n    --temperature 0.0 `\n    --trials 3 `\n    --seed 42 `\n    --output results/eval_$(Get-Date -Format \"yyyyMMdd\").jsonl `\n    --metrics-output results/metrics_$(Get-Date -Format \"yyyyMMdd\").json\n</code></pre>"},{"location":"research/reproducibility/#after-running","title":"After Running","text":"<pre><code># 1. Archive results with metadata\npython scripts/archive_results.py --run-dir results/\n\n# 2. Verify metrics\npython scripts/summarize_results.py --results-dir results/\n\n# 3. Commit results (if using git for tracking)\ngit add results/\ngit commit -m \"Evaluation run 2026-01-15\"\n</code></pre>"},{"location":"research/reproducibility/#multi-trial-evaluation","title":"Multi-Trial Evaluation","text":"<p>For robust results with variance estimates:</p> <pre><code>python scripts/evaluate_llms.py `\n    --dataset data/benchmark.jsonl `\n    --models gpt-5.1 `\n    --trials 5 `\n    --temperature 0.0 `\n    --seed 42\n</code></pre> <p>This produces:</p> <ul> <li>Individual results for each trial</li> <li>Summary statistics (mean, std) across trials</li> <li>Per-trial metrics breakdown</li> </ul>"},{"location":"research/reproducibility/#comparing-configurations","title":"Comparing Configurations","text":""},{"location":"research/reproducibility/#valid-comparison","title":"Valid Comparison","text":"<pre><code># Same dataset, same seed, different models\npython scripts/evaluate_llms.py `\n    --dataset data/benchmark.jsonl `\n    --models gpt-5.1,gpt-5-mini `\n    --seed 42 `\n    --temperature 0.0\n</code></pre>"},{"location":"research/reproducibility/#invalid-comparison","title":"Invalid Comparison","text":"<pre><code># Different datasets - NOT comparable\npython scripts/evaluate_llms.py --dataset datasetA.jsonl --models gpt-5.1\npython scripts/evaluate_llms.py --dataset datasetB.jsonl --models gpt-5-mini\n</code></pre>"},{"location":"research/reproducibility/#paired-analysis","title":"Paired Analysis","text":"<p>Use <code>summarize_results.py</code> for statistical tests:</p> <pre><code>python scripts/summarize_results.py `\n    --results-dir results/ `\n    --paired-comparison \"gpt-5.1 vs gpt-5-mini\"\n</code></pre>"},{"location":"research/reproducibility/#dataset-digest","title":"Dataset Digest","text":"<p>Compute dataset identifier for comparison:</p> <pre><code>import hashlib\nimport json\n\nwith open(\"data/benchmark.jsonl\") as f:\n    samples = [json.loads(line) for line in f]\n\nids = sorted([s[\"id\"] for s in samples])\ndigest = hashlib.sha256(str(ids).encode()).hexdigest()[:16]\nprint(f\"Dataset digest: {digest}\")\n</code></pre> <p>Compare digests to ensure same dataset:</p> Run Dataset Digest Comparable? A f987165daff0de70 \u2014 B f987165daff0de70 \u2705 Yes C 1f78e35118013ed4 \u274c No"},{"location":"research/reproducibility/#prompt-versioning","title":"Prompt Versioning","text":""},{"location":"research/reproducibility/#hash-prompts","title":"Hash Prompts","text":"<pre><code>import hashlib\nfrom src.agents.prompts import SYSTEM_PROMPT, USER_PROMPT_TEMPLATE\n\nprompt_content = SYSTEM_PROMPT + USER_PROMPT_TEMPLATE\nprompt_hash = hashlib.sha256(prompt_content.encode()).hexdigest()[:16]\n</code></pre>"},{"location":"research/reproducibility/#record-in-results","title":"Record in Results","text":"<pre><code>{\n    \"prompt_version\": {\n        \"system_prompt_hash\": \"abc123...\",\n        \"user_prompt_hash\": \"def456...\",\n        \"baml_version\": \"v1.2.3\"\n    }\n}\n</code></pre>"},{"location":"research/reproducibility/#known-non-determinism","title":"Known Non-Determinism","text":"<p>Even with best practices, some variability exists:</p> Source Typical Impact Notes LLM API ~1-2% accuracy variance Temperature 0 helps Floating point Minimal Hardware dependent Tool execution order Minimal Usually consistent Network latency Latency only Not accuracy"},{"location":"research/reproducibility/#accounting-for-variability","title":"Accounting for Variability","text":"<ul> <li>Run multiple trials</li> <li>Report confidence intervals</li> <li>Use Wilson score for proportions</li> <li>Acknowledge in limitations</li> </ul>"},{"location":"research/reproducibility/#artifact-storage","title":"Artifact Storage","text":""},{"location":"research/reproducibility/#directory-structure","title":"Directory Structure","text":"<pre><code>results/\n\u251c\u2500\u2500 eval_20260115/\n\u2502   \u251c\u2500\u2500 raw_results.jsonl\n\u2502   \u251c\u2500\u2500 metrics.json\n\u2502   \u251c\u2500\u2500 config.json\n\u2502   \u251c\u2500\u2500 environment.txt\n\u2502   \u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"research/reproducibility/#metadata-file","title":"Metadata File","text":"<pre><code>{\n    \"run_id\": \"eval_20260115\",\n    \"timestamp\": \"2026-01-15T10:30:00Z\",\n    \"git_commit\": \"abc123def456\",\n    \"dataset_digest\": \"f987165daff0de70\",\n    \"models\": [\"gpt-5.1\"],\n    \"notes\": \"Baseline evaluation with tools enabled\"\n}\n</code></pre>"},{"location":"research/reproducibility/#external-reproduction","title":"External Reproduction","text":"<p>For others to reproduce your results:</p>"},{"location":"research/reproducibility/#provide","title":"Provide","text":"<ol> <li>Code version \u2014 Git commit or release tag</li> <li>Dataset manifest \u2014 IDs, sources, hashes (if data can't be shared)</li> <li>Configuration \u2014 All parameters used</li> <li>Environment \u2014 Python version, package versions</li> <li>Results \u2014 Raw outputs for verification</li> </ol>"},{"location":"research/reproducibility/#document","title":"Document","text":"<ol> <li>Steps to run \u2014 Exact commands</li> <li>Expected outputs \u2014 Sample results</li> <li>Known issues \u2014 Any caveats</li> </ol>"},{"location":"research/reproducibility/#see-also","title":"See Also","text":"<ul> <li>Methodology \u2014 Evaluation framework</li> <li>Dataset Provenance \u2014 Dataset documentation</li> <li>Limitations \u2014 Known limitations</li> </ul>"},{"location":"tools/code-execution/","title":"Code Execution Tool","text":"<p>Dynamic Python code execution for custom forensic analysis.</p>"},{"location":"tools/code-execution/#overview","title":"Overview","text":"<p>The <code>execute_python_code</code> tool enables the agent to write and execute arbitrary Python code for specialized analysis not covered by the standard tools.</p>"},{"location":"tools/code-execution/#purpose","title":"Purpose","text":""},{"location":"tools/code-execution/#use-cases","title":"Use Cases","text":"<ul> <li>Custom statistical analysis</li> <li>Image manipulation operations</li> <li>Specialized forensic techniques</li> <li>Exploratory data analysis</li> <li>Verification of hypotheses</li> </ul>"},{"location":"tools/code-execution/#when-used","title":"When Used","text":"<p>The agent may invoke code execution when:</p> <ul> <li>Standard tools don't address a specific question</li> <li>Custom computation is needed</li> <li>Complex analysis requires programming</li> <li>Results need verification</li> </ul>"},{"location":"tools/code-execution/#capabilities","title":"Capabilities","text":""},{"location":"tools/code-execution/#available-libraries","title":"Available Libraries","text":"<p>The code execution environment includes:</p> Library Purpose <code>numpy</code> Numerical computation <code>PIL/Pillow</code> Image processing <code>scipy</code> Scientific computing <code>cv2</code> (OpenCV) Computer vision <code>json</code> Data serialization <code>math</code> Mathematical functions"},{"location":"tools/code-execution/#inputoutput","title":"Input/Output","text":"<p>Input:</p> <ul> <li>Python code as string</li> <li>Image path available to code</li> </ul> <p>Output:</p> <pre><code>{\n    \"tool\": \"execute_python_code\",\n    \"status\": \"completed\",\n    \"stdout\": \"Analysis result: 0.85\",\n    \"stderr\": \"\",\n    \"return_value\": {\"score\": 0.85}\n}\n</code></pre>"},{"location":"tools/code-execution/#example-usage","title":"Example Usage","text":""},{"location":"tools/code-execution/#custom-pixel-analysis","title":"Custom Pixel Analysis","text":"<pre><code># Agent-generated code example\nfrom PIL import Image\nimport numpy as np\n\nimg = Image.open(image_path)\narr = np.array(img)\n\n# Check for unusual pixel value distributions\nhist, _ = np.histogram(arr.flatten(), bins=256)\nentropy = -np.sum(hist[hist&gt;0] / hist.sum() * np.log2(hist[hist&gt;0] / hist.sum()))\n\nprint(f\"Pixel entropy: {entropy:.2f}\")\n</code></pre>"},{"location":"tools/code-execution/#edge-detection-analysis","title":"Edge Detection Analysis","text":"<pre><code>import cv2\nimport numpy as np\n\nimg = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\nedges = cv2.Canny(img, 100, 200)\n\nedge_density = edges.sum() / edges.size\nprint(f\"Edge density: {edge_density:.4f}\")\n</code></pre>"},{"location":"tools/code-execution/#color-distribution-check","title":"Color Distribution Check","text":"<pre><code>from PIL import Image\nimport numpy as np\n\nimg = Image.open(image_path).convert('RGB')\narr = np.array(img)\n\n# Check for unusual color clustering\nr_std = arr[:,:,0].std()\ng_std = arr[:,:,1].std()\nb_std = arr[:,:,2].std()\n\nprint(f\"R std: {r_std:.2f}, G std: {g_std:.2f}, B std: {b_std:.2f}\")\n</code></pre>"},{"location":"tools/code-execution/#security-considerations","title":"Security Considerations","text":""},{"location":"tools/code-execution/#sandboxing","title":"Sandboxing","text":"<p>Execution Environment</p> <p>Code executes with access to the local filesystem and system resources. In production deployments, consider containerization or sandboxing.</p>"},{"location":"tools/code-execution/#current-protections","title":"Current Protections","text":"<ul> <li>Timeout limits prevent infinite loops</li> <li>Output size limits prevent memory exhaustion</li> <li>Standard library restrictions (configurable)</li> </ul>"},{"location":"tools/code-execution/#risks","title":"Risks","text":"Risk Mitigation File system access Restrict to data directory Network access Consider blocking Resource exhaustion Timeout and memory limits Code injection Agent generates code, not user"},{"location":"tools/code-execution/#forensic-applications","title":"Forensic Applications","text":""},{"location":"tools/code-execution/#verifying-tool-results","title":"Verifying Tool Results","text":"<pre><code># Manual verification of ELA-like analysis\nfrom PIL import Image\nimport numpy as np\n\noriginal = Image.open(image_path)\noriginal.save('/tmp/recompressed.jpg', 'JPEG', quality=75)\nrecompressed = Image.open('/tmp/recompressed.jpg')\n\ndiff = np.abs(np.array(original).astype(float) - np.array(recompressed).astype(float))\nprint(f\"Mean difference: {diff.mean():.2f}\")\nprint(f\"Max difference: {diff.max():.2f}\")\n</code></pre>"},{"location":"tools/code-execution/#custom-feature-extraction","title":"Custom Feature Extraction","text":"<pre><code>import cv2\nimport numpy as np\n\nimg = cv2.imread(image_path)\n\n# Detect and count faces\nface_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\nfaces = face_cascade.detectMultiScale(gray, 1.1, 4)\n\nprint(f\"Detected {len(faces)} faces\")\n</code></pre>"},{"location":"tools/code-execution/#statistical-tests","title":"Statistical Tests","text":"<pre><code>from scipy import stats\nimport numpy as np\nfrom PIL import Image\n\nimg = np.array(Image.open(image_path).convert('L'))\n\n# Test for uniform noise (Gaussian test)\n_, p_value = stats.normaltest(img.flatten())\nprint(f\"Normality test p-value: {p_value:.4f}\")\n</code></pre>"},{"location":"tools/code-execution/#agent-behavior","title":"Agent Behavior","text":""},{"location":"tools/code-execution/#when-agent-uses-code-execution","title":"When Agent Uses Code Execution","text":"<ol> <li>Gap in tools \u2014 No standard tool addresses the question</li> <li>Verification \u2014 Double-checking other tool results</li> <li>Exploration \u2014 Investigating specific hypothesis</li> <li>Custom metrics \u2014 Computing domain-specific measures</li> </ol>"},{"location":"tools/code-execution/#agent-code-quality","title":"Agent Code Quality","text":"<p>The agent generates code that:</p> <ul> <li>Uses available libraries</li> <li>Handles errors gracefully (usually)</li> <li>Outputs interpretable results</li> <li>Avoids infinite loops (usually)</li> </ul>"},{"location":"tools/code-execution/#limitations","title":"Limitations","text":"<p>Agent-generated code may:</p> <ul> <li>Have bugs or errors</li> <li>Not be optimal</li> <li>Miss edge cases</li> <li>Produce hard-to-interpret output</li> </ul>"},{"location":"tools/code-execution/#output-interpretation","title":"Output Interpretation","text":""},{"location":"tools/code-execution/#successful-execution","title":"Successful Execution","text":"<pre><code>{\n    \"status\": \"completed\",\n    \"stdout\": \"Result: 0.85\",\n    \"stderr\": \"\",\n    \"execution_time\": 0.23\n}\n</code></pre>"},{"location":"tools/code-execution/#failed-execution","title":"Failed Execution","text":"<pre><code>{\n    \"status\": \"error\",\n    \"stdout\": \"\",\n    \"stderr\": \"NameError: name 'undefined_var' is not defined\",\n    \"execution_time\": 0.01\n}\n</code></pre>"},{"location":"tools/code-execution/#timeout","title":"Timeout","text":"<pre><code>{\n    \"status\": \"timeout\",\n    \"stdout\": \"Partial output...\",\n    \"stderr\": \"Execution exceeded time limit\",\n    \"execution_time\": 30.0\n}\n</code></pre>"},{"location":"tools/code-execution/#best-practices","title":"Best Practices","text":""},{"location":"tools/code-execution/#for-analysis","title":"For Analysis","text":"<ul> <li>Treat code execution results as supplementary</li> <li>Verify unusual findings manually</li> <li>Consider execution errors as inconclusive</li> </ul>"},{"location":"tools/code-execution/#for-security","title":"For Security","text":"<ul> <li>Review execution logs periodically</li> <li>Consider sandboxing in production</li> <li>Limit available libraries if needed</li> <li>Monitor resource usage</li> </ul>"},{"location":"tools/code-execution/#for-reproducibility","title":"For Reproducibility","text":"<ul> <li>Log all executed code</li> <li>Save intermediate results</li> <li>Document any custom analysis</li> </ul>"},{"location":"tools/code-execution/#configuration","title":"Configuration","text":""},{"location":"tools/code-execution/#timeout_1","title":"Timeout","text":"<p>Default: 30 seconds</p>"},{"location":"tools/code-execution/#memory-limit","title":"Memory Limit","text":"<p>Inherits from Python process</p>"},{"location":"tools/code-execution/#available-libraries_1","title":"Available Libraries","text":"<p>Configured in tool implementation</p>"},{"location":"tools/code-execution/#see-also","title":"See Also","text":"<ul> <li>Tools Overview \u2014 All forensic tools</li> <li>Agent Pipeline \u2014 How agent uses tools</li> <li>Troubleshooting \u2014 Error handling</li> </ul>"},{"location":"tools/ela/","title":"Error Level Analysis (ELA)","text":"<p>Error Level Analysis is a classic forensic technique that detects localized editing by comparing compression artifacts across image regions.</p>"},{"location":"tools/ela/#overview","title":"Overview","text":"<p>ELA (Error Level Analysis) works by recompressing an image at a fixed JPEG quality and measuring the pixel-wise difference. Regions that were edited or pasted from different sources often compress differently, producing higher error levels.</p> Aspect Detail Purpose Detect localized JPEG editing/manipulation Technique Recompression difference analysis Input JPEG only (skips other formats) Output Error statistics and optional visualization"},{"location":"tools/ela/#how-it-works","title":"How It Works","text":"<pre><code>flowchart LR\n    subgraph Input\n        ORIG[Original JPEG]\n    end\n\n    subgraph Process[\"ELA Process\"]\n        RECOMP[Recompress&lt;br/&gt;at fixed quality]\n        DIFF[Compute&lt;br/&gt;pixel difference]\n        SCALE[Scale and&lt;br/&gt;enhance]\n    end\n\n    subgraph Analysis\n        STATS[Statistics&lt;br/&gt;mean, std, anomaly]\n        MAP[Error Map&lt;br/&gt;visualization]\n    end\n\n    ORIG --&gt; RECOMP\n    ORIG --&gt; DIFF\n    RECOMP --&gt; DIFF\n    DIFF --&gt; SCALE\n    SCALE --&gt; STATS\n    SCALE --&gt; MAP</code></pre>"},{"location":"tools/ela/#the-principle","title":"The Principle","text":"<p>When a JPEG image is saved, each 8x8 block is compressed based on its content. If regions were:</p> <ul> <li>Saved at different quality levels \u2014 They'll compress differently</li> <li>Pasted from other images \u2014 Their compression history differs</li> <li>Edited after saving \u2014 The edited areas haven't been compressed as much</li> </ul>"},{"location":"tools/ela/#visual-example","title":"Visual Example","text":"<pre><code>Original image regions:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Photo at quality 85         \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510             \u2502\n\u2502   \u2502 Pasted    \u2502 \u2190 Saved at  \u2502\n\u2502   \u2502 element   \u2502   quality 95\u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2502\n\u2502                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nELA output:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Low error (dark)            \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510             \u2502\n\u2502   \u2502 HIGH      \u2502 \u2190 Anomaly!  \u2502\n\u2502   \u2502 ERROR     \u2502             \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2502\n\u2502                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"tools/ela/#usage","title":"Usage","text":""},{"location":"tools/ela/#tool-call-format","title":"Tool Call Format","text":"<pre><code>{\n    \"path\": \"/path/to/image.jpg\",\n    \"quality\": 75\n}\n</code></pre>"},{"location":"tools/ela/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>path</code> string required Path to JPEG image <code>quality</code> int 75 Recompression quality (1-100)"},{"location":"tools/ela/#output-format","title":"Output Format","text":"<pre><code>{\n    \"tool\": \"perform_ela\",\n    \"status\": \"completed\",\n    \"image_path\": \"/path/to/image.jpg\",\n    \"quality\": 75,\n    \"ela_mean\": 12.5,\n    \"ela_std\": 8.3,\n    \"ela_anomaly_score\": 2.1,\n    \"ela_map\": null,\n    \"ela_map_size\": null,\n    \"note\": \"ELA recompresses at fixed JPEG quality...\"\n}\n</code></pre>"},{"location":"tools/ela/#interpreting-results","title":"Interpreting Results","text":""},{"location":"tools/ela/#anomaly-score","title":"Anomaly Score","text":"<p>The <code>ela_anomaly_score</code> is a z-score of the 95<sup>th</sup> percentile vs mean:</p> Score Range Interpretation 0.0 - 1.5 Normal compression consistency 1.5 - 2.5 Slight anomalies \u2014 may be natural variation 2.5 - 4.0 Moderate anomalies \u2014 investigate further 4.0+ Strong anomalies \u2014 likely localized editing"},{"location":"tools/ela/#mean-and-standard-deviation","title":"Mean and Standard Deviation","text":"<ul> <li>ela_mean \u2014 Average error level across the image</li> <li>ela_std \u2014 Variation in error levels</li> </ul> <p>Higher values generally indicate more processing, but interpretation depends on:</p> <ul> <li>Original image quality</li> <li>Content complexity</li> <li>Compression history</li> </ul>"},{"location":"tools/ela/#jpeg-only-restriction","title":"JPEG-Only Restriction","text":"<p>JPEG Only</p> <p>ELA is fundamentally a JPEG recompression technique. Running it on PNG, WebP, or other formats produces misleading results.</p> <p>DF3 automatically skips ELA for non-JPEG inputs:</p> <pre><code>{\n    \"tool\": \"perform_ela\",\n    \"status\": \"skipped\",\n    \"reason\": \"ELA is JPEG-specific; skipping for .png input.\"\n}\n</code></pre>"},{"location":"tools/ela/#limitations","title":"Limitations","text":""},{"location":"tools/ela/#false-positives","title":"False Positives","text":"<p>ELA can produce high anomaly scores from:</p> <ol> <li>Watermarks and overlays \u2014 Always show high error</li> <li>Text and graphics \u2014 Sharp edges compress differently</li> <li>Heavy recompression \u2014 Multiple saves accumulate artifacts</li> <li>Platform processing \u2014 Social media processing confounds analysis</li> </ol>"},{"location":"tools/ela/#false-negatives","title":"False Negatives","text":"<p>ELA may miss manipulation when:</p> <ol> <li>Same quality \u2014 Edited region saved at same quality as original</li> <li>Careful blending \u2014 Skilled editors minimize compression differences</li> <li>AI-generated content \u2014 Internally consistent compression</li> </ol>"},{"location":"tools/ela/#compression-confounds","title":"Compression Confounds","text":"<ul> <li>Multi-generation \u2014 Images saved repeatedly show uniform high error</li> <li>Quality changes \u2014 Resaving at different quality affects entire image</li> <li>Format conversion \u2014 JPEG\u2192PNG\u2192JPEG destroys meaningful signals</li> </ul>"},{"location":"tools/ela/#best-practices","title":"Best Practices","text":""},{"location":"tools/ela/#for-analysis","title":"For Analysis","text":"<ol> <li>Use on first-generation JPEGs when possible</li> <li>Compare to similar regions \u2014 Backgrounds should be uniform</li> <li>Look for localized anomalies \u2014 Not overall high scores</li> <li>Combine with TruFor \u2014 Corroborating evidence is stronger</li> </ol>"},{"location":"tools/ela/#quality-parameter-selection","title":"Quality Parameter Selection","text":"Original Quality Recommended ELA Quality Unknown 75 (default) High (90+) 85-90 Medium (70-90) 70-80 Low (&lt;70) 60-70 <p>Generally, use a quality slightly below the estimated original quality.</p>"},{"location":"tools/ela/#technical-details","title":"Technical Details","text":""},{"location":"tools/ela/#algorithm","title":"Algorithm","text":"<pre><code>def perform_ela(image_path, quality=75):\n    # Load original\n    original = load_image(image_path)\n\n    # Recompress at fixed quality\n    recompressed = save_as_jpeg(original, quality)\n    recompressed = load_image(recompressed)\n\n    # Compute difference\n    difference = abs(original - recompressed)\n\n    # Scale for visibility\n    ela_map = sqrt(difference / 255) * scale_factor\n\n    # Compute statistics\n    mean = average(ela_map)\n    std = standard_deviation(ela_map)\n    p95 = percentile_95(ela_map)\n    anomaly_score = (p95 - mean) / (std + epsilon)\n\n    return ela_map, mean, std, anomaly_score\n</code></pre>"},{"location":"tools/ela/#implementation-notes","title":"Implementation Notes","text":"<ul> <li>Based on Sherloq ELA implementation</li> <li>Uses non-linear scaling (sqrt) for better visualization</li> <li>Applies contrast adjustment via LUT</li> <li>Outputs color (RGB) map by default</li> </ul>"},{"location":"tools/ela/#enabling-the-map-output","title":"Enabling the Map Output","text":"<p>The error map is disabled by default. Enable it programmatically:</p> <pre><code>from src.tools.forensic import perform_ela\nimport json\n\nresult = perform_ela(json.dumps({\n    \"path\": \"image.jpg\",\n    \"quality\": 75,\n    \"return_map\": True,\n    \"max_size\": 1024  # Limit map dimensions\n}))\n\noutput = json.loads(result)\n# output[\"ela_map\"] contains base64 PNG\n</code></pre>"},{"location":"tools/ela/#see-also","title":"See Also","text":"<ul> <li>TruFor \u2014 Neural forgery detection</li> <li>JPEG Analysis \u2014 Compression artifact analysis</li> <li>Tools Overview \u2014 Complete tool reference</li> </ul>"},{"location":"tools/frequency/","title":"Frequency Analysis","text":"<p>Analyzes image frequency domain features using DCT and FFT transforms to detect synthetic regularities and processing artifacts.</p>"},{"location":"tools/frequency/#overview","title":"Overview","text":"<p>Frequency analysis examines an image's representation in the frequency domain, where:</p> <ul> <li>Low frequencies \u2014 Represent smooth gradients and large features</li> <li>High frequencies \u2014 Represent edges, texture, and fine detail</li> </ul> <p>AI-generated images and processed images often exhibit unusual frequency characteristics.</p> Aspect Detail Purpose Detect synthetic regularities and processing artifacts Technique DCT (Discrete Cosine Transform) and FFT (Fast Fourier Transform) Input Any common image format Output Frequency domain statistics"},{"location":"tools/frequency/#how-it-works","title":"How It Works","text":"<pre><code>flowchart LR\n    subgraph Input\n        IMG[Image]\n    end\n\n    subgraph Transform[\"Frequency Transform\"]\n        DCT[DCT Analysis&lt;br/&gt;8x8 blocks]\n        FFT[FFT Analysis&lt;br/&gt;Global spectrum]\n    end\n\n    subgraph Analysis\n        STATS[Statistical&lt;br/&gt;features]\n        PEAKS[Peak&lt;br/&gt;detection]\n        PATTERNS[Pattern&lt;br/&gt;analysis]\n    end\n\n    IMG --&gt; DCT\n    IMG --&gt; FFT\n    DCT --&gt; STATS\n    DCT --&gt; PATTERNS\n    FFT --&gt; PEAKS\n    FFT --&gt; PATTERNS</code></pre>"},{"location":"tools/frequency/#what-each-transform-reveals","title":"What Each Transform Reveals","text":"<p>DCT (Discrete Cosine Transform):</p> <ul> <li>Block-based analysis matching JPEG compression</li> <li>Reveals quantization patterns</li> <li>Useful for detecting JPEG artifacts</li> </ul> <p>FFT (Fast Fourier Transform):</p> <ul> <li>Global frequency spectrum</li> <li>Reveals periodic patterns across entire image</li> <li>Can detect AI-generated regularities</li> </ul>"},{"location":"tools/frequency/#usage","title":"Usage","text":""},{"location":"tools/frequency/#tool-call-format","title":"Tool Call Format","text":"<pre><code>/path/to/image.jpg\n</code></pre>"},{"location":"tools/frequency/#output-format","title":"Output Format","text":"<pre><code>{\n    \"tool\": \"analyze_frequency_domain\",\n    \"status\": \"completed\",\n    \"image_path\": \"/path/to/image.jpg\",\n    \"dct_features\": {\n        \"mean_energy\": 12500.3,\n        \"energy_distribution\": [0.45, 0.25, 0.15, 0.08, 0.07],\n        \"blocking_score\": 0.23\n    },\n    \"fft_features\": {\n        \"spectral_centroid\": 45.2,\n        \"spectral_flatness\": 0.78,\n        \"peak_frequencies\": [...]\n    },\n    \"note\": \"Frequency domain analysis for anomaly detection...\"\n}\n</code></pre>"},{"location":"tools/frequency/#interpreting-results","title":"Interpreting Results","text":""},{"location":"tools/frequency/#dct-features","title":"DCT Features","text":"Feature Meaning <code>mean_energy</code> Overall frequency content <code>energy_distribution</code> How energy is distributed across frequency bands <code>blocking_score</code> Strength of 8x8 block artifacts (JPEG signature)"},{"location":"tools/frequency/#fft-features","title":"FFT Features","text":"Feature Meaning <code>spectral_centroid</code> \"Center of mass\" of frequency content <code>spectral_flatness</code> How evenly distributed frequencies are <code>peak_frequencies</code> Dominant periodic patterns"},{"location":"tools/frequency/#what-to-look-for","title":"What to Look For","text":"<p>Signs of AI generation:</p> <ul> <li>Unusual frequency peaks not matching natural images</li> <li>Periodic patterns in the spectrum</li> <li>Abnormal energy distribution</li> </ul> <p>Signs of processing:</p> <ul> <li>Strong JPEG blocking in DCT</li> <li>Upscaling artifacts in FFT (periodic peaks)</li> <li>Smoothing/sharpening signatures</li> </ul>"},{"location":"tools/frequency/#limitations","title":"Limitations","text":""},{"location":"tools/frequency/#content-dependence","title":"Content Dependence","text":"<p>Frequency characteristics vary greatly with content:</p> <ul> <li>Natural scenes \u2014 Rich, distributed spectrum</li> <li>Graphics/text \u2014 Sparse, sharp peaks</li> <li>Portraits \u2014 Moderate frequency content</li> </ul> <p>Comparison Required</p> <p>Interpret frequency results by comparing to similar content types, not absolute thresholds.</p>"},{"location":"tools/frequency/#not-definitive","title":"Not Definitive","text":"<p>Frequency analysis alone is rarely conclusive:</p> <ul> <li>Many AI generators produce natural-looking spectra</li> <li>Post-processing can mask anomalies</li> <li>Natural images can have unusual spectra</li> </ul> <p>Use as corroborating evidence with other tools.</p>"},{"location":"tools/frequency/#technical-background","title":"Technical Background","text":""},{"location":"tools/frequency/#dct-transform","title":"DCT Transform","text":"<p>The DCT represents image blocks as sums of cosine functions:</p> <pre><code>For each 8x8 block:\nF(u,v) = C(u)C(v)/4 \u00d7 \u03a3\u03a3 f(x,y) \u00d7 cos[(2x+1)u\u03c0/16] \u00d7 cos[(2y+1)v\u03c0/16]\n</code></pre> <p>Properties:</p> <ul> <li>Energy compaction \u2014 Most energy in low-frequency coefficients</li> <li>JPEG compatible \u2014 Same transform used in JPEG compression</li> <li>Block-based \u2014 Analyzes local frequency content</li> </ul>"},{"location":"tools/frequency/#fft-transform","title":"FFT Transform","text":"<p>The FFT decomposes the entire image into sinusoidal components:</p> <pre><code>F(u,v) = \u03a3\u03a3 f(x,y) \u00d7 e^(-j2\u03c0(ux/M + vy/N))\n</code></pre> <p>Properties:</p> <ul> <li>Global analysis \u2014 Sees patterns across entire image</li> <li>Periodic detection \u2014 Reveals repeating patterns</li> <li>Phase information \u2014 Can detect subtle regularities</li> </ul>"},{"location":"tools/frequency/#forensic-applications","title":"Forensic Applications","text":""},{"location":"tools/frequency/#detecting-upscaling","title":"Detecting Upscaling","text":"<p>Upscaled images show characteristic FFT patterns:</p> <pre><code>Natural image FFT:     Upscaled image FFT:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502       \u2022         \u2502    \u2502   \u2022   \u2022   \u2022     \u2502\n\u2502     \u2022   \u2022       \u2502    \u2502       \u2022         \u2502\n\u2502   \u2022   X   \u2022     \u2502    \u2502 \u2022   \u2022 X \u2022   \u2022   \u2502\n\u2502     \u2022   \u2022       \u2502    \u2502       \u2022         \u2502\n\u2502       \u2022         \u2502    \u2502   \u2022   \u2022   \u2022     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2191 Periodic peaks\n</code></pre>"},{"location":"tools/frequency/#detecting-ai-generation","title":"Detecting AI Generation","text":"<p>Some AI generators leave frequency fingerprints:</p> <ul> <li>GAN artifacts create specific spectral signatures</li> <li>Diffusion models may show unusual high-frequency patterns</li> <li>Upscaling networks leave characteristic marks</li> </ul>"},{"location":"tools/frequency/#best-practices","title":"Best Practices","text":""},{"location":"tools/frequency/#for-analysis","title":"For Analysis","text":"<ol> <li>Compare to references \u2014 What does a similar authentic image look like?</li> <li>Look for anomalies \u2014 Unusual peaks or patterns</li> <li>Consider context \u2014 Content type affects expected spectrum</li> <li>Combine evidence \u2014 Use with other tools</li> </ol>"},{"location":"tools/frequency/#when-useful","title":"When Useful","text":"Scenario Frequency Analysis Value Suspected upscaling High \u2014 clear signature JPEG history investigation Moderate \u2014 DCT features help AI generation detection Variable \u2014 depends on generator General manipulation Low \u2014 other tools better"},{"location":"tools/frequency/#see-also","title":"See Also","text":"<ul> <li>Residual Analysis \u2014 Noise-based analysis</li> <li>TruFor \u2014 Neural forgery detection</li> <li>Tools Overview \u2014 Complete tool reference</li> </ul>"},{"location":"tools/jpeg/","title":"JPEG Analysis Tools","text":"<p>Two tools analyze JPEG compression artifacts: <code>analyze_jpeg_compression</code> for general compression analysis and <code>detect_jpeg_quantization</code> for quantization table analysis.</p>"},{"location":"tools/jpeg/#overview","title":"Overview","text":"<p>JPEG compression artifacts carry forensic information about an image's processing history. These tools examine:</p> <ul> <li>Quantization tables \u2014 Quality settings used during compression</li> <li>Double compression \u2014 Signs of recompression at different qualities</li> <li>Blocking artifacts \u2014 8x8 DCT block patterns</li> </ul> Tool Purpose Key Output <code>analyze_jpeg_compression</code> General compression analysis Compression artifacts, quality indicators <code>detect_jpeg_quantization</code> Quantization table analysis Quality estimate, double-compression flags <p>JPEG Only</p> <p>Both tools automatically skip for non-JPEG inputs.</p>"},{"location":"tools/jpeg/#detect_jpeg_quantization","title":"detect_jpeg_quantization","text":""},{"location":"tools/jpeg/#purpose","title":"Purpose","text":"<p>Extract and analyze JPEG quantization tables to:</p> <ul> <li>Estimate original compression quality</li> <li>Detect double-compression (different quality saves)</li> <li>Identify standard vs custom quantization tables</li> </ul>"},{"location":"tools/jpeg/#usage","title":"Usage","text":"<pre><code>/path/to/image.jpg\n</code></pre>"},{"location":"tools/jpeg/#output","title":"Output","text":"<pre><code>{\n    \"tool\": \"detect_jpeg_quantization\",\n    \"status\": \"completed\",\n    \"image_path\": \"/path/to/image.jpg\",\n    \"estimated_quality\": 85,\n    \"luminance_table\": [...],\n    \"chrominance_table\": [...],\n    \"double_compression_indicators\": {\n        \"periodic_patterns\": false,\n        \"table_mismatch\": false\n    },\n    \"note\": \"Quantization tables and quality estimation...\"\n}\n</code></pre>"},{"location":"tools/jpeg/#interpreting-quality-estimates","title":"Interpreting Quality Estimates","text":"Estimated Quality Interpretation 95-100 High quality, minimal compression loss 85-95 Good quality, typical for cameras 70-85 Moderate compression, web-optimized &lt; 70 Heavy compression, quality degraded"},{"location":"tools/jpeg/#double-compression-detection","title":"Double Compression Detection","text":"<p>Double compression occurs when a JPEG is opened and re-saved. Indicators:</p> <ol> <li>Periodic DCT patterns \u2014 Statistical anomalies in coefficient distribution</li> <li>Quality mismatch \u2014 Tables don't match standard quality levels</li> <li>Blocking artifacts \u2014 Visible 8x8 grid patterns</li> </ol>"},{"location":"tools/jpeg/#analyze_jpeg_compression","title":"analyze_jpeg_compression","text":""},{"location":"tools/jpeg/#purpose_1","title":"Purpose","text":"<p>Broader compression artifact analysis including:</p> <ul> <li>Overall compression quality assessment</li> <li>Artifact visibility metrics</li> <li>Compression history indicators</li> </ul>"},{"location":"tools/jpeg/#usage_1","title":"Usage","text":"<pre><code>/path/to/image.jpg\n</code></pre>"},{"location":"tools/jpeg/#output_1","title":"Output","text":"<pre><code>{\n    \"tool\": \"analyze_jpeg_compression\",\n    \"status\": \"completed\",\n    \"image_path\": \"/path/to/image.jpg\",\n    \"compression_analysis\": {\n        \"blocking_artifacts\": 0.15,\n        \"quality_estimate\": 82,\n        \"compression_ratio\": 12.5\n    },\n    \"note\": \"JPEG compression artifact analysis...\"\n}\n</code></pre>"},{"location":"tools/jpeg/#forensic-applications","title":"Forensic Applications","text":""},{"location":"tools/jpeg/#quality-mismatch-detection","title":"Quality Mismatch Detection","text":"<p>If different regions have different quality characteristics:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Quality 85 throughout       \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510             \u2502\n\u2502   \u2502 Quality   \u2502 \u2190 Pasted    \u2502\n\u2502   \u2502    95     \u2502   from      \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   elsewhere \u2502\n\u2502                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>This indicates:</p> <ul> <li>Element was saved at different quality</li> <li>Possible splicing from another source</li> <li>Requires additional verification</li> </ul>"},{"location":"tools/jpeg/#double-compression-history","title":"Double-Compression History","text":"<p>When an image is re-saved:</p> <ol> <li>Original DCT coefficients are quantized</li> <li>Re-quantization at different quality creates periodic patterns</li> <li>These patterns are detectable via statistical analysis</li> </ol>"},{"location":"tools/jpeg/#limitations","title":"Limitations","text":""},{"location":"tools/jpeg/#common-issues","title":"Common Issues","text":"<ol> <li>Platform recompression \u2014 Social media re-encodes all uploads</li> <li>Unknown original \u2014 Can't compare to original quality</li> <li>Standard tables \u2014 Many cameras use standard quantization</li> <li>Multiple re-saves \u2014 Patterns become ambiguous</li> </ol>"},{"location":"tools/jpeg/#when-analysis-is-inconclusive","title":"When Analysis is Inconclusive","text":"<ul> <li>Single compression can't rule out manipulation</li> <li>Standard quality tables are common</li> <li>Heavy compression masks subtle differences</li> </ul>"},{"location":"tools/jpeg/#best-practices","title":"Best Practices","text":""},{"location":"tools/jpeg/#for-analysis","title":"For Analysis","text":"<ol> <li>Compare to known sources \u2014 Device-specific tables are identifiable</li> <li>Look for inconsistencies \u2014 Uniform quality is expected</li> <li>Combine with ELA \u2014 Corroborating compression evidence</li> <li>Consider context \u2014 Social media processing confounds</li> </ol>"},{"location":"tools/jpeg/#when-to-use","title":"When to Use","text":"Scenario Tool Choice Quality estimation <code>detect_jpeg_quantization</code> Double-compression check <code>detect_jpeg_quantization</code> General artifact analysis <code>analyze_jpeg_compression</code> Both for thorough analysis Use both tools"},{"location":"tools/jpeg/#technical-background","title":"Technical Background","text":""},{"location":"tools/jpeg/#jpeg-compression-overview","title":"JPEG Compression Overview","text":"<pre><code>flowchart LR\n    subgraph Encode\n        IMG[Image] --&gt; DCT[8x8 DCT&lt;br/&gt;Transform]\n        DCT --&gt; QUANT[Quantization&lt;br/&gt;\u00f7 Q-table]\n        QUANT --&gt; ENCODE[Entropy&lt;br/&gt;Encoding]\n    end\n\n    subgraph Decode\n        DECODE[Entropy&lt;br/&gt;Decoding] --&gt; DEQUANT[Dequantization&lt;br/&gt;\u00d7 Q-table]\n        DEQUANT --&gt; IDCT[Inverse&lt;br/&gt;DCT]\n        IDCT --&gt; OUT[Reconstructed]\n    end\n\n    ENCODE --&gt; DECODE</code></pre>"},{"location":"tools/jpeg/#quantization-tables","title":"Quantization Tables","text":"<p>The quantization table determines quality/size tradeoff:</p> <ul> <li>Lower values \u2192 Less quantization \u2192 Higher quality</li> <li>Higher values \u2192 More quantization \u2192 Smaller files</li> </ul> <p>Standard tables are scaled by quality factor (1-100).</p>"},{"location":"tools/jpeg/#double-compression-artifacts","title":"Double Compression Artifacts","text":"<p>When re-saving at different quality:</p> <pre><code>DCT coefficients after first save: [32, 0, -8, 0, 4, ...]\n                                    \u2193 (quantized by Q1)\nAfter dequantization: [32, 0, -8, 0, 4, ...]\n                                    \u2193 (re-quantized by Q2)\nDCT coefficients after second save: [35, 0, -9, 0, 5, ...]\n</code></pre> <p>The mismatch creates periodic patterns in coefficient distributions.</p>"},{"location":"tools/jpeg/#see-also","title":"See Also","text":"<ul> <li>ELA \u2014 Compression-based editing detection</li> <li>TruFor \u2014 Neural forgery detection</li> <li>Tools Overview \u2014 Complete tool reference</li> </ul>"},{"location":"tools/metadata/","title":"Metadata &amp; C2PA Analysis","text":"<p>Extraction and interpretation of image metadata including EXIF, XMP, ICC profiles, and Content Credentials (C2PA).</p>"},{"location":"tools/metadata/#overview","title":"Overview","text":"<p>The <code>metadata</code> tool extracts embedded information from image files that can reveal provenance, capture details, and processing history.</p>"},{"location":"tools/metadata/#metadata-types","title":"Metadata Types","text":""},{"location":"tools/metadata/#exif-exchangeable-image-file-format","title":"EXIF (Exchangeable Image File Format)","text":"<p>Standard metadata embedded by cameras:</p> Field Forensic Value Make/Model Camera identification DateTimeOriginal Capture timestamp GPSInfo Location (if enabled) ExposureTime Camera settings Software Processing software ImageDescription User annotations"},{"location":"tools/metadata/#xmp-extensible-metadata-platform","title":"XMP (Extensible Metadata Platform)","text":"<p>Adobe's metadata standard:</p> Field Forensic Value CreatorTool Editing software History Edit history DerivedFrom Source file reference DocumentID Unique identifier"},{"location":"tools/metadata/#icc-profile","title":"ICC Profile","text":"<p>Color management information:</p> Field Forensic Value ProfileDescription Color space name ProfileCreator Creating software Creation date Profile timestamp"},{"location":"tools/metadata/#c2pa-content-credentials","title":"C2PA (Content Credentials)","text":"<p>Cryptographic provenance standard:</p> Field Forensic Value has_c2pa_manifest Presence of credentials is_valid_c2pa Cryptographic validity c2pa_actions Recorded modifications claim_generator Creating software"},{"location":"tools/metadata/#tool-output","title":"Tool Output","text":""},{"location":"tools/metadata/#example-output","title":"Example Output","text":"<pre><code>{\n    \"tool\": \"metadata\",\n    \"status\": \"completed\",\n    \"image_path\": \"photo.jpg\",\n    \"exif_data\": {\n        \"Make\": \"Canon\",\n        \"Model\": \"EOS R5\",\n        \"DateTimeOriginal\": \"2025:12:15 14:30:00\",\n        \"GPSLatitude\": 37.7749,\n        \"GPSLongitude\": -122.4194,\n        \"Software\": \"Adobe Photoshop 25.0\"\n    },\n    \"xmp_data\": {\n        \"CreatorTool\": \"Adobe Photoshop 25.0\",\n        \"History\": [\n            {\"action\": \"created\", \"when\": \"2025-12-15T14:30:00\"},\n            {\"action\": \"saved\", \"when\": \"2025-12-15T15:45:00\"}\n        ]\n    },\n    \"icc_profile_summary\": {\n        \"description\": \"sRGB IEC61966-2.1\",\n        \"color_space\": \"RGB\"\n    },\n    \"c2pa_data\": {\n        \"has_c2pa_manifest\": false,\n        \"is_valid_c2pa\": null,\n        \"c2pa_actions\": []\n    },\n    \"file_info\": {\n        \"file_size_bytes\": 2456789,\n        \"format\": \"JPEG\"\n    },\n    \"note\": \"Metadata extraction complete. Camera: Canon EOS R5. Software: Adobe Photoshop 25.0.\"\n}\n</code></pre>"},{"location":"tools/metadata/#forensic-interpretation","title":"Forensic Interpretation","text":""},{"location":"tools/metadata/#presence-vs-absence","title":"Presence vs. Absence","text":"Scenario Interpretation Full EXIF present Consistent with camera capture EXIF stripped Common with social media, could be intentional Software tag present Image was processed C2PA valid Strong provenance evidence C2PA invalid Tampering or corruption possible"},{"location":"tools/metadata/#suspicious-indicators","title":"Suspicious Indicators","text":"<p>Red Flags</p> <ul> <li>EXIF timestamps inconsistent with claimed date</li> <li>Camera model doesn't match image characteristics</li> <li>Multiple software tags suggesting heavy processing</li> <li>Invalid or corrupted C2PA manifest</li> <li>GPS coordinates inconsistent with claimed location</li> </ul>"},{"location":"tools/metadata/#benign-explanations","title":"Benign Explanations","text":"<p>Not all metadata anomalies indicate manipulation:</p> <ul> <li>Social media stripping \u2014 Platforms routinely remove metadata</li> <li>Privacy protection \u2014 Users intentionally remove GPS/camera info</li> <li>Format conversion \u2014 Converting formats can lose metadata</li> <li>Legitimate editing \u2014 Color correction, cropping adds software tags</li> </ul>"},{"location":"tools/metadata/#c2pa-deep-dive","title":"C2PA Deep Dive","text":""},{"location":"tools/metadata/#what-is-c2pa","title":"What is C2PA?","text":"<p>The Coalition for Content Provenance and Authenticity (C2PA) defines a standard for cryptographically signed metadata that tracks content creation and modification.</p>"},{"location":"tools/metadata/#c2pa-verification","title":"C2PA Verification","text":"<pre><code>Manifest present \u2192 Check signature validity\n  \u2193\nValid signature \u2192 Check claim chain\n  \u2193\nValid chain \u2192 Review recorded actions\n</code></pre>"},{"location":"tools/metadata/#interpretation","title":"Interpretation","text":"C2PA Status Meaning <code>has_c2pa_manifest: true, is_valid_c2pa: true</code> Verified provenance <code>has_c2pa_manifest: true, is_valid_c2pa: false</code> Tampered or corrupted <code>has_c2pa_manifest: false</code> No credentials (common)"},{"location":"tools/metadata/#actions-recorded","title":"Actions Recorded","text":"<p>C2PA manifests may record:</p> <ul> <li><code>c2pa.created</code> \u2014 Initial creation</li> <li><code>c2pa.edited</code> \u2014 Modifications</li> <li><code>c2pa.cropped</code> \u2014 Cropping operations</li> <li><code>c2pa.resized</code> \u2014 Size changes</li> <li><code>c2pa.filtered</code> \u2014 Filter applications</li> </ul>"},{"location":"tools/metadata/#limitations","title":"Limitations","text":""},{"location":"tools/metadata/#metadata-can-be-forged","title":"Metadata Can Be Forged","text":"<p>Metadata is Not Proof</p> <p>Metadata can be easily manipulated. EXIF data, timestamps, and GPS coordinates can be altered or fabricated using common tools.</p>"},{"location":"tools/metadata/#absence-is-not-evidence","title":"Absence Is Not Evidence","text":"<p>Missing metadata doesn't indicate manipulation:</p> <ul> <li>Many legitimate workflows strip metadata</li> <li>Social media platforms remove it</li> <li>Privacy-conscious users remove it</li> </ul>"},{"location":"tools/metadata/#c2pa-adoption","title":"C2PA Adoption","text":"<ul> <li>Relatively new standard</li> <li>Not yet widely deployed</li> <li>Absence is normal (for now)</li> </ul>"},{"location":"tools/metadata/#usage","title":"Usage","text":""},{"location":"tools/metadata/#tool-invocation","title":"Tool Invocation","text":"<pre><code># Agent calls metadata tool\nresult = agent.call_tool(\"metadata\", \"path/to/image.jpg\")\n</code></pre>"},{"location":"tools/metadata/#command-line","title":"Command Line","text":"<pre><code># Analysis includes metadata by default\npython scripts/analyze_image.py --image photo.jpg\n</code></pre>"},{"location":"tools/metadata/#best-practices","title":"Best Practices","text":""},{"location":"tools/metadata/#when-metadata-supports-authenticity","title":"When Metadata Supports Authenticity","text":"<p>Strong evidence when:</p> <ul> <li>EXIF matches claimed camera/date</li> <li>C2PA manifest is valid</li> <li>Edit history is minimal and consistent</li> <li>GPS matches claimed location</li> </ul>"},{"location":"tools/metadata/#when-metadata-raises-questions","title":"When Metadata Raises Questions","text":"<p>Investigate further when:</p> <ul> <li>Timestamps are impossible or inconsistent</li> <li>Camera model seems wrong for image quality</li> <li>Multiple editing software in history</li> <li>C2PA signature is invalid</li> <li>Metadata seems too perfect (possibly fabricated)</li> </ul>"},{"location":"tools/metadata/#integration-with-other-tools","title":"Integration with Other Tools","text":"<p>Metadata should corroborate other evidence:</p> <pre><code>flowchart TD\n    Meta[Metadata Analysis] --&gt; Decision\n    Visual[Visual Analysis] --&gt; Decision\n    TruFor[TruFor Score] --&gt; Decision\n    Decision{Consistent?}\n    Decision --&gt;|Yes| Confidence[Higher Confidence]\n    Decision --&gt;|No| Review[Needs Review]</code></pre>"},{"location":"tools/metadata/#see-also","title":"See Also","text":"<ul> <li>Tools Overview \u2014 All forensic tools</li> <li>TruFor \u2014 Neural forgery detection</li> <li>SWGDE Best Practices \u2014 Forensic standards</li> </ul>"},{"location":"tools/overview/","title":"Forensic Tools Overview","text":"<p>DF3 includes seven specialized forensic analysis tools. This page provides an overview of the toolchain; see individual tool pages for detailed documentation.</p>"},{"location":"tools/overview/#tool-summary","title":"Tool Summary","text":"Tool Purpose Best For Input Requirements TruFor Neural forgery detection Manipulation, splicing Any image format ELA Error Level Analysis JPEG editing detection JPEG only JPEG Analysis Compression artifacts Double compression JPEG only Frequency Analysis DCT/FFT patterns Synthetic regularities Any format Residual Analysis Noise pattern analysis Generation artifacts Any format Metadata EXIF/XMP/C2PA Provenance, tampering Any format Code Execution Custom Python Specialized analysis Any format"},{"location":"tools/overview/#tool-selection-philosophy","title":"Tool Selection Philosophy","text":"<p>The DF3 agent decides which tools to use based on:</p> <ol> <li>Image format \u2014 JPEG-specific tools skip for PNG/WebP</li> <li>Initial visual analysis \u2014 What artifacts were observed</li> <li>Investigation needs \u2014 What questions need answering</li> </ol>"},{"location":"tools/overview/#typical-tool-selection-patterns","title":"Typical Tool Selection Patterns","text":"Scenario Likely Tools Suspected manipulation/editing TruFor, ELA Suspected AI generation Frequency analysis, residuals Unknown provenance Metadata, TruFor JPEG quality questions JPEG analysis, detect_jpeg_quantization Need custom analysis Code execution"},{"location":"tools/overview/#tool-categories","title":"Tool Categories","text":""},{"location":"tools/overview/#manipulation-detectors","title":"Manipulation Detectors","text":"<p>TruFor and ELA are primary manipulation detectors:</p> <pre><code>flowchart LR\n    subgraph Input\n        IMG[Image]\n    end\n\n    subgraph TruFor\n        TF1[RGB Features]\n        TF2[Noiseprint++]\n        TF3[Transformer Fusion]\n        TF1 --&gt; TF3\n        TF2 --&gt; TF3\n    end\n\n    subgraph ELA\n        E1[Recompress JPEG]\n        E2[Compute Difference]\n        E3[Find Anomalies]\n        E1 --&gt; E2 --&gt; E3\n    end\n\n    IMG --&gt; TruFor\n    IMG --&gt; ELA\n\n    TruFor --&gt; PROB[manipulation_probability]\n    ELA --&gt; ANOM[ela_anomaly_score]\n\n    style TruFor fill:#4a1d96,color:#fff\n    style ELA fill:#b45309,color:#fff</code></pre> <p>Key distinction:</p> <ul> <li>TruFor \u2014 AI-driven, detects subtle manipulation patterns</li> <li>ELA \u2014 Classic technique, detects compression inconsistencies</li> </ul>"},{"location":"tools/overview/#format-analyzers","title":"Format Analyzers","text":"<p>JPEG tools analyze compression artifacts:</p> <ul> <li><code>analyze_jpeg_compression</code> \u2014 Overall compression analysis</li> <li><code>detect_jpeg_quantization</code> \u2014 Quality estimation, double-compression detection</li> </ul> <p>JPEG Only</p> <p>These tools automatically skip for non-JPEG inputs with an informative message.</p>"},{"location":"tools/overview/#signal-analyzers","title":"Signal Analyzers","text":"<p>Frequency and Residual analysis examine image signals:</p> <ul> <li><code>analyze_frequency_domain</code> \u2014 DCT/FFT frequency patterns</li> <li><code>extract_residuals</code> \u2014 DRUNet neural denoiser residuals</li> </ul> <p>These can reveal:</p> <ul> <li>AI generation regularities</li> <li>Processing artifacts</li> <li>Unusual noise distributions</li> </ul>"},{"location":"tools/overview/#provenance-tools","title":"Provenance Tools","text":"<p>Metadata extraction reveals:</p> <ul> <li>Camera/device information (EXIF)</li> <li>Software history (XMP)</li> <li>Content credentials (C2PA)</li> <li>Timestamps and GPS</li> </ul>"},{"location":"tools/overview/#custom-analysis","title":"Custom Analysis","text":"<p>Code execution allows:</p> <ul> <li>Custom Python scripts</li> <li>Region cropping and inspection</li> <li>Statistical analysis</li> <li>Visualization generation</li> </ul>"},{"location":"tools/overview/#tool-output-format","title":"Tool Output Format","text":"<p>All tools return JSON strings with consistent structure:</p> <pre><code>{\n    \"tool\": \"tool_name\",\n    \"status\": \"completed\" | \"error\" | \"skipped\",\n    \"image_path\": \"/path/to/image.jpg\",\n\n    // Tool-specific fields...\n    \"manipulation_probability\": 0.15,\n\n    // Optional error information\n    \"error\": null,\n\n    // Optional notes/interpretation guidance\n    \"note\": \"Higher values suggest...\"\n}\n</code></pre>"},{"location":"tools/overview/#status-values","title":"Status Values","text":"Status Meaning <code>completed</code> Tool ran successfully <code>error</code> Tool encountered an error (see <code>error</code> field) <code>skipped</code> Tool not applicable (e.g., JPEG tool on PNG)"},{"location":"tools/overview/#tool-caching","title":"Tool Caching","text":"<p>Tool outputs are cached to avoid redundant computation:</p> <pre><code># Cache key components\ncache_key = (tool_name, image_hash, parameters_hash)\n</code></pre>"},{"location":"tools/overview/#enabledisable-caching","title":"Enable/Disable Caching","text":"<pre><code># With caching (default)\npython scripts/evaluate_llms.py --enable-tool-cache ...\n\n# Without caching (for benchmarking)\npython scripts/evaluate_llms.py --disable-tool-cache ...\n</code></pre>"},{"location":"tools/overview/#cache-location","title":"Cache Location","text":"<p>Default: <code>.tool_cache/</code> in project root</p>"},{"location":"tools/overview/#tool-registration","title":"Tool Registration","text":"<p>Tools are registered in <code>src/tools/forensic_tools.py</code>:</p> <pre><code>from langchain_core.tools import Tool, StructuredTool\n\ntools = [\n    Tool(\n        name=\"metadata\",\n        func=metadata,\n        description=\"Extract image metadata (EXIF/XMP/ICC)...\",\n    ),\n    StructuredTool.from_function(\n        func=_trufor_structured,\n        name=\"perform_trufor\",\n        description=\"Run TruFor AI-driven forgery detection...\",\n        args_schema=TruForInput,\n    ),\n    # ... more tools\n]\n</code></pre>"},{"location":"tools/overview/#tool-input-formats","title":"Tool Input Formats","text":"Tool Input Format Example metadata String path or JSON <code>\"path/to/image.jpg\"</code> perform_trufor StructuredTool <code>{\"path\": \"image.jpg\"}</code> perform_ela StructuredTool <code>{\"path\": \"image.jpg\", \"quality\": 75}</code> analyze_* String path <code>\"path/to/image.jpg\"</code> execute_python_code StructuredTool <code>{\"code\": \"...\", \"image_path\": \"...\"}</code>"},{"location":"tools/overview/#interpreting-tool-results","title":"Interpreting Tool Results","text":""},{"location":"tools/overview/#trufor-results","title":"TruFor Results","text":"Field Range Interpretation <code>manipulation_probability</code> 0.0 - 1.0 Near 0 = clean, Near 1 = manipulated <code>detection_score</code> 0.0 - 1.0 Raw detection confidence"},{"location":"tools/overview/#ela-results","title":"ELA Results","text":"Field Typical Range Interpretation <code>ela_mean</code> 0 - 50 Average error level <code>ela_std</code> 0 - 30 Error variation <code>ela_anomaly_score</code> 0 - 10+ Z-score of 95<sup>th</sup> percentile"},{"location":"tools/overview/#frequency-results","title":"Frequency Results","text":"<ul> <li>Periodic peaks may indicate synthetic regularities</li> <li>Unusual distributions suggest processing</li> <li>Compare to reference images when possible</li> </ul>"},{"location":"tools/overview/#residual-results","title":"Residual Results","text":"Field Interpretation <code>residual_energy</code> Overall noise level <code>residual_skew</code> Distribution asymmetry <code>residual_kurtosis</code> Distribution peakedness"},{"location":"tools/overview/#limitations","title":"Limitations","text":""},{"location":"tools/overview/#common-limitations","title":"Common Limitations","text":"<ol> <li>Heavy compression \u2014 Masks subtle artifacts</li> <li>Small images \u2014 Less signal to analyze</li> <li>Screenshots \u2014 Additional processing confounds analysis</li> <li>Platform recompression \u2014 Social media alters images</li> </ol>"},{"location":"tools/overview/#tool-specific-limitations","title":"Tool-Specific Limitations","text":"Tool Key Limitations TruFor Detects manipulation, not synthesis ELA JPEG-only, confounded by recompression JPEG tools JPEG-only Frequency Highly content-dependent Residuals Statistical overlap between classes Metadata Can be stripped or forged"},{"location":"tools/overview/#best-practices","title":"Best Practices","text":""},{"location":"tools/overview/#for-accurate-analysis","title":"For Accurate Analysis","text":"<ol> <li>Use original images \u2014 Avoid screenshots, re-saves</li> <li>Preserve JPEG \u2014 Don't convert to PNG before analysis</li> <li>Consider format \u2014 JPEG tools only work on JPEG</li> <li>Combine evidence \u2014 No single tool is definitive</li> </ol>"},{"location":"tools/overview/#for-performance","title":"For Performance","text":"<ol> <li>Enable caching \u2014 Avoid redundant computation</li> <li>Use appropriate tools \u2014 Skip irrelevant tools</li> <li>Check format first \u2014 Avoid JPEG tools on PNG</li> </ol>"},{"location":"tools/overview/#for-interpretation","title":"For Interpretation","text":"<ol> <li>Context matters \u2014 Scores are relative, not absolute</li> <li>Corroborate findings \u2014 Multiple tools agreeing is stronger</li> <li>Consider limitations \u2014 Know what tools can't detect</li> <li>Document uncertainty \u2014 Report when evidence is weak</li> </ol>"},{"location":"tools/overview/#next-steps","title":"Next Steps","text":""},{"location":"tools/overview/#trufor","title":"TruFor","text":"<p>Neural forgery detection and localization.</p> <p>TruFor \u2192</p>"},{"location":"tools/overview/#ela","title":"ELA","text":"<p>Error Level Analysis for JPEG editing.</p> <p>ELA \u2192</p>"},{"location":"tools/overview/#jpeg-analysis","title":"JPEG Analysis","text":"<p>Compression artifact analysis.</p> <p>JPEG \u2192</p>"},{"location":"tools/overview/#frequency","title":"Frequency","text":"<p>DCT/FFT frequency analysis.</p> <p>Frequency \u2192</p>"},{"location":"tools/residuals/","title":"Residual Analysis","text":"<p>Uses the DRUNet neural denoiser to extract and analyze image residuals (noise patterns) that can reveal generation artifacts.</p>"},{"location":"tools/residuals/#overview","title":"Overview","text":"<p>Residual analysis applies a neural network denoiser to an image and examines the \"residual\" \u2014 the difference between the original and denoised versions. This residual contains noise patterns that differ between real photographs and processed/generated images.</p> Aspect Detail Purpose Analyze noise patterns for authenticity cues Technique DRUNet deep learning denoiser Input Any common image format Output Residual statistics and distribution metrics"},{"location":"tools/residuals/#how-it-works","title":"How It Works","text":"<pre><code>flowchart LR\n    subgraph Input\n        IMG[Original Image]\n    end\n\n    subgraph DRUNet[\"Neural Denoiser\"]\n        DN[DRUNet Model]\n    end\n\n    subgraph Process\n        DENOISE[Denoised Image]\n        DIFF[Residual&lt;br/&gt;= Original - Denoised]\n    end\n\n    subgraph Analysis\n        STATS[Statistical&lt;br/&gt;Analysis]\n        DIST[Distribution&lt;br/&gt;Metrics]\n    end\n\n    IMG --&gt; DN\n    DN --&gt; DENOISE\n    IMG --&gt; DIFF\n    DENOISE --&gt; DIFF\n    DIFF --&gt; STATS\n    DIFF --&gt; DIST</code></pre>"},{"location":"tools/residuals/#the-principle","title":"The Principle","text":"<p>Different image sources produce different noise characteristics:</p> Source Noise Characteristics Camera photos Sensor noise, shot noise, read noise AI-generated May be unnaturally clean or have artificial patterns Manipulated Mixed noise characteristics in different regions Heavily processed Smoothed or altered noise structure"},{"location":"tools/residuals/#usage","title":"Usage","text":""},{"location":"tools/residuals/#tool-call-format","title":"Tool Call Format","text":"<pre><code>/path/to/image.jpg\n</code></pre>"},{"location":"tools/residuals/#output-format","title":"Output Format","text":"<pre><code>{\n    \"tool\": \"extract_residuals\",\n    \"status\": \"completed\",\n    \"image_path\": \"/path/to/image.jpg\",\n    \"residual_mean\": 0.0012,\n    \"residual_std\": 8.45,\n    \"residual_skew\": 0.23,\n    \"residual_kurtosis\": 3.12,\n    \"residual_energy\": 71.4,\n    \"residual_energy_mean\": 0.0089,\n    \"residual_energy_std\": 0.0034,\n    \"residual_energy_p95\": 0.0156,\n    \"note\": \"DRUNet residual statistics for noise analysis...\"\n}\n</code></pre>"},{"location":"tools/residuals/#interpreting-results","title":"Interpreting Results","text":""},{"location":"tools/residuals/#key-metrics","title":"Key Metrics","text":"Metric Meaning <code>residual_mean</code> Average residual value (should be near 0) <code>residual_std</code> Standard deviation \u2014 overall noise level <code>residual_skew</code> Distribution asymmetry <code>residual_kurtosis</code> Distribution peakedness (3 = normal) <code>residual_energy</code> Total noise energy <code>residual_energy_p95</code> 95<sup>th</sup> percentile energy (high values = anomalies)"},{"location":"tools/residuals/#what-to-look-for","title":"What to Look For","text":"<p>Authentic photographs typically show:</p> <ul> <li>Moderate residual_std (sensor noise present)</li> <li>Near-zero skew (symmetric distribution)</li> <li>Kurtosis near 3 (normal-like distribution)</li> <li>Consistent energy across the image</li> </ul> <p>AI-generated images may show:</p> <ul> <li>Very low residual_std (unnaturally clean)</li> <li>Abnormal distribution shape</li> <li>Unusual energy patterns</li> </ul> <p>Manipulated images may show:</p> <ul> <li>Inconsistent statistics across regions</li> <li>Mixed noise characteristics</li> <li>Energy anomalies in edited areas</li> </ul>"},{"location":"tools/residuals/#limitations","title":"Limitations","text":""},{"location":"tools/residuals/#statistical-overlap","title":"Statistical Overlap","text":"<p>Residual statistics overlap significantly between authentic and fake images:</p> <ul> <li>Many authentic images are heavily processed (low noise)</li> <li>Many AI-generated images have realistic noise</li> <li>Statistical thresholds are not reliable</li> </ul> <p>Corroborating Evidence Only</p> <p>Use residual analysis as supporting evidence, not as a primary detector.</p>"},{"location":"tools/residuals/#content-dependence","title":"Content Dependence","text":"<ul> <li>Smooth regions have low residuals regardless of source</li> <li>High-frequency content shows higher residuals</li> <li>Comparison across content types is problematic</li> </ul>"},{"location":"tools/residuals/#processing-effects","title":"Processing Effects","text":"<ul> <li>Heavy compression destroys noise structure</li> <li>Sharpening and noise reduction alter residuals</li> <li>Social media processing confounds analysis</li> </ul>"},{"location":"tools/residuals/#technical-details","title":"Technical Details","text":""},{"location":"tools/residuals/#drunet-model","title":"DRUNet Model","text":"<p>DRUNet (Dilated Residual U-Net) is a state-of-the-art blind denoiser:</p> <ul> <li>Architecture: U-Net with residual blocks and dilated convolutions</li> <li>Training: Trained on natural images with various noise levels</li> <li>Property: Removes structured noise while preserving signal</li> </ul>"},{"location":"tools/residuals/#weight-location","title":"Weight Location","text":"<p>Weights are automatically downloaded on first use:</p> <pre><code>src/tools/forensic/drunet/weights/drunet_gray.pth\n</code></pre>"},{"location":"tools/residuals/#pre-warming","title":"Pre-warming","text":"<p>For batch evaluation:</p> <pre><code>from src.tools.forensic import prewarm_residual_extractor\n\n# Pre-warm before multi-threaded evaluation\nprewarm_residual_extractor()\n</code></pre>"},{"location":"tools/residuals/#forensic-applications","title":"Forensic Applications","text":""},{"location":"tools/residuals/#comparing-regions","title":"Comparing Regions","text":"<p>For suspected splicing, compare residual statistics across regions:</p> <pre><code># Analyze suspicious region vs background\n# Significant differences may indicate manipulation\n</code></pre>"},{"location":"tools/residuals/#baseline-comparison","title":"Baseline Comparison","text":"<p>If reference images from the same camera are available:</p> <ul> <li>Compare residual statistics</li> <li>Same camera should have similar noise characteristics</li> <li>Different source may indicate manipulation</li> </ul>"},{"location":"tools/residuals/#best-practices","title":"Best Practices","text":""},{"location":"tools/residuals/#for-analysis","title":"For Analysis","text":"<ol> <li>Use as supporting evidence \u2014 Not definitive alone</li> <li>Compare similar content \u2014 Different scenes have different residuals</li> <li>Consider processing history \u2014 Compression affects results</li> <li>Look for inconsistencies \u2014 More valuable than absolute values</li> </ol>"},{"location":"tools/residuals/#when-useful","title":"When Useful","text":"Scenario Residual Analysis Value Detecting AI generation Moderate \u2014 some generators detectable Splicing detection Moderate \u2014 regional inconsistencies Heavy compression Low \u2014 noise structure destroyed Original camera images High \u2014 noise is a fingerprint"},{"location":"tools/residuals/#see-also","title":"See Also","text":"<ul> <li>Frequency Analysis \u2014 Complementary signal analysis</li> <li>TruFor \u2014 Primary manipulation detector</li> <li>Tools Overview \u2014 Complete tool reference</li> </ul>"},{"location":"tools/trufor/","title":"TruFor","text":"<p>TruFor is an AI-driven neural forgery detection and localization tool that combines RGB features with Noiseprint++ through a transformer-based fusion architecture.</p>"},{"location":"tools/trufor/#overview","title":"Overview","text":"<p>TruFor (Trustworthy Forensics) is the primary manipulation detector in DF3. It was developed by the Research Group of University Federico II of Naples (GRIP-UNINA) and published at CVPR 2023.</p> Aspect Detail Purpose Detect and localize image manipulations Architecture Transformer fusion of RGB + Noiseprint++ Input Any common image format Output Manipulation probability (0-1), optional localization map"},{"location":"tools/trufor/#how-it-works","title":"How It Works","text":"<pre><code>flowchart LR\n    subgraph Input\n        IMG[Image]\n    end\n\n    subgraph Features[\"Feature Extraction\"]\n        RGB[RGB Features&lt;br/&gt;High-level semantics]\n        NP[Noiseprint++&lt;br/&gt;Low-level noise fingerprint]\n    end\n\n    subgraph Fusion[\"Transformer Fusion\"]\n        TF[CMX Architecture&lt;br/&gt;Cross-attention]\n    end\n\n    subgraph Output\n        DET[Detection Score]\n        LOC[Localization Map]\n    end\n\n    IMG --&gt; RGB\n    IMG --&gt; NP\n    RGB --&gt; TF\n    NP --&gt; TF\n    TF --&gt; DET\n    TF --&gt; LOC</code></pre>"},{"location":"tools/trufor/#key-components","title":"Key Components","text":"<ol> <li>RGB Features \u2014 High-level semantic features from the image</li> <li>Noiseprint++ \u2014 Low-level noise fingerprint sensitive to processing</li> <li>Transformer Fusion \u2014 Combines both feature types via cross-attention</li> <li>Detection Head \u2014 Produces overall manipulation probability</li> <li>Localization Head \u2014 Pixel-wise manipulation map</li> </ol>"},{"location":"tools/trufor/#usage","title":"Usage","text":""},{"location":"tools/trufor/#tool-call-format","title":"Tool Call Format","text":"<pre><code>{\"path\": \"/path/to/image.jpg\"}\n</code></pre> <p>Or as plain string:</p> <pre><code>/path/to/image.jpg\n</code></pre>"},{"location":"tools/trufor/#output-format","title":"Output Format","text":"<pre><code>{\n    \"tool\": \"perform_trufor\",\n    \"status\": \"completed\",\n    \"image_path\": \"/path/to/image.jpg\",\n    \"manipulation_probability\": 0.15,\n    \"detection_score\": 0.15,\n    \"localization_map\": null,\n    \"localization_map_size\": null,\n    \"note\": \"TruFor combines RGB features with Noiseprint++ for forgery detection...\"\n}\n</code></pre>"},{"location":"tools/trufor/#interpreting-results","title":"Interpreting Results","text":""},{"location":"tools/trufor/#manipulation-probability","title":"Manipulation Probability","text":"Score Range Interpretation Action 0.0 - 0.2 Low manipulation evidence Likely unedited 0.2 - 0.4 Some indicators Minor concern, investigate 0.4 - 0.6 Moderate evidence Probable manipulation 0.6 - 0.8 Strong evidence Likely manipulated 0.8 - 1.0 Very strong evidence Almost certainly manipulated"},{"location":"tools/trufor/#important-caveats","title":"Important Caveats","text":"<p>TruFor Detects Manipulation, Not Synthesis</p> <p>TruFor is designed to detect post-hoc editing \u2014 splicing, copy-move, inpainting, etc.</p> <ul> <li>Low score means the image wasn't edited after creation</li> <li>Low score does NOT mean the image is authentic</li> <li>AI-generated images often score low because they were never manipulated</li> </ul> <p>For AI-generated images, rely on visual analysis and other indicators.</p>"},{"location":"tools/trufor/#when-high-scores-are-meaningful","title":"When High Scores Are Meaningful","text":"<p>High TruFor scores are strong evidence when:</p> <ul> <li>Localized regions show high manipulation probability</li> <li>Score correlates with visually suspicious regions</li> <li>ELA also shows anomalies in the same regions</li> <li>Image content suggests editing (e.g., object insertion)</li> </ul>"},{"location":"tools/trufor/#when-low-scores-dont-mean-real","title":"When Low Scores Don't Mean \"Real\"","text":"<p>Low TruFor scores should be treated as neutral when:</p> <ul> <li>Image shows clear AI generation artifacts (anatomy, texture)</li> <li>Image is known to come from an AI generator</li> <li>Visual analysis suggests synthesis</li> </ul>"},{"location":"tools/trufor/#localization-map","title":"Localization Map","text":"<p>By default, DF3 disables the localization map to reduce output size. Enable it programmatically:</p> <pre><code>from src.tools.forensic import perform_trufor\nimport json\n\n# Request localization map\nresult = perform_trufor(json.dumps({\n    \"path\": \"image.jpg\",\n    \"return_map\": True\n}))\n\noutput = json.loads(result)\n# output[\"localization_map\"] contains base64 PNG\n</code></pre>"},{"location":"tools/trufor/#map-interpretation","title":"Map Interpretation","text":"<p>The localization map is a grayscale image where:</p> <ul> <li>Bright regions \u2014 High manipulation probability</li> <li>Dark regions \u2014 Low manipulation probability</li> </ul> <pre><code>Localization map for spliced image:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Dark (authentic)          \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510             \u2502\n\u2502   \u2502 BRIGHT    \u2502 \u2190 Spliced   \u2502\n\u2502   \u2502 (edited)  \u2502   region    \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2502\n\u2502                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"tools/trufor/#technical-details","title":"Technical Details","text":""},{"location":"tools/trufor/#model-architecture","title":"Model Architecture","text":"<p>TruFor uses the CMX (Cross-Modal Transformer) architecture:</p> <ul> <li>Backbone: SegFormer-based dual encoder</li> <li>Fusion: Transformer cross-attention</li> <li>Decoder: MLP-based segmentation head</li> </ul>"},{"location":"tools/trufor/#model-weights","title":"Model Weights","text":"<p>Weights are automatically downloaded on first use:</p> <ul> <li>Location: <code>weights/trufor/trufor.pth.tar</code></li> <li>Size: ~180 MB</li> <li>Source: GRIP-UNINA official release</li> </ul>"},{"location":"tools/trufor/#gpu-acceleration","title":"GPU Acceleration","text":"<p>TruFor benefits significantly from GPU acceleration:</p> Device Typical Inference Time CPU 10-30 seconds GPU (CUDA) 1-3 seconds"},{"location":"tools/trufor/#device-selection","title":"Device Selection","text":"<p>Device is determined automatically:</p> <ol> <li>Check <code>DF3_TRUFOR_DEVICE</code> environment variable</li> <li>Use CUDA if available</li> <li>Fall back to CPU</li> </ol> <pre><code># Force CPU mode\n$env:DF3_TRUFOR_DEVICE = \"cpu\"\n\n# Force specific GPU\n$env:DF3_TRUFOR_DEVICE = \"cuda:0\"\n</code></pre>"},{"location":"tools/trufor/#pre-warming","title":"Pre-warming","text":"<p>For batch evaluation, pre-warm the model before starting workers:</p> <pre><code>from src.tools.forensic import prewarm_trufor_model\n\n# Pre-warm before multi-threaded evaluation\nprewarm_trufor_model()\n</code></pre> <p>This avoids concurrent model loading issues.</p>"},{"location":"tools/trufor/#limitations","title":"Limitations","text":"<ol> <li>Manipulation focus \u2014 Designed for editing detection, not synthesis detection</li> <li>Compression sensitivity \u2014 Heavy JPEG compression can mask artifacts</li> <li>Resolution dependence \u2014 Very small images may lack sufficient signal</li> <li>Novel techniques \u2014 May not detect cutting-edge manipulation methods</li> </ol>"},{"location":"tools/trufor/#citation","title":"Citation","text":"<p>If you use TruFor in your research, please cite:</p> <pre><code>@InProceedings{Guillaro_2023_CVPR,\n    author    = {Guillaro, Fabrizio and Cozzolino, Davide and Sud, Avneesh \n                 and Dufour, Nicholas and Verdoliva, Luisa},\n    title     = {TruFor: Leveraging All-Round Clues for Trustworthy Image \n                 Forgery Detection and Localization},\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision \n                 and Pattern Recognition (CVPR)},\n    month     = {June},\n    year      = {2023},\n    pages     = {20606-20615}\n}\n</code></pre>"},{"location":"tools/trufor/#see-also","title":"See Also","text":"<ul> <li>ELA \u2014 Complementary compression-based detection</li> <li>Tools Overview \u2014 Complete tool reference</li> <li>Interpreting Results \u2014 Understanding tool outputs</li> </ul>"}]}