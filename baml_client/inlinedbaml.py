# ----------------------------------------------------------------------------
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml
#
# ----------------------------------------------------------------------------

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code using: baml-cli generate
# baml-cli is available with the baml package.

_file_map = {

    "clients.baml": "// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\n\n// Using the new OpenAI Responses API for enhanced formatting\nclient<llm> CustomGPT5 {\n  provider openai-responses\n  options {\n    model \"gpt-5\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomGPT5Mini {\n  provider openai-responses\n  retry_policy Exponential\n  options {\n    model \"gpt-5-mini\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\n// Dynamic client for forensic analysis - model will be overridden at runtime via ClientRegistry\n// This allows the model to be specified at runtime via --models parameter\n// Default model is gpt-5-mini, but will be overridden by Python code using ClientRegistry\nclient<llm> DynamicForensicClient {\n  provider openai-responses\n  retry_policy Exponential\n  options {\n    model \"gpt-5-mini\"  // Default, will be overridden at runtime\n    api_key env.OPENAI_API_KEY\n  }\n}\n\n// Openai with chat completion\nclient<llm> CustomGPT5Chat {\n  provider openai\n  options {\n    model \"gpt-5\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\n// Latest Anthropic Claude 4 models\nclient<llm> CustomOpus4 {\n  provider anthropic\n  options {\n    model \"claude-opus-4-1-20250805\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\nclient<llm> CustomSonnet4 {\n  provider anthropic\n  options {\n    model \"claude-sonnet-4-20250514\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\nclient<llm> CustomHaiku {\n  provider anthropic\n  retry_policy Constant\n  options {\n    model \"claude-3-5-haiku-20241022\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n// Example Google AI client (uncomment to use)\n// client<llm> CustomGemini {\n//   provider google-ai\n//   options {\n//     model \"gemini-2.5-pro\"\n//     api_key env.GOOGLE_API_KEY\n//   }\n// }\n\n// Example AWS Bedrock client (uncomment to use)\n// client<llm> CustomBedrock {\n//   provider aws-bedrock\n//   options {\n//     model \"anthropic.claude-sonnet-4-20250514-v1:0\"\n//     region \"us-east-1\"\n//     // AWS credentials are auto-detected from env vars\n//   }\n// }\n\n// Example Azure OpenAI client (uncomment to use)\n// client<llm> CustomAzure {\n//   provider azure-openai\n//   options {\n//     model \"gpt-5\"\n//     api_key env.AZURE_OPENAI_API_KEY\n//     base_url \"https://MY_RESOURCE_NAME.openai.azure.com/openai/deployments/MY_DEPLOYMENT_ID\"\n//     api_version \"2024-10-01-preview\"\n//   }\n// }\n\n// Example Vertex AI client (uncomment to use)\n// client<llm> CustomVertex {\n//   provider vertex-ai\n//   options {\n//     model \"gemini-2.5-pro\"\n//     location \"us-central1\"\n//     // Uses Google Cloud Application Default Credentials\n//   }\n// }\n\n// Example Ollama client for local models (uncomment to use)\n// client<llm> CustomOllama {\n//   provider openai-generic\n//   options {\n//     base_url \"http://localhost:11434/v1\"\n//     model \"llama4\"\n//     default_role \"user\" // Most local models prefer the user role\n//     // No API key needed for local Ollama\n//   }\n// }\n\n// https://docs.boundaryml.com/docs/snippets/clients/round-robin\nclient<llm> CustomFast {\n  provider round-robin\n  options {\n    // This will alternate between the two clients\n    strategy [CustomGPT5Mini, CustomHaiku]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/fallback\nclient<llm> OpenaiFallback {\n  provider fallback\n  options {\n    // This will try the clients in order until one succeeds\n    strategy [CustomGPT5Mini, CustomGPT5]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/retry\nretry_policy Constant {\n  max_retries 3\n  strategy {\n    type constant_delay\n    delay_ms 200\n  }\n}\n\nretry_policy Exponential {\n  max_retries 2\n  strategy {\n    type exponential_backoff\n    delay_ms 300\n    multiplier 1.5\n    max_delay_ms 10000\n  }\n}",
    "forensic_analysis.baml": "// Forensic Analysis BAML Functions\n// Following the multi-step approach to avoid reasoning degradation:\n// Step 1: Reasoning (unstructured) - allows LLM to reason freely\n// Step 2: Structuring (structured) - extracts structured data from reasoning\n\n// Enum for verdict\nenum Verdict {\n  REAL @description(\"Image appears authentic and natural\")\n  FAKE @description(\"Image appears AI-generated, synthetic, or manipulated\")\n  UNCERTAIN @description(\"Insufficient evidence or conflicting indicators\")\n}\n\n// Enum for confidence level (avoiding numeric intervals per BAML best practices)\nenum ConfidenceLevel {\n  HIGH @description(\"High confidence in the verdict\")\n  MEDIUM @description(\"Medium confidence in the verdict\")\n  LOW @description(\"Low confidence in the verdict\")\n}\n\n// Structured output model (used only in structuring step, not during reasoning)\nclass ForensicAnalysisResult {\n  verdict Verdict\n  confidence float @assert(between_0_and_1, {{ this >= 0.0 and this <= 1.0 }})\n  rationale string @description(\"Brief justification for the verdict (max 80 words)\")\n  visual_description string @description(\"Description of what is in the image\")\n  forensic_summary string @description(\"Summary of forensic tools used or 'No tools used'\")\n  full_text string @description(\"Complete formatted narrative combining all sections\")\n}\n\n// Step 1: Vision-only reasoning (UNSTRUCTURED - allows free reasoning)\n// This function returns unstructured text to avoid constraining reasoning\nfunction AnalyzeImageVisionOnly(image: image) -> string {\n  client DynamicForensicClient\n  prompt #\"\n    You are a forensic image analyst specializing in detecting AI-generated or manipulated images.\n    \n    CRITICAL: You MUST always start your analysis by describing what is actually in the image - the subjects, scene, objects, people, animals, environment, etc. Do NOT skip directly to forensic metrics.\n    \n    Analyze this image and decide one of three outcomes:\n    - real: authentic photograph\n    - fake: AI-generated, synthetic, or manipulated\n    - uncertain: inconclusive â†’ route to human/manual review\n    \n    Safety rule: Do NOT output \"real\" unless you have affirmative reasons it is authentic. Prefer \"uncertain\" over guessing.\n    \n    Provide your analysis in MARKDOWN format with this structure:\n    ### Visual Description\n    - Describe what is visibly in the image (subjects, scene, objects, people/animals, environment, colors, composition)\n    - Analyze lighting: sources, direction, intensity, shadows, reflections, consistency\n    - Check physics: perspective, shadows, reflections, physical interactions, textures\n    - Note any visual anomalies or inconsistencies you observe\n    \n    ### Forensic Analysis\n    - Vision-only pass; note \"No tools used\" and list any visual cues for/against synthesis\n    \n    ### Conclusion\n    - State if the image looks synthetic/AI vs natural, and why (refer to observations above)\n    - Include a line \"Verdict: real | fake | uncertain\" (uncertain means inconclusive/manual review)\n    \n    ### Confidence\n    - State High / Medium / Low with a brief justification\n    - Include \"Confidence (0-1): <value between 0 and 1>\"\n    \n    Think through your reasoning step by step. Do not constrain your thinking - provide detailed analysis.\n    \n    {{ _.role(\"user\") }} {{ image }}\n  \"#\n}\n\n// Note: Agent reasoning with tools is handled by LangChain/LangGraph in the Python code\n// BAML is used for vision-only analysis and structuring steps to avoid reasoning degradation\n\n// Step 3: Structuring function (STRUCTURED - extracts structured data from unstructured reasoning)\n// This function takes the unstructured reasoning output and extracts structured data\n// This separation ensures reasoning quality is not degraded by format constraints\nfunction StructureForensicAnalysis(reasoning_output: string) -> ForensicAnalysisResult {\n  client DynamicForensicClient\n  prompt #\"\n    Extract structured information from this forensic analysis reasoning output.\n    \n    The analysis may contain:\n    - A visual description of the image\n    - Forensic tool results or summaries\n    - A conclusion with a verdict (real/fake/uncertain)\n    - A confidence level and value\n    \n    Extract the following information:\n    - verdict: The final verdict (real, fake, or uncertain). Treat \"inconclusive\" / \"cannot determine\" as \"uncertain\".\n    - confidence: A float between 0 and 1\n    - rationale: A brief justification (max 80 words)\n    - visual_description: Description of what's in the image\n    - forensic_summary: Summary of tools used or \"No tools used\"\n    - full_text: The complete formatted narrative from the reasoning output\n    \n    Reasoning output:\n    {{ reasoning_output }}\n    \n    {{ ctx.output_format }}\n  \"#\n}\n\n// Helper function for vision-only analysis that returns structured output\n// NOTE: This function directly returns structured output, which may cause reasoning degradation.\n// For best results, prefer the two-step approach: AnalyzeImageVisionOnly + StructureForensicAnalysis\n// This function is provided for convenience in simple cases where reasoning complexity is low.\nfunction AnalyzeImageVisionOnlyStructured(image: image) -> ForensicAnalysisResult {\n  client DynamicForensicClient\n  prompt #\"\n    You are a forensic image analyst. Analyze this image and assess whether it appears AI-generated, synthetic, or a deepfake.\n    \n    CRITICAL: You MUST always start your analysis by describing what is actually in the image - the subjects, scene, objects, people, animals, environment, etc.\n    \n    Think through your reasoning step by step. Consider:\n    1. Visual description: What is in the image, lighting, physics\n    2. Synthesis indicators: Visual cues for/against synthesis\n    3. Verdict: real, fake, or uncertain (uncertain means inconclusive/manual review)\n    4. Confidence: A value between 0 and 1\n    5. Rationale: Brief justification (max 80 words)\n    \n    Safety rule: Do NOT output \"real\" unless you have affirmative reasons it is authentic. Prefer \"uncertain\" over guessing.\n    \n    After reasoning through these points, provide your structured answer.\n    \n    {{ _.role(\"user\") }} {{ image }}\n    \n    {{ ctx.output_format }}\n  \"#\n}\n",
    "generators.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"python/pydantic\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.214.0\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode async\n}\n",
}

def get_baml_files():
    return _file_map